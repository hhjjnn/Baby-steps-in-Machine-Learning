{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups()\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bjorndahl@augustana.ab.ca\n",
      "Subject: Re: document of .RTF\n",
      "Organization: Augustana University College, Camrose, Alberta\n",
      "Lines: 10\n",
      "\n",
      "In article <1993Mar30.113436.7339@worak.kaist.ac.kr>, tjyu@eve.kaist.ac.kr (Yu TaiJung) writes:\n",
      "> Does anybody have document of .RTF file or know where I can get it?\n",
      "> \n",
      "> Thanks in advance. :)\n",
      "\n",
      "I got one from Microsoft tech support.\n",
      "\n",
      "-- \n",
      "Sterling G. Bjorndahl, bjorndahl@Augustana.AB.CA or bjorndahl@camrose.uucp\n",
      "Augustana University College, Camrose, Alberta, Canada      (403) 679-1100\n",
      " comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "raw_text = dataset.data\n",
    "test_text = test_data.data\n",
    "target = dataset.target\n",
    "test_target = test_data.target\n",
    "print(raw_text[500],dataset.target_names[target[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "- split metadata and text.\n",
    "- For metadata\n",
    "\n",
    "    - Pick only Subject from metadata.\n",
    "\n",
    "- For text\n",
    "    1. split into sentences\n",
    "    2. lower case each sentence\n",
    "    3. tokenize into words\n",
    "    \n",
    "- Tokenizer: \n",
    "    1. delete email address\n",
    "    2. collection of numbers => NUM\n",
    "    2. keep \\$\n",
    "    3. delete all other punctuations\n",
    "\n",
    "- Maintain a word list for words that appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# split metadata and text\n",
    "def split_metadata(data):\n",
    "    occurrence = data.find(\"\\n\\n\")\n",
    "    metadata = data[0:occurrence] + \"\\n\"\n",
    "    text = data[occurrence+2:]\n",
    "    return metadata, text\n",
    "# get subject from metadata\n",
    "def get_subject(metadata):\n",
    "    # subject start with Subject: end with \\n\n",
    "    regex = 'Subject: (.*)\\n'\n",
    "    match = re.search(regex, metadata)\n",
    "    return match.group(1)\n",
    "# delete email address\n",
    "def del_email(text):\n",
    "    regex = '\\S*@\\S*'\n",
    "    return re.sub(regex, \" EMAIL \", text)\n",
    "# replace number collection\n",
    "def replace_num(text):\n",
    "    regex = '[0-9]+'\n",
    "    return re.sub(regex, \" NUM \", text)\n",
    "# remove special characters\n",
    "def remove(text):\n",
    "    regex = '[^\\w\\s$]'\n",
    "    return re.sub(regex,\" \", text)\n",
    "# process raw text\n",
    "def process_text(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in word_tokenize(remove(replace_num(del_email(text.lower()))))\\\n",
    "            if word not in stop_words]\n",
    "\n",
    "def process_data(data):\n",
    "\n",
    "    metadata, text = split_metadata(data)\n",
    "    \n",
    "    subject = get_subject(metadata)\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for sent in sent_tokenize(text):\n",
    "        tokenized_text += process(sent)\n",
    "        \n",
    "    tokenized_subject = []\n",
    "    for sent in sent_tokenize(subject):\n",
    "        tokenized_subject += process(sent)\n",
    "    return tokenized_subject, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car']\n",
      "['fair', 'number', 'brave', 'soul', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experience', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'NUM', 'NUM', 'NUM', 'floppy', 'especially', 'requested', 'summarizing', 'next', 'two', 'day', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'EMAIL']\n"
     ]
    }
   ],
   "source": [
    "tokenized_subject = []\n",
    "tokenized_text = []\n",
    "for data in raw_text:\n",
    "    subject, text = process_data(data)\n",
    "    tokenized_subject.append(subject)\n",
    "    tokenized_text.append(text)\n",
    "\n",
    "print(tokenized_subject[0])\n",
    "print(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'info', 'NUM', 'NUM', 'bonneville']\n",
      "['familiar', 'format', 'x', 'face', 'thingies', 'seeing', 'folk', 'header', 'got', 'see', 'maybe', 'make', 'one', 'got', 'dpg', 'view', 'linux', 'box', 'display', 'uncompressed', 'x', 'face', 'managed', 'compile', 'un', 'compface', 'looking', 'seem', 'find', 'x', 'face', 'anyones', 'news', 'header', 'could', 'would', 'please', 'send', 'x', 'face', 'header', 'know', 'probably', 'get', 'little', 'swamped', 'handle', 'hope', 'rick', 'miller', 'EMAIL', 'EMAIL', 'ricxjo', 'muelisto', 'send', 'postcard', 'get', 'one', 'back', 'enposxtigu', 'bildkarton', 'kaj', 'vi', 'ricevos', 'alion', 'rick', 'miller', 'NUM', 'wood', 'muskego', 'wi', 'NUM', 'usa']\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_subject = []\n",
    "tokenized_test_text = []\n",
    "for data in test_text:\n",
    "    subject, text = process_data(data)\n",
    "    tokenized_test_subject.append(subject)\n",
    "    tokenized_test_text.append(text)\n",
    "\n",
    "print(tokenized_test_subject[0])\n",
    "print(tokenized_test_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(tokenized_text)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "id2token = {dictionary.token2id[key]:key for key in dictionary.token2id}\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOPIC 0\n",
      "key 0.027637761\n",
      "phone 0.00797428\n",
      "pgp 0.0062342826\n",
      "get 0.006120797\n",
      "division 0.0056556957\n",
      "chip 0.0054952595\n",
      "know 0.0052064387\n",
      "bit 0.0050913943\n",
      "rsa 0.005068707\n",
      "session 0.004716287\n",
      "\n",
      "TOPIC 1\n",
      "israel 0.01053801\n",
      "law 0.009119934\n",
      "one 0.008596661\n",
      "people 0.008404893\n",
      "israeli 0.007079925\n",
      "jew 0.0064443494\n",
      "food 0.0052900016\n",
      "msg 0.0051528616\n",
      "state 0.005066714\n",
      "article 0.004532835\n",
      "\n",
      "TOPIC 2\n",
      "would 0.008945552\n",
      "article 0.007668558\n",
      "get 0.0071309935\n",
      "one 0.006503978\n",
      "like 0.0060583386\n",
      "time 0.005537521\n",
      "use 0.0044212183\n",
      "think 0.004137352\n",
      "make 0.0036717732\n",
      "people 0.003663801\n",
      "\n",
      "TOPIC 3\n",
      "people 0.012566865\n",
      "militia 0.008943163\n",
      "amendment 0.008261151\n",
      "right 0.008076568\n",
      "bear 0.007920167\n",
      "state 0.00783085\n",
      "article 0.007772535\n",
      "arm 0.0073593035\n",
      "would 0.0071598855\n",
      "second 0.006968814\n",
      "\n",
      "TOPIC 4\n",
      "ax 0.599545\n",
      "max 0.043476943\n",
      "f 0.03361659\n",
      "q 0.03059271\n",
      "$ 0.02973423\n",
      "g 0.029133486\n",
      "v 0.025313847\n",
      "r 0.013342343\n",
      "p 0.013195112\n",
      "b 0.01280684\n",
      "\n",
      "TOPIC 5\n",
      "leaf 0.007704196\n",
      "player 0.0068196985\n",
      "$ 0.006501871\n",
      "article 0.0060061654\n",
      "would 0.005703623\n",
      "get 0.0052672504\n",
      "wing 0.0051358063\n",
      "gilmour 0.0044979234\n",
      "francis 0.0042141085\n",
      "jagr 0.0041947733\n",
      "\n",
      "TOPIC 6\n",
      "one 0.008390104\n",
      "would 0.0070853597\n",
      "article 0.0064002145\n",
      "thing 0.005947193\n",
      "better 0.0056688506\n",
      "think 0.0053906976\n",
      "greek 0.00538748\n",
      "like 0.0051493845\n",
      "much 0.004845643\n",
      "see 0.0043212348\n",
      "\n",
      "TOPIC 7\n",
      "turkish 0.008125059\n",
      "armenian 0.005467035\n",
      "year 0.004780242\n",
      "article 0.0044961157\n",
      "center 0.0044460036\n",
      "people 0.0044304\n",
      "government 0.0043692156\n",
      "one 0.004213483\n",
      "turkey 0.0041268035\n",
      "april 0.0040814243\n",
      "\n",
      "TOPIC 8\n",
      "x 0.13239743\n",
      "entry 0.018470285\n",
      "c 0.010923114\n",
      "file 0.010339974\n",
      "program 0.009656353\n",
      "n 0.008703309\n",
      "section 0.00808352\n",
      "$ 0.0065137255\n",
      "output 0.005148732\n",
      "rule 0.004934716\n",
      "\n",
      "TOPIC 9\n",
      "c 0.051437147\n",
      "image 0.03474099\n",
      "jpeg 0.01540257\n",
      "color 0.013325781\n",
      "file 0.009134996\n",
      "graphic 0.008965742\n",
      "source 0.0071923584\n",
      "format 0.0068781846\n",
      "package 0.006268883\n",
      "code 0.0060648345\n",
      "\n",
      "TOPIC 10\n",
      "gun 0.015414555\n",
      "weapon 0.0134223625\n",
      "would 0.0115147745\n",
      "think 0.008498641\n",
      "article 0.007043788\n",
      "crime 0.0056238486\n",
      "one 0.005452358\n",
      "much 0.0049817394\n",
      "rate 0.0049300017\n",
      "like 0.0047299555\n",
      "\n",
      "TOPIC 11\n",
      "game 0.014417917\n",
      "team 0.010783665\n",
      "year 0.010496014\n",
      "article 0.006105458\n",
      "season 0.006042514\n",
      "insurance 0.005281555\n",
      "get 0.004775442\n",
      "health 0.004729386\n",
      "would 0.0046558254\n",
      "medical 0.0043575903\n",
      "\n",
      "TOPIC 12\n",
      "president 0.019297183\n",
      "q 0.012894633\n",
      "u 0.009472225\n",
      "bill 0.0077340417\n",
      "government 0.0065688905\n",
      "bush 0.0063511827\n",
      "state 0.00613668\n",
      "year 0.006130627\n",
      "congress 0.0060869195\n",
      "senator 0.0059297197\n",
      "\n",
      "TOPIC 13\n",
      "god 0.030024225\n",
      "one 0.011199437\n",
      "jesus 0.010003437\n",
      "christian 0.009304914\n",
      "bible 0.008379419\n",
      "would 0.007777579\n",
      "say 0.0073766317\n",
      "believe 0.0071045537\n",
      "belief 0.006099048\n",
      "think 0.005866856\n",
      "\n",
      "TOPIC 14\n",
      "__ 0.037131757\n",
      "scsi 0.028970785\n",
      "w 0.020576134\n",
      "___ 0.017689921\n",
      "event 0.016568\n",
      "widget 0.015856447\n",
      "_____ 0.014478397\n",
      "$ 0.012246368\n",
      "mb 0.0099321185\n",
      "id 0.009663514\n",
      "\n",
      "TOPIC 15\n",
      "file 0.013257046\n",
      "window 0.007837527\n",
      "server 0.007529354\n",
      "ftp 0.0071285786\n",
      "mail 0.007091845\n",
      "r 0.006891489\n",
      "edu 0.006652501\n",
      "version 0.006648548\n",
      "please 0.005835324\n",
      "available 0.0057283835\n",
      "\n",
      "TOPIC 16\n",
      "people 0.012465984\n",
      "would 0.009789792\n",
      "article 0.008510871\n",
      "one 0.00789019\n",
      "know 0.0071133864\n",
      "time 0.006109689\n",
      "like 0.0058722096\n",
      "thing 0.005795136\n",
      "never 0.0055228565\n",
      "want 0.0050634043\n",
      "\n",
      "TOPIC 17\n",
      "w 0.030971177\n",
      "$ 0.014659557\n",
      "ucs 0.009681026\n",
      "hostname 0.0074701416\n",
      "u 0.006984432\n",
      "v 0.006754786\n",
      "p 0.0055576256\n",
      "mc_ 0.005511512\n",
      "cbn 0.005232631\n",
      "cub 0.0052014114\n",
      "\n",
      "TOPIC 18\n",
      "key 0.013847679\n",
      "one 0.008298035\n",
      "use 0.007556453\n",
      "system 0.007458729\n",
      "would 0.0073429258\n",
      "chip 0.006650604\n",
      "encryption 0.005666279\n",
      "information 0.004931326\n",
      "ripem 0.0046130954\n",
      "security 0.0045715477\n",
      "\n",
      "TOPIC 19\n",
      "driver 0.01602317\n",
      "article 0.008404126\n",
      "one 0.007953981\n",
      "card 0.007871505\n",
      "would 0.007455543\n",
      "com 0.007289336\n",
      "anyone 0.0070543815\n",
      "know 0.006435394\n",
      "get 0.0063364417\n",
      "car 0.0063032843\n",
      "\n",
      "TOPIC 20\n",
      "gun 0.019062078\n",
      "firearm 0.00966699\n",
      "mr 0.008709097\n",
      "stephanopoulos 0.008352848\n",
      "people 0.008237535\n",
      "would 0.007734889\n",
      "q 0.007324026\n",
      "u 0.0064054704\n",
      "right 0.0058363574\n",
      "one 0.0058087073\n",
      "\n",
      "TOPIC 21\n",
      "people 0.011513219\n",
      "armenian 0.009192021\n",
      "said 0.009011008\n",
      "one 0.008127059\n",
      "u 0.007687912\n",
      "know 0.006988927\n",
      "go 0.005404896\n",
      "could 0.004655477\n",
      "would 0.0044115516\n",
      "school 0.0043827137\n",
      "\n",
      "TOPIC 22\n",
      "$ 0.17757493\n",
      "st 0.008862779\n",
      "son 0.00850924\n",
      "cover 0.008278178\n",
      "copy 0.0075826803\n",
      "father 0.0065452303\n",
      "man 0.0063721547\n",
      "game 0.006060173\n",
      "art 0.0055826493\n",
      "new 0.005576442\n",
      "\n",
      "TOPIC 23\n",
      "israeli 0.0115945395\n",
      "israel 0.010430284\n",
      "arab 0.009558972\n",
      "palestinian 0.007175681\n",
      "jew 0.007158683\n",
      "right 0.007138597\n",
      "state 0.0066353898\n",
      "war 0.006335275\n",
      "people 0.0060490295\n",
      "human 0.004621368\n",
      "\n",
      "TOPIC 24\n",
      "x 0.018359115\n",
      "window 0.015364972\n",
      "use 0.009187087\n",
      "card 0.008807369\n",
      "bit 0.008632059\n",
      "problem 0.0086186845\n",
      "file 0.007840212\n",
      "using 0.0064241407\n",
      "color 0.0060189264\n",
      "one 0.0058929254\n",
      "\n",
      "TOPIC 25\n",
      "w 0.102119476\n",
      "$ 0.02166544\n",
      "min 0.017349428\n",
      "gm 0.013245096\n",
      "v 0.011917788\n",
      "u 0.011607194\n",
      "ww 0.0101970835\n",
      "x 0.0100598885\n",
      "q 0.009910102\n",
      "p 0.008989137\n",
      "\n",
      "TOPIC 26\n",
      "would 0.008973024\n",
      "article 0.0085924715\n",
      "year 0.008236873\n",
      "like 0.007470906\n",
      "think 0.0068044425\n",
      "good 0.006758518\n",
      "one 0.006222176\n",
      "game 0.0057966835\n",
      "team 0.0051940214\n",
      "get 0.0048606307\n",
      "\n",
      "TOPIC 27\n",
      "space 0.022370107\n",
      "detector 0.009994612\n",
      "satellite 0.009800092\n",
      "radar 0.009032188\n",
      "nasa 0.008350685\n",
      "visual 0.0076316395\n",
      "shuttle 0.007625106\n",
      "mar 0.005168916\n",
      "comic 0.005109602\n",
      "orbit 0.004758749\n",
      "\n",
      "TOPIC 28\n",
      "drive 0.015834205\n",
      "$ 0.008214284\n",
      "system 0.007189685\n",
      "disk 0.0070907054\n",
      "get 0.0058438443\n",
      "one 0.0057970914\n",
      "like 0.0057015247\n",
      "thanks 0.005460148\n",
      "data 0.0052945185\n",
      "work 0.00528681\n",
      "\n",
      "TOPIC 29\n",
      "$ 0.060111467\n",
      "_ 0.049157448\n",
      "w 0.046125527\n",
      "r 0.0459895\n",
      "p 0.041353527\n",
      "g 0.04120192\n",
      "u 0.03460507\n",
      "c 0.029194316\n",
      "q 0.027608555\n",
      "x 0.025837135\n",
      "\n",
      "TOPIC 30\n",
      "v 0.021021709\n",
      "window 0.014317894\n",
      "la 0.013589837\n",
      "printer 0.011934826\n",
      "win 0.01002335\n",
      "det 0.009640642\n",
      "bos 0.009555012\n",
      "cal 0.009433731\n",
      "tor 0.0085244\n",
      "van 0.008515957\n",
      "\n",
      "TOPIC 31\n",
      "would 0.013861534\n",
      "captain 0.009654687\n",
      "article 0.007700704\n",
      "u 0.006836734\n",
      "traded 0.0064850617\n",
      "get 0.0049591623\n",
      "ticket 0.004522231\n",
      "know 0.0041295527\n",
      "sky 0.0039684633\n",
      "time 0.0038492086\n",
      "\n",
      "TOPIC 32\n",
      "people 0.008749509\n",
      "armenian 0.007151049\n",
      "turk 0.006805467\n",
      "_ 0.00662605\n",
      "one 0.006580155\n",
      "like 0.0048377705\n",
      "fpu 0.004553819\n",
      "would 0.004243966\n",
      "know 0.004150167\n",
      "turkish 0.0040538134\n",
      "\n",
      "TOPIC 33\n",
      "would 0.0109100165\n",
      "question 0.0062072775\n",
      "file 0.0052132276\n",
      "know 0.005185433\n",
      "audio 0.0048577725\n",
      "like 0.004833647\n",
      "package 0.004611066\n",
      "system 0.004483709\n",
      "one 0.0044505177\n",
      "mail 0.004396579\n",
      "\n",
      "TOPIC 34\n",
      "right 0.009073165\n",
      "people 0.009010598\n",
      "one 0.008538161\n",
      "would 0.0074247965\n",
      "law 0.007135477\n",
      "article 0.006446888\n",
      "u 0.0058981534\n",
      "say 0.005519759\n",
      "word 0.0047955746\n",
      "mean 0.0046868385\n",
      "\n",
      "TOPIC 35\n",
      "one 0.011413558\n",
      "team 0.007866817\n",
      "car 0.007761014\n",
      "year 0.00699998\n",
      "time 0.006718775\n",
      "go 0.0054743947\n",
      "back 0.0049861027\n",
      "like 0.004828136\n",
      "know 0.0047707045\n",
      "think 0.0042353743\n",
      "\n",
      "TOPIC 36\n",
      "would 0.007763848\n",
      "u 0.006983609\n",
      "system 0.006951791\n",
      "one 0.005836806\n",
      "use 0.005105529\n",
      "technology 0.0050626975\n",
      "law 0.0050179595\n",
      "government 0.0048724897\n",
      "chip 0.0048663146\n",
      "accelerator 0.004628409\n",
      "\n",
      "TOPIC 37\n",
      "ax 0.08614574\n",
      "$ 0.021180501\n",
      "f 0.020585807\n",
      "georgia 0.017342215\n",
      "rockefeller 0.013477089\n",
      "q 0.011776501\n",
      "p 0.011027139\n",
      "b 0.010576263\n",
      "g 0.010419609\n",
      "ghost 0.010201222\n",
      "\n",
      "TOPIC 38\n",
      "bike 0.015127464\n",
      "article 0.010215619\n",
      "dod 0.0076569575\n",
      "$ 0.006117793\n",
      "rider 0.006022423\n",
      "would 0.0058324416\n",
      "new 0.0056219497\n",
      "car 0.0053106053\n",
      "make 0.005166394\n",
      "ride 0.005151803\n",
      "\n",
      "TOPIC 39\n",
      "pt 0.035343207\n",
      "chicago 0.012592223\n",
      "period 0.011702728\n",
      "new 0.01161457\n",
      "detroit 0.0115428865\n",
      "ranger 0.011099907\n",
      "pittsburgh 0.01103908\n",
      "play 0.011022849\n",
      "ax 0.010709749\n",
      "st 0.010040654\n"
     ]
    }
   ],
   "source": [
    "for topicid in range(40):\n",
    "    words = lda.get_topic_terms(topicid, 10)\n",
    "    print(\"\\nTOPIC\", topicid)\n",
    "    for word in words:\n",
    "        print(id2token[word[0]], word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [dictionary.doc2bow(text) for text in tokenized_test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.4316437), (28, 0.46394077), (30, 0.08185455)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(test_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_training_feature = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
