{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups()\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bjorndahl@augustana.ab.ca\n",
      "Subject: Re: document of .RTF\n",
      "Organization: Augustana University College, Camrose, Alberta\n",
      "Lines: 10\n",
      "\n",
      "In article <1993Mar30.113436.7339@worak.kaist.ac.kr>, tjyu@eve.kaist.ac.kr (Yu TaiJung) writes:\n",
      "> Does anybody have document of .RTF file or know where I can get it?\n",
      "> \n",
      "> Thanks in advance. :)\n",
      "\n",
      "I got one from Microsoft tech support.\n",
      "\n",
      "-- \n",
      "Sterling G. Bjorndahl, bjorndahl@Augustana.AB.CA or bjorndahl@camrose.uucp\n",
      "Augustana University College, Camrose, Alberta, Canada      (403) 679-1100\n",
      " comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "raw_text = dataset.data\n",
    "target = dataset.target\n",
    "print(raw_text[500],dataset.target_names[target[500]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "- split metadata and text.\n",
    "- For metadata\n",
    "\n",
    "    - Pick only Subject from metadata.\n",
    "\n",
    "- For text\n",
    "    1. split into sentences\n",
    "    2. lower case each sentence\n",
    "    3. tokenize into words\n",
    "    \n",
    "- Tokenizer: \n",
    "    1. delete email address\n",
    "    2. collection of numbers => NUM\n",
    "    2. keep \\$\n",
    "    3. delete all other punctuations\n",
    "\n",
    "- Maintain a word list for words that appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "# split metadata and text\n",
    "def split_metadata(data):\n",
    "    occurrence = data.find(\"\\n\\n\")\n",
    "    metadata = data[0:occurrence] + \"\\n\"\n",
    "    text = data[occurrence+2:]\n",
    "    return metadata, text\n",
    "# get subject from metadata\n",
    "def get_subject(metadata):\n",
    "    # subject start with Subject: end with \\n\n",
    "    regex = 'Subject: (.*)\\n'\n",
    "    match = re.search(regex, metadata)\n",
    "    return match.group(1)\n",
    "# delete email address\n",
    "def del_email(text):\n",
    "    regex = '\\S*@\\S*'\n",
    "    return re.sub(regex, \" EMAIL \", text)\n",
    "# replace number collection\n",
    "def replace_num(text):\n",
    "    regex = '[0-9]+'\n",
    "    return re.sub(regex, \" NUM \", text)\n",
    "# remove special characters\n",
    "def remove(text):\n",
    "    regex = '[^\\w\\s$]'\n",
    "    return re.sub(regex,\" \", text)\n",
    "# process raw text\n",
    "def process_text(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in word_tokenize(remove(replace_num(del_email(text.lower()))))\\\n",
    "            if word not in stop_words]\n",
    "\n",
    "def process_data(data):\n",
    "    metadata, text = split_metadata(data)\n",
    "    subject = get_subject(metadata)\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for sent in sent_tokenize(text):\n",
    "        tokenized_text += process(sent)\n",
    "        \n",
    "    tokenized_subject = []\n",
    "    for sent in sent_tokenize(subject):\n",
    "        tokenized_subject += process(sent)\n",
    "    return tokenized_subject, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['si', 'clock', 'poll', 'final', 'call'], ['fair', 'number', 'brave', 'soul', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experience', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'NUM', 'NUM', 'NUM', 'floppy', 'especially', 'requested', 'summarizing', 'next', 'two', 'day', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'EMAIL'])\n"
     ]
    }
   ],
   "source": [
    "print(process_data(raw_text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car']\n",
      "['fair', 'number', 'brave', 'soul', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experience', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'NUM', 'NUM', 'NUM', 'floppy', 'especially', 'requested', 'summarizing', 'next', 'two', 'day', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'EMAIL']\n"
     ]
    }
   ],
   "source": [
    "tokenized_subject = []\n",
    "tokenized_text = []\n",
    "for data in raw_text:\n",
    "    subject, text = process_data(data)\n",
    "    tokenized_subject.append(subject)\n",
    "    tokenized_text.append(text)\n",
    "\n",
    "print(tokenized_subject[0])\n",
    "print(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeinzed_test_subject = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self):\n",
    "        self.word2index = {'NUM':0, 'EMAIL':1}\n",
    "        self.index2word = {0: 'NUM', 1: 'EMAIL'}\n",
    "        self.word2count = {'NUM':0, 'EMAIL':0}\n",
    "        self.vocab_count = 2\n",
    "    def addWord(self, word):\n",
    "        if word in self.word2index:\n",
    "            self.word2count[word] += 1\n",
    "        else:\n",
    "            self.word2index[word] = self.vocab_count\n",
    "            self.index2word[self.vocab_count] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.vocab_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = Language()\n",
    "for subject in tokenized_subject:\n",
    "    for word in subject:\n",
    "        language.addWord(word)\n",
    "for text in tokenized_text:\n",
    "    for sent in text:\n",
    "        for word in sent:\n",
    "            language.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(tokenized_text)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "id2token = {dictionary.token2id[key]:key for key in dictionary.token2id}\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "#lda = LdaModel(common_corpus, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'NUM', 'door', 'sport', 'car', 'looked', 'late', 'NUM', 'early', 'NUM', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'e', 'mail', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOPIC 0\n",
      "law 0.006431582\n",
      "people 0.0063895327\n",
      "president 0.006371348\n",
      "would 0.0062407856\n",
      "mr 0.005817471\n",
      "homosexual 0.00553603\n",
      "gay 0.0047088563\n",
      "american 0.004490718\n",
      "one 0.004413492\n",
      "u 0.004280061\n",
      "\n",
      "TOPIC 1\n",
      "god 0.017337106\n",
      "would 0.010128813\n",
      "people 0.009740007\n",
      "one 0.009711972\n",
      "say 0.0076309047\n",
      "jesus 0.0071131466\n",
      "think 0.0069431816\n",
      "christian 0.0066268863\n",
      "time 0.005566497\n",
      "article 0.0053566946\n",
      "\n",
      "TOPIC 2\n",
      "space 0.0144479405\n",
      "would 0.008123416\n",
      "one 0.006169648\n",
      "article 0.005750418\n",
      "year 0.005576694\n",
      "moon 0.004814346\n",
      "earth 0.0047900146\n",
      "disease 0.0043016686\n",
      "launch 0.00424096\n",
      "time 0.004181001\n",
      "\n",
      "TOPIC 3\n",
      "motif 0.01399266\n",
      "software 0.010708763\n",
      "version 0.008588853\n",
      "r 0.0073932535\n",
      "type 0.0059639467\n",
      "graphic 0.0057806023\n",
      "window 0.005261542\n",
      "anyone 0.0048559792\n",
      "run 0.004808031\n",
      "sun 0.0046218676\n",
      "\n",
      "TOPIC 4\n",
      "orbit 0.010910105\n",
      "mission 0.010855371\n",
      "db 0.01016462\n",
      "space 0.00946086\n",
      "nasa 0.008483583\n",
      "spacecraft 0.008471987\n",
      "hawk 0.007453851\n",
      "data 0.006903461\n",
      "probe 0.0068511157\n",
      "b 0.0063499\n",
      "\n",
      "TOPIC 5\n",
      "scsi 0.04312879\n",
      "ide 0.018520363\n",
      "drive 0.015457402\n",
      "mb 0.012976202\n",
      "article 0.008416951\n",
      "$ 0.0075424835\n",
      "bit 0.007502598\n",
      "controller 0.007137423\n",
      "o 0.006730257\n",
      "sec 0.006607467\n",
      "\n",
      "TOPIC 6\n",
      "$ 0.020291915\n",
      "car 0.00895962\n",
      "e 0.0066411784\n",
      "article 0.0060296264\n",
      "x 0.00537838\n",
      "would 0.0046307216\n",
      "one 0.0044197543\n",
      "new 0.00423374\n",
      "c 0.0039967336\n",
      "get 0.0037395493\n",
      "\n",
      "TOPIC 7\n",
      "use 0.008358846\n",
      "file 0.008310926\n",
      "system 0.007398538\n",
      "encryption 0.007068383\n",
      "key 0.006946301\n",
      "gun 0.00618384\n",
      "would 0.0060973046\n",
      "one 0.0054594832\n",
      "public 0.004948455\n",
      "law 0.0048990934\n",
      "\n",
      "TOPIC 8\n",
      "drive 0.023755252\n",
      "one 0.008760091\n",
      "like 0.008001661\n",
      "keyboard 0.006573754\n",
      "lib 0.006504314\n",
      "use 0.005575426\n",
      "get 0.0054368447\n",
      "work 0.005277443\n",
      "also 0.005240683\n",
      "know 0.0048290878\n",
      "\n",
      "TOPIC 9\n",
      "one 0.011073976\n",
      "would 0.00859582\n",
      "atheist 0.006694164\n",
      "israel 0.005975629\n",
      "israeli 0.0055318717\n",
      "article 0.005498039\n",
      "jew 0.005277151\n",
      "question 0.0052441764\n",
      "god 0.0052143447\n",
      "arab 0.0049400376\n",
      "\n",
      "TOPIC 10\n",
      "key 0.02026985\n",
      "window 0.0092469305\n",
      "ripem 0.0076364204\n",
      "bit 0.007322021\n",
      "one 0.0070899115\n",
      "get 0.0070071584\n",
      "would 0.0060553635\n",
      "use 0.0058868947\n",
      "like 0.0056083314\n",
      "using 0.0054235165\n",
      "\n",
      "TOPIC 11\n",
      "armenian 0.01852774\n",
      "jew 0.008332753\n",
      "nazi 0.0063065193\n",
      "one 0.0060394392\n",
      "armenia 0.005939099\n",
      "turkish 0.0057719564\n",
      "turkey 0.005663602\n",
      "turk 0.0055424976\n",
      "german 0.0053808754\n",
      "jewish 0.0050067534\n",
      "\n",
      "TOPIC 12\n",
      "$ 0.09114245\n",
      "sale 0.0084619885\n",
      "mouse 0.00842122\n",
      "price 0.008274534\n",
      "software 0.006681292\n",
      "offer 0.006524742\n",
      "new 0.006213931\n",
      "shipping 0.006180459\n",
      "also 0.005176075\n",
      "university 0.0047157025\n",
      "\n",
      "TOPIC 13\n",
      "pt 0.026379626\n",
      "run 0.02026407\n",
      "game 0.011680533\n",
      "alomar 0.008711441\n",
      "team 0.008468275\n",
      "la 0.008431666\n",
      "score 0.008283239\n",
      "chicago 0.00748845\n",
      "play 0.0071185254\n",
      "rbi 0.006726387\n",
      "\n",
      "TOPIC 14\n",
      "cd 0.014786404\n",
      "mhz 0.010466049\n",
      "would 0.009567427\n",
      "system 0.009554133\n",
      "cpu 0.0077053993\n",
      "thanks 0.006668767\n",
      "clock 0.006317016\n",
      "anyone 0.0061152964\n",
      "chip 0.0061100773\n",
      "work 0.005111792\n",
      "\n",
      "TOPIC 15\n",
      "w 0.034963656\n",
      "mat 0.025558963\n",
      "heretic 0.0114921145\n",
      "u 0.010870506\n",
      "deg 0.009042016\n",
      "chemistry 0.008956821\n",
      "avg 0.0073279915\n",
      "axis 0.007222992\n",
      "forgiving 0.007121948\n",
      "paranoia 0.0063961167\n",
      "\n",
      "TOPIC 16\n",
      "water 0.012569212\n",
      "san 0.0071182544\n",
      "$ 0.005515574\n",
      "one 0.0054283184\n",
      "ott 0.0051394985\n",
      "year 0.0046621487\n",
      "m 0.0041857935\n",
      "mask 0.0040961523\n",
      "article 0.0040029855\n",
      "go 0.0037445975\n",
      "\n",
      "TOPIC 17\n",
      "drug 0.010362806\n",
      "article 0.008591573\n",
      "people 0.00672708\n",
      "would 0.005687588\n",
      "state 0.004965541\n",
      "u 0.004773191\n",
      "p 0.0040431935\n",
      "c 0.0036060477\n",
      "day 0.0032158452\n",
      "health 0.0031284115\n",
      "\n",
      "TOPIC 18\n",
      "_ 0.18210581\n",
      "__ 0.05168459\n",
      "___ 0.047707662\n",
      "helmet 0.013547906\n",
      "_____ 0.009038514\n",
      "____ 0.008811747\n",
      "article 0.0072773765\n",
      "pa 0.0054864157\n",
      "like 0.0049383044\n",
      "would 0.0044789044\n",
      "\n",
      "TOPIC 19\n",
      "w 0.06378094\n",
      "$ 0.06053639\n",
      "r 0.046098895\n",
      "p 0.039745584\n",
      "c 0.036756914\n",
      "g 0.035753615\n",
      "u 0.03465909\n",
      "_ 0.034376938\n",
      "x 0.030022316\n",
      "k 0.02503931\n",
      "\n",
      "TOPIC 20\n",
      "gm 0.033781156\n",
      "government 0.013726211\n",
      "ax 0.010108405\n",
      "libertarian 0.009139808\n",
      "would 0.008370444\n",
      "trial 0.0075958115\n",
      "new 0.0075843097\n",
      "article 0.0060537825\n",
      "loser 0.0059259213\n",
      "occupant 0.0046868236\n",
      "\n",
      "TOPIC 21\n",
      "one 0.007868668\n",
      "article 0.00526431\n",
      "also 0.0051857503\n",
      "mormon 0.0049576196\n",
      "echo 0.004808762\n",
      "book 0.004706541\n",
      "e 0.004349855\n",
      "right 0.0037204362\n",
      "first 0.0035320576\n",
      "spirit 0.0034535239\n",
      "\n",
      "TOPIC 22\n",
      "car 0.009921453\n",
      "like 0.008459132\n",
      "one 0.008116705\n",
      "would 0.0076069017\n",
      "article 0.0062966943\n",
      "year 0.0062816\n",
      "good 0.0062496657\n",
      "get 0.005983988\n",
      "bike 0.0058787744\n",
      "much 0.005469834\n",
      "\n",
      "TOPIC 23\n",
      "one 0.010951542\n",
      "cub 0.009357554\n",
      "article 0.009237596\n",
      "maria 0.0065247193\n",
      "like 0.005168693\n",
      "min 0.004757964\n",
      "university 0.004282159\n",
      "georgia 0.0042337193\n",
      "also 0.0039935214\n",
      "current 0.0038891651\n",
      "\n",
      "TOPIC 24\n",
      "ax 0.50706005\n",
      "q 0.037196085\n",
      "max 0.03678751\n",
      "f 0.035335816\n",
      "g 0.034811433\n",
      "$ 0.033691514\n",
      "v 0.024916086\n",
      "p 0.017835548\n",
      "r 0.01684\n",
      "b 0.01654017\n",
      "\n",
      "TOPIC 25\n",
      "file 0.023596369\n",
      "widget 0.01597344\n",
      "edu 0.012675973\n",
      "r 0.012623512\n",
      "x 0.011006643\n",
      "window 0.0108158\n",
      "ftp 0.009997073\n",
      "resource 0.009046023\n",
      "code 0.0077863503\n",
      "application 0.0076257368\n",
      "\n",
      "TOPIC 26\n",
      "card 0.014356261\n",
      "problem 0.011602073\n",
      "system 0.008761329\n",
      "drive 0.008559627\n",
      "disk 0.008451683\n",
      "thanks 0.008362454\n",
      "driver 0.008221496\n",
      "know 0.008023985\n",
      "mb 0.007808385\n",
      "one 0.0077300905\n",
      "\n",
      "TOPIC 27\n",
      "one 0.006699961\n",
      "may 0.0066856155\n",
      "article 0.0055715297\n",
      "weight 0.005136635\n",
      "francis 0.0049170996\n",
      "presentation 0.004879118\n",
      "ticket 0.0046871123\n",
      "beauchaine 0.004376602\n",
      "u 0.004338337\n",
      "code 0.0041116453\n",
      "\n",
      "TOPIC 28\n",
      "window 0.022556825\n",
      "new 0.009679279\n",
      "event 0.008792688\n",
      "expose 0.0072850874\n",
      "limbaugh 0.0070544896\n",
      "xlib 0.0069609485\n",
      "cursor 0.00684739\n",
      "ctrl 0.0066306544\n",
      "r 0.0061199367\n",
      "handler 0.0059884405\n",
      "\n",
      "TOPIC 29\n",
      "captain 0.018081022\n",
      "music 0.010543803\n",
      "traded 0.008430035\n",
      "would 0.0061643715\n",
      "go 0.0061069424\n",
      "godhead 0.00600744\n",
      "son 0.005939814\n",
      "receiver 0.005802874\n",
      "p 0.0055924496\n",
      "one 0.005190008\n",
      "\n",
      "TOPIC 30\n",
      "turkish 0.008160103\n",
      "center 0.00648262\n",
      "article 0.006052205\n",
      "david 0.005668887\n",
      "armenian 0.0052314554\n",
      "page 0.0051014246\n",
      "plane 0.0049859467\n",
      "japanese 0.004611007\n",
      "point 0.004607856\n",
      "science 0.004599951\n",
      "\n",
      "TOPIC 31\n",
      "one 0.008831962\n",
      "would 0.0079570245\n",
      "like 0.006434288\n",
      "article 0.006341759\n",
      "say 0.006195432\n",
      "said 0.0055647837\n",
      "know 0.005354489\n",
      "get 0.005303326\n",
      "going 0.005028798\n",
      "think 0.004986069\n",
      "\n",
      "TOPIC 32\n",
      "x 0.12300254\n",
      "file 0.016397301\n",
      "entry 0.016039498\n",
      "image 0.01360235\n",
      "program 0.011867186\n",
      "c 0.0073462026\n",
      "use 0.006917062\n",
      "section 0.0066792527\n",
      "line 0.0064014187\n",
      "source 0.0061058085\n",
      "\n",
      "TOPIC 33\n",
      "people 0.008675428\n",
      "one 0.008599743\n",
      "would 0.007399488\n",
      "think 0.006913318\n",
      "article 0.00612708\n",
      "know 0.0054273084\n",
      "like 0.005256259\n",
      "right 0.0044535506\n",
      "question 0.0038727531\n",
      "also 0.0038315759\n",
      "\n",
      "TOPIC 34\n",
      "team 0.019830327\n",
      "game 0.018732354\n",
      "player 0.010611195\n",
      "year 0.009688202\n",
      "hockey 0.009457442\n",
      "season 0.007832025\n",
      "play 0.0066639204\n",
      "win 0.006255464\n",
      "league 0.0059596817\n",
      "fan 0.005727578\n",
      "\n",
      "TOPIC 35\n",
      "food 0.01375463\n",
      "msg 0.010355656\n",
      "would 0.008948586\n",
      "never 0.0077350326\n",
      "one 0.0066146054\n",
      "article 0.006168239\n",
      "time 0.005315503\n",
      "people 0.0048509794\n",
      "chinese 0.0043359236\n",
      "day 0.0036397104\n",
      "\n",
      "TOPIC 36\n",
      "ax 0.036552057\n",
      "centaur 0.010779028\n",
      "f 0.010335758\n",
      "proton 0.008412425\n",
      "fixture 0.007920827\n",
      "knife 0.0071558906\n",
      "josephus 0.006769583\n",
      "u 0.0061686044\n",
      "b 0.006060258\n",
      "q 0.005882744\n",
      "\n",
      "TOPIC 37\n",
      "people 0.014351076\n",
      "greek 0.007614026\n",
      "war 0.0070943143\n",
      "would 0.006908788\n",
      "right 0.0065592206\n",
      "u 0.00637223\n",
      "one 0.0063470034\n",
      "mr 0.006100041\n",
      "think 0.00542833\n",
      "said 0.0049663405\n",
      "\n",
      "TOPIC 38\n",
      "israel 0.008074119\n",
      "state 0.007272944\n",
      "israeli 0.0061874664\n",
      "rate 0.005275636\n",
      "space 0.0047483896\n",
      "may 0.00425982\n",
      "gun 0.0041200817\n",
      "attack 0.0040661264\n",
      "available 0.0038392895\n",
      "also 0.0036259901\n",
      "\n",
      "TOPIC 39\n",
      "font 0.016732844\n",
      "printer 0.014935877\n",
      "key 0.014318891\n",
      "k 0.012684877\n",
      "e 0.012393827\n",
      "w 0.010454735\n",
      "n 0.009281353\n",
      "$ 0.0091392\n",
      "print 0.0076063126\n",
      "u 0.0074626263\n"
     ]
    }
   ],
   "source": [
    "for topicid in range(40):\n",
    "    words = lda.get_topic_terms(topicid, 10)\n",
    "    print(\"\\nTOPIC\", topicid)\n",
    "    for word in words:\n",
    "        print(id2token[word[0]], word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 4), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n",
      "drive\n",
      "scsi\n",
      "b\n",
      "mb\n",
      "ide\n",
      "controller\n",
      "jumper\n",
      "bus\n",
      "offer\n"
     ]
    }
   ],
   "source": [
    "words = lda.get_topic_terms(1, 10)\n",
    "for word in words:\n",
    "    print(id2token[word[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gun 0.021801889\n",
      "law 0.010995762\n",
      "government 0.009796374\n",
      "weapon 0.009748626\n",
      "state 0.009195521\n",
      "right 0.008972493\n",
      "would 0.008389489\n",
      "firearm 0.0075030485\n",
      "crime 0.007343184\n",
      "stephanopoulos 0.006955789\n"
     ]
    }
   ],
   "source": [
    "words = lda.get_topic_terms(5, 10)\n",
    "for word in words:\n",
    "    print(id2token[word[0]], word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
