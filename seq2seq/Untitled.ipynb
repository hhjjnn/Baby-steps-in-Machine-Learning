{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['elle est mannequin .', 'she s a model .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    # input shape: (B, 1)\n",
    "    # hidden shape: (B, hidden_size)\n",
    "    # output shape: (B, hidden_size)\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, device):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device=device\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # input shape: (B, hidden size)\n",
    "    # output shape: (B, hidden size)\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        #output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input and output vocabulary\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "hidden_size = 10\n",
    "# encoder decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for translation\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, device):\n",
    "        super(dataset, self).__init__()\n",
    "        self.data = pairs\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input, output = pairs[idx]\n",
    "        input_token = torch.tensor([input_lang.word2index[word] for word in input.split()] +\n",
    "                                   [EOS_token],\n",
    "                                   device=self.device)\n",
    "        output_token = torch.tensor([SOS_token] + \n",
    "                                    [output_lang.word2index[word] for word in output.split()] +\n",
    "                                    [EOS_token],\n",
    "                                    device=self.device)\n",
    "        return input_token, output_token\n",
    "    \n",
    "data = dataset(device)\n",
    "#dataloader = DataLoader(data,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"je suis mannequin\"\n",
    "input_token = torch.tensor([input_lang.word2index[word] for word in input_sentence.split()] +\n",
    "                                   [EOS_token],\n",
    "                                   device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq():\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size, self.device).to(self.device)\n",
    "        self.decoder = DecoderRNN(hidden_size,output_size, self.device).to(self.device)\n",
    "    def train(self, dataset, epochs=5):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        encoder_optimizer = torch.optim.SGD(self.encoder.parameters(), lr=0.01)\n",
    "        decoder_optimizer = torch.optim.SGD(self.decoder.parameters(), lr=0.01)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for token_fra, token_eng in dataset:\n",
    "                # start running encoder\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                \n",
    "                encoder_hidden = self.encoder.initHidden()\n",
    "                decoder_hidden = self.decoder.initHidden()\n",
    "                loss = 0\n",
    "                for token in token_fra:\n",
    "                    encoder_output, encoder_hidden = self.encoder.forward(token\n",
    "                                                                          ,encoder_hidden)\n",
    "\n",
    "                decoder_hidden = encoder_output\n",
    "                for index, token in enumerate(token_eng[0:len(token_eng)-1]):\n",
    "                    decoder_output, decoder_hidden = self.decoder.forward(token\n",
    "                                                                          ,decoder_hidden)\n",
    "                    loss += criterion(decoder_output, torch.tensor([token_eng[index+1]]))\n",
    "                \n",
    "                epoch_loss += loss\n",
    "                loss.backward()\n",
    "                decoder_optimizer.step()\n",
    "                encoder_optimizer.step()\n",
    "            print(\"epoch:\", epoch, epoch_loss/len(dataset))\n",
    "    # data is a encoded sentence like nn.tensor([13,24,56,...])\n",
    "    def predict(data):\n",
    "        with torch.no_grad():\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            loss = 0\n",
    "            for token in input_token:\n",
    "                encoder_output, encoder_hidden = self.encoder.forward(token\n",
    "                                                                ,encoder_hidden)\n",
    "            decoder_hidden = encoder_output\n",
    "            decoder_output = None\n",
    "            eng_output = \"SOS\"\n",
    "            decoder_token = torch.tensor([SOS_token])\n",
    "    \n",
    "            while eng_output != \"EOS\":\n",
    "                decoder_output,decoder_hidden = self.decoder.forward(decoder_token, decoder_hidden)\n",
    "                decoder_token = torch.argmax(decoder_output)\n",
    "                eng_output = output_lang.index2word[int(decoder_token.numpy())]\n",
    "                print(eng_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq(input_size=input_lang.n_words, hidden_size=5,output_size=output_lang.n_words, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(data,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6,  11, 746,   1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = model.encoder.initHidden()\n",
    "        loss = 0\n",
    "        for token in input_token:\n",
    "            encoder_output, encoder_hidden = model.encoder.forward(token\n",
    "                                                                ,encoder_hidden)\n",
    "        decoder_hidden = encoder_output\n",
    "        decoder_output = None\n",
    "        eng_output = \"SOS\"\n",
    "        decoder_token = torch.tensor([SOS_token])\n",
    "    \n",
    "        while eng_output != \"EOS\":\n",
    "            decoder_output,decoder_hidden = model.decoder.forward(decoder_token, decoder_hidden)\n",
    "            decoder_token = torch.argmax(decoder_output)\n",
    "            eng_output = output_lang.index2word[int(decoder_token.numpy())]\n",
    "            print(eng_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "m\n",
      "going\n",
      "to\n",
      "the\n",
      "the\n",
      "be\n",
      "you\n",
      ".\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "predict(input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
