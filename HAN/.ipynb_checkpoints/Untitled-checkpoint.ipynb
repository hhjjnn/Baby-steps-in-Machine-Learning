{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size=1000, hidden_dim=100, seq_length=50):\n",
    "        super(Word_Encoder, self).__init__()\n",
    "        self.word_encoder = tf.keras.layers.Embedding(vocab_size, hidden_dim,\n",
    "                                                       mask_zero=True, input_length=seq_length)\n",
    "        self.bidirectional_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=hidden_dim))\n",
    "        \n",
    "    def call(self, tokenized_sentence):\n",
    "        word_embedding = self.word_encoder(tokenized_sentence)\n",
    "        word_embedding = self.bidirectional_encoder(word_embedding)\n",
    "        return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Attention(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=100):\n",
    "        super(Word_Encoder, self).__init__()\n",
    "        self.MLP = tf.keras.layers.Dense(units=hidden_dim, use_bias=True)\n",
    "        self.word_attention = tf.keras.layers.Attention(use_scale=True)\n",
    "    def call(self, word_embedding):\n",
    "        word_embedding = self.MLP(word_embedding)\n",
    "        sentence_embedding = self.word_attention([word_embedding, word_embedding, word_embedding])\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed each document\n",
    "class Sentence_Encoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=100):\n",
    "        super(Sentence_Encoder, self).__init__()\n",
    "        self.bidirectional_encoder = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=hidden_dim))\n",
    "        self.MLP = tf.keras.layers.Dense(units=hidden_dim)\n",
    "        self.sentence_attention = tf.keras.layers.Attention(use_scale=True)\n",
    "    def call(self, sentence_embedding):\n",
    "        #num_sent, embed_dim = sentence_embedding.shape\n",
    "        #sentence_embedding = tf.reshape(sentence_embedding,shape=[1, num_sent, embed_dim])\n",
    "        sentence_embedding = self.bidirectional_encoder(sentence_embedding)\n",
    "        sentence_embedding = self.MLP(sentence_embedding)\n",
    "        sentence_embedding = self.sentence_attention([sentence_embedding,sentence_embedding,sentence_embedding])\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Attention(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=100):\n",
    "        super(Sentence_Encoder, self).__init__()\n",
    "        self.MLP = tf.keras.layers.Dense(units=hidden_dim)\n",
    "        self.sentence_attention = tf.keras.layers.Attention(use_scale=True)\n",
    "    def call(self, sentence_embedding):\n",
    "        sentence_embedding = self.MLP(encoding)\n",
    "        document_embedding = self.sentence_attention([encoding,encoding,encoding])\n",
    "        return document_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document_Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, sentence_len, hidden_dim):\n",
    "        super(Document_Encoder, self).__init__()\n",
    "        self.word_encoder = Word_Encoder(vocab_size, hidden_dim, sentence_len)\n",
    "        self.word_attention = Word_Attention(hidden_dim)\n",
    "        self.sentence_encoder = Sentence_Encoder(hidden_dim)\n",
    "        self.sentence_attention = Sentence_Attention(hidden_dim)\n",
    "    def call(self, tokenized_documents):\n",
    "        # input is (batch_size of documents, number of sentence, sentence length)\n",
    "        \n",
    "    def cond():\n",
    "        # while loop cond\n",
    "    def body():\n",
    "        # while loop body\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentence_encoder(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2457, shape=(1, 100), dtype=float32, numpy=\n",
       "array([[ 0.00374433, -0.00017227, -0.00042337,  0.00081117,  0.00104366,\n",
       "        -0.00261186,  0.00332164, -0.00471697,  0.00230467, -0.00045518,\n",
       "         0.00195811,  0.00130047,  0.00549739, -0.00119504,  0.00294714,\n",
       "         0.00147063,  0.00177251,  0.0022942 ,  0.00310814,  0.00182766,\n",
       "        -0.00243195,  0.00296373,  0.00207229,  0.00040427,  0.00150102,\n",
       "         0.00330019, -0.00090425,  0.0015603 , -0.00041818,  0.001962  ,\n",
       "        -0.00019167, -0.00310446, -0.00162634, -0.0003005 , -0.0034497 ,\n",
       "        -0.00191648, -0.00232193, -0.00282545, -0.00035645, -0.00053136,\n",
       "        -0.00368092, -0.00099442, -0.00432673,  0.00054355, -0.00034382,\n",
       "         0.00060201, -0.00416987, -0.00303631,  0.00409231, -0.00239753,\n",
       "         0.00227109,  0.0005152 ,  0.00105489,  0.00132972,  0.00191649,\n",
       "         0.00093768,  0.00026332,  0.00136469,  0.00095317,  0.00017627,\n",
       "         0.00247985, -0.00152331, -0.00346897, -0.00066158, -0.00554176,\n",
       "         0.00250683, -0.00027755,  0.00062536,  0.00097494,  0.00530051,\n",
       "        -0.00110359,  0.00164832,  0.00118266, -0.00322339, -0.00302038,\n",
       "        -0.00598036,  0.00174515,  0.0027959 ,  0.00073235,  0.00221799,\n",
       "        -0.00122425, -0.00560509, -0.0001001 , -0.00057045,  0.00082043,\n",
       "        -0.00162399, -0.00282402,  0.00115334, -0.00159521, -0.00089814,\n",
       "         0.00431314, -0.00135946,  0.00469862, -0.00129608, -0.00100713,\n",
       "        -0.00583277, -0.00113909,  0.00357168, -0.00502868, -0.00178874]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "Pair = collections.namedtuple('Pair', 'j, k')\n",
    "ijk_0 = (tf.constant(0), Pair(tf.constant(1), tf.constant(2)))\n",
    "c = lambda i, p: i < 10\n",
    "b = lambda i, p: (i + 1, Pair((p.j + p.k), (p.j - p.k)))\n",
    "ijk_final = tf.while_loop(c, b, ijk_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2581, shape=(), dtype=int32, numpy=32>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ijk_final[1].j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
