{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f85ade070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import cosine_similarity\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate raw corpus for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# vocab set and vocab size\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# construct dictionary to lookup \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "# construct training data: (context, target) pair\n",
    "raw_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    raw_data.append((context, target))\n",
    "print(raw_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 13, 30, 11]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context, target = raw_data[0]\n",
    "context\n",
    "[word_to_ix[word] for word in context]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbow_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.dataset[idx]\n",
    "        return {\"context\":torch.tensor([word_to_ix[word] for word in context]), \"target\":torch.tensor(word_to_ix[target])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cbow_dataset(raw_data)\n",
    "dataloader = DataLoader(dataset,batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        # parameter of shape (vocab_size, 3)\n",
    "        self.embedding = nn.Embedding(vocab_size, 3)\n",
    "        # matrix of shape (3, vocab_size)\n",
    "        self.linear = nn.Linear(3, vocab_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        # for batch this would be (B, 3)\n",
    "        context_embed = self.embedding(x).sum(1)\n",
    "        x = self.linear(context_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.00402282\n",
      "[1,    11] loss: 0.04147724\n",
      "[2,     1] loss: 0.00342317\n",
      "[2,    11] loss: 0.03446168\n",
      "[3,     1] loss: 0.00285365\n",
      "[3,    11] loss: 0.02923987\n",
      "[4,     1] loss: 0.00249494\n",
      "[4,    11] loss: 0.02584207\n",
      "[5,     1] loss: 0.00226498\n",
      "[5,    11] loss: 0.02343580\n",
      "[6,     1] loss: 0.00210213\n",
      "[6,    11] loss: 0.02161346\n",
      "[7,     1] loss: 0.00197792\n",
      "[7,    11] loss: 0.02017254\n",
      "[8,     1] loss: 0.00187943\n",
      "[8,    11] loss: 0.01899862\n",
      "[9,     1] loss: 0.00179976\n",
      "[9,    11] loss: 0.01802174\n",
      "[10,     1] loss: 0.00173438\n",
      "[10,    11] loss: 0.01719609\n",
      "[11,     1] loss: 0.00167990\n",
      "[11,    11] loss: 0.01648965\n",
      "[12,     1] loss: 0.00163375\n",
      "[12,    11] loss: 0.01587869\n",
      "[13,     1] loss: 0.00159396\n",
      "[13,    11] loss: 0.01534478\n",
      "[14,     1] loss: 0.00155908\n",
      "[14,    11] loss: 0.01487314\n",
      "[15,     1] loss: 0.00152801\n",
      "[15,    11] loss: 0.01445185\n",
      "[16,     1] loss: 0.00149994\n",
      "[16,    11] loss: 0.01407127\n",
      "[17,     1] loss: 0.00147425\n",
      "[17,    11] loss: 0.01372370\n",
      "[18,     1] loss: 0.00145047\n",
      "[18,    11] loss: 0.01340304\n",
      "[19,     1] loss: 0.00142820\n",
      "[19,    11] loss: 0.01310444\n",
      "[20,     1] loss: 0.00140712\n",
      "[20,    11] loss: 0.01282413\n",
      "[21,     1] loss: 0.00138699\n",
      "[21,    11] loss: 0.01255914\n",
      "[22,     1] loss: 0.00136757\n",
      "[22,    11] loss: 0.01230718\n",
      "[23,     1] loss: 0.00134868\n",
      "[23,    11] loss: 0.01206650\n",
      "[24,     1] loss: 0.00133016\n",
      "[24,    11] loss: 0.01183578\n",
      "[25,     1] loss: 0.00131185\n",
      "[25,    11] loss: 0.01161411\n",
      "[26,     1] loss: 0.00129364\n",
      "[26,    11] loss: 0.01140085\n",
      "[27,     1] loss: 0.00127542\n",
      "[27,    11] loss: 0.01119566\n",
      "[28,     1] loss: 0.00125709\n",
      "[28,    11] loss: 0.01099835\n",
      "[29,     1] loss: 0.00123857\n",
      "[29,    11] loss: 0.01080884\n",
      "[30,     1] loss: 0.00121984\n",
      "[30,    11] loss: 0.01062699\n",
      "[31,     1] loss: 0.00120088\n",
      "[31,    11] loss: 0.01045254\n",
      "[32,     1] loss: 0.00118178\n",
      "[32,    11] loss: 0.01028503\n",
      "[33,     1] loss: 0.00116267\n",
      "[33,    11] loss: 0.01012382\n",
      "[34,     1] loss: 0.00114371\n",
      "[34,    11] loss: 0.00996815\n",
      "[35,     1] loss: 0.00112509\n",
      "[35,    11] loss: 0.00981733\n",
      "[36,     1] loss: 0.00110697\n",
      "[36,    11] loss: 0.00967077\n",
      "[37,     1] loss: 0.00108947\n",
      "[37,    11] loss: 0.00952808\n",
      "[38,     1] loss: 0.00107264\n",
      "[38,    11] loss: 0.00938903\n",
      "[39,     1] loss: 0.00105648\n",
      "[39,    11] loss: 0.00925351\n",
      "[40,     1] loss: 0.00104098\n",
      "[40,    11] loss: 0.00912150\n",
      "[41,     1] loss: 0.00102610\n",
      "[41,    11] loss: 0.00899300\n",
      "[42,     1] loss: 0.00101177\n",
      "[42,    11] loss: 0.00886803\n",
      "[43,     1] loss: 0.00099796\n",
      "[43,    11] loss: 0.00874658\n",
      "[44,     1] loss: 0.00098462\n",
      "[44,    11] loss: 0.00862863\n",
      "[45,     1] loss: 0.00097171\n",
      "[45,    11] loss: 0.00851415\n",
      "[46,     1] loss: 0.00095920\n",
      "[46,    11] loss: 0.00840305\n",
      "[47,     1] loss: 0.00094707\n",
      "[47,    11] loss: 0.00829528\n",
      "[48,     1] loss: 0.00093530\n",
      "[48,    11] loss: 0.00819073\n",
      "[49,     1] loss: 0.00092387\n",
      "[49,    11] loss: 0.00808930\n",
      "[50,     1] loss: 0.00091279\n",
      "[50,    11] loss: 0.00799088\n",
      "[51,     1] loss: 0.00090203\n",
      "[51,    11] loss: 0.00789534\n",
      "[52,     1] loss: 0.00089160\n",
      "[52,    11] loss: 0.00780256\n",
      "[53,     1] loss: 0.00088149\n",
      "[53,    11] loss: 0.00771243\n",
      "[54,     1] loss: 0.00087169\n",
      "[54,    11] loss: 0.00762482\n",
      "[55,     1] loss: 0.00086219\n",
      "[55,    11] loss: 0.00753961\n",
      "[56,     1] loss: 0.00085300\n",
      "[56,    11] loss: 0.00745670\n",
      "[57,     1] loss: 0.00084409\n",
      "[57,    11] loss: 0.00737598\n",
      "[58,     1] loss: 0.00083548\n",
      "[58,    11] loss: 0.00729734\n",
      "[59,     1] loss: 0.00082714\n",
      "[59,    11] loss: 0.00722070\n",
      "[60,     1] loss: 0.00081907\n",
      "[60,    11] loss: 0.00714595\n",
      "[61,     1] loss: 0.00081126\n",
      "[61,    11] loss: 0.00707303\n",
      "[62,     1] loss: 0.00080370\n",
      "[62,    11] loss: 0.00700184\n",
      "[63,     1] loss: 0.00079638\n",
      "[63,    11] loss: 0.00693232\n",
      "[64,     1] loss: 0.00078930\n",
      "[64,    11] loss: 0.00686439\n",
      "[65,     1] loss: 0.00078244\n",
      "[65,    11] loss: 0.00679798\n",
      "[66,     1] loss: 0.00077580\n",
      "[66,    11] loss: 0.00673304\n",
      "[67,     1] loss: 0.00076937\n",
      "[67,    11] loss: 0.00666950\n",
      "[68,     1] loss: 0.00076314\n",
      "[68,    11] loss: 0.00660731\n",
      "[69,     1] loss: 0.00075710\n",
      "[69,    11] loss: 0.00654641\n",
      "[70,     1] loss: 0.00075124\n",
      "[70,    11] loss: 0.00648675\n",
      "[71,     1] loss: 0.00074555\n",
      "[71,    11] loss: 0.00642828\n",
      "[72,     1] loss: 0.00074003\n",
      "[72,    11] loss: 0.00637096\n",
      "[73,     1] loss: 0.00073466\n",
      "[73,    11] loss: 0.00631474\n",
      "[74,     1] loss: 0.00072944\n",
      "[74,    11] loss: 0.00625958\n",
      "[75,     1] loss: 0.00072436\n",
      "[75,    11] loss: 0.00620544\n",
      "[76,     1] loss: 0.00071942\n",
      "[76,    11] loss: 0.00615228\n",
      "[77,     1] loss: 0.00071459\n",
      "[77,    11] loss: 0.00610007\n",
      "[78,     1] loss: 0.00070989\n",
      "[78,    11] loss: 0.00604877\n",
      "[79,     1] loss: 0.00070529\n",
      "[79,    11] loss: 0.00599835\n",
      "[80,     1] loss: 0.00070080\n",
      "[80,    11] loss: 0.00594879\n",
      "[81,     1] loss: 0.00069640\n",
      "[81,    11] loss: 0.00590004\n",
      "[82,     1] loss: 0.00069210\n",
      "[82,    11] loss: 0.00585209\n",
      "[83,     1] loss: 0.00068789\n",
      "[83,    11] loss: 0.00580492\n",
      "[84,     1] loss: 0.00068375\n",
      "[84,    11] loss: 0.00575849\n",
      "[85,     1] loss: 0.00067970\n",
      "[85,    11] loss: 0.00571278\n",
      "[86,     1] loss: 0.00067572\n",
      "[86,    11] loss: 0.00566778\n",
      "[87,     1] loss: 0.00067181\n",
      "[87,    11] loss: 0.00562346\n",
      "[88,     1] loss: 0.00066796\n",
      "[88,    11] loss: 0.00557980\n",
      "[89,     1] loss: 0.00066418\n",
      "[89,    11] loss: 0.00553679\n",
      "[90,     1] loss: 0.00066047\n",
      "[90,    11] loss: 0.00549440\n",
      "[91,     1] loss: 0.00065680\n",
      "[91,    11] loss: 0.00545262\n",
      "[92,     1] loss: 0.00065320\n",
      "[92,    11] loss: 0.00541144\n",
      "[93,     1] loss: 0.00064965\n",
      "[93,    11] loss: 0.00537083\n",
      "[94,     1] loss: 0.00064615\n",
      "[94,    11] loss: 0.00533078\n",
      "[95,     1] loss: 0.00064270\n",
      "[95,    11] loss: 0.00529127\n",
      "[96,     1] loss: 0.00063930\n",
      "[96,    11] loss: 0.00525229\n",
      "[97,     1] loss: 0.00063594\n",
      "[97,    11] loss: 0.00521384\n",
      "[98,     1] loss: 0.00063263\n",
      "[98,    11] loss: 0.00517588\n",
      "[99,     1] loss: 0.00062936\n",
      "[99,    11] loss: 0.00513841\n",
      "[100,     1] loss: 0.00062614\n",
      "[100,    11] loss: 0.00510142\n",
      "[101,     1] loss: 0.00062295\n",
      "[101,    11] loss: 0.00506490\n",
      "[102,     1] loss: 0.00061980\n",
      "[102,    11] loss: 0.00502882\n",
      "[103,     1] loss: 0.00061669\n",
      "[103,    11] loss: 0.00499319\n",
      "[104,     1] loss: 0.00061362\n",
      "[104,    11] loss: 0.00495798\n",
      "[105,     1] loss: 0.00061058\n",
      "[105,    11] loss: 0.00492320\n",
      "[106,     1] loss: 0.00060757\n",
      "[106,    11] loss: 0.00488882\n",
      "[107,     1] loss: 0.00060460\n",
      "[107,    11] loss: 0.00485485\n",
      "[108,     1] loss: 0.00060166\n",
      "[108,    11] loss: 0.00482126\n",
      "[109,     1] loss: 0.00059875\n",
      "[109,    11] loss: 0.00478806\n",
      "[110,     1] loss: 0.00059587\n",
      "[110,    11] loss: 0.00475524\n",
      "[111,     1] loss: 0.00059301\n",
      "[111,    11] loss: 0.00472278\n",
      "[112,     1] loss: 0.00059018\n",
      "[112,    11] loss: 0.00469068\n",
      "[113,     1] loss: 0.00058738\n",
      "[113,    11] loss: 0.00465894\n",
      "[114,     1] loss: 0.00058460\n",
      "[114,    11] loss: 0.00462754\n",
      "[115,     1] loss: 0.00058185\n",
      "[115,    11] loss: 0.00459649\n",
      "[116,     1] loss: 0.00057912\n",
      "[116,    11] loss: 0.00456577\n",
      "[117,     1] loss: 0.00057641\n",
      "[117,    11] loss: 0.00453539\n",
      "[118,     1] loss: 0.00057372\n",
      "[118,    11] loss: 0.00450534\n",
      "[119,     1] loss: 0.00057105\n",
      "[119,    11] loss: 0.00447561\n",
      "[120,     1] loss: 0.00056839\n",
      "[120,    11] loss: 0.00444620\n",
      "[121,     1] loss: 0.00056576\n",
      "[121,    11] loss: 0.00441710\n",
      "[122,     1] loss: 0.00056314\n",
      "[122,    11] loss: 0.00438831\n",
      "[123,     1] loss: 0.00056054\n",
      "[123,    11] loss: 0.00435983\n",
      "[124,     1] loss: 0.00055795\n",
      "[124,    11] loss: 0.00433166\n",
      "[125,     1] loss: 0.00055538\n",
      "[125,    11] loss: 0.00430378\n",
      "[126,     1] loss: 0.00055281\n",
      "[126,    11] loss: 0.00427619\n",
      "[127,     1] loss: 0.00055026\n",
      "[127,    11] loss: 0.00424889\n",
      "[128,     1] loss: 0.00054772\n",
      "[128,    11] loss: 0.00422187\n",
      "[129,     1] loss: 0.00054519\n",
      "[129,    11] loss: 0.00419513\n",
      "[130,     1] loss: 0.00054267\n",
      "[130,    11] loss: 0.00416867\n",
      "[131,     1] loss: 0.00054016\n",
      "[131,    11] loss: 0.00414246\n",
      "[132,     1] loss: 0.00053766\n",
      "[132,    11] loss: 0.00411652\n",
      "[133,     1] loss: 0.00053516\n",
      "[133,    11] loss: 0.00409084\n",
      "[134,     1] loss: 0.00053267\n",
      "[134,    11] loss: 0.00406540\n",
      "[135,     1] loss: 0.00053018\n",
      "[135,    11] loss: 0.00404021\n",
      "[136,     1] loss: 0.00052769\n",
      "[136,    11] loss: 0.00401525\n",
      "[137,     1] loss: 0.00052521\n",
      "[137,    11] loss: 0.00399052\n",
      "[138,     1] loss: 0.00052274\n",
      "[138,    11] loss: 0.00396602\n",
      "[139,     1] loss: 0.00052026\n",
      "[139,    11] loss: 0.00394174\n",
      "[140,     1] loss: 0.00051778\n",
      "[140,    11] loss: 0.00391768\n",
      "[141,     1] loss: 0.00051531\n",
      "[141,    11] loss: 0.00389382\n",
      "[142,     1] loss: 0.00051283\n",
      "[142,    11] loss: 0.00387017\n",
      "[143,     1] loss: 0.00051035\n",
      "[143,    11] loss: 0.00384672\n",
      "[144,     1] loss: 0.00050787\n",
      "[144,    11] loss: 0.00382346\n",
      "[145,     1] loss: 0.00050539\n",
      "[145,    11] loss: 0.00380040\n",
      "[146,     1] loss: 0.00050290\n",
      "[146,    11] loss: 0.00377752\n",
      "[147,     1] loss: 0.00050042\n",
      "[147,    11] loss: 0.00375483\n",
      "[148,     1] loss: 0.00049792\n",
      "[148,    11] loss: 0.00373232\n",
      "[149,     1] loss: 0.00049542\n",
      "[149,    11] loss: 0.00370999\n",
      "[150,     1] loss: 0.00049292\n",
      "[150,    11] loss: 0.00368783\n",
      "[151,     1] loss: 0.00049041\n",
      "[151,    11] loss: 0.00366584\n",
      "[152,     1] loss: 0.00048789\n",
      "[152,    11] loss: 0.00364403\n",
      "[153,     1] loss: 0.00048536\n",
      "[153,    11] loss: 0.00362238\n",
      "[154,     1] loss: 0.00048283\n",
      "[154,    11] loss: 0.00360090\n",
      "[155,     1] loss: 0.00048029\n",
      "[155,    11] loss: 0.00357959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156,     1] loss: 0.00047775\n",
      "[156,    11] loss: 0.00355843\n",
      "[157,     1] loss: 0.00047519\n",
      "[157,    11] loss: 0.00353744\n",
      "[158,     1] loss: 0.00047262\n",
      "[158,    11] loss: 0.00351661\n",
      "[159,     1] loss: 0.00047005\n",
      "[159,    11] loss: 0.00349594\n",
      "[160,     1] loss: 0.00046747\n",
      "[160,    11] loss: 0.00347543\n",
      "[161,     1] loss: 0.00046488\n",
      "[161,    11] loss: 0.00345508\n",
      "[162,     1] loss: 0.00046228\n",
      "[162,    11] loss: 0.00343488\n",
      "[163,     1] loss: 0.00045967\n",
      "[163,    11] loss: 0.00341484\n",
      "[164,     1] loss: 0.00045705\n",
      "[164,    11] loss: 0.00339496\n",
      "[165,     1] loss: 0.00045442\n",
      "[165,    11] loss: 0.00337523\n",
      "[166,     1] loss: 0.00045178\n",
      "[166,    11] loss: 0.00335565\n",
      "[167,     1] loss: 0.00044913\n",
      "[167,    11] loss: 0.00333623\n",
      "[168,     1] loss: 0.00044648\n",
      "[168,    11] loss: 0.00331696\n",
      "[169,     1] loss: 0.00044381\n",
      "[169,    11] loss: 0.00329784\n",
      "[170,     1] loss: 0.00044114\n",
      "[170,    11] loss: 0.00327888\n",
      "[171,     1] loss: 0.00043846\n",
      "[171,    11] loss: 0.00326007\n",
      "[172,     1] loss: 0.00043577\n",
      "[172,    11] loss: 0.00324141\n",
      "[173,     1] loss: 0.00043308\n",
      "[173,    11] loss: 0.00322289\n",
      "[174,     1] loss: 0.00043038\n",
      "[174,    11] loss: 0.00320453\n",
      "[175,     1] loss: 0.00042767\n",
      "[175,    11] loss: 0.00318632\n",
      "[176,     1] loss: 0.00042496\n",
      "[176,    11] loss: 0.00316826\n",
      "[177,     1] loss: 0.00042225\n",
      "[177,    11] loss: 0.00315034\n",
      "[178,     1] loss: 0.00041953\n",
      "[178,    11] loss: 0.00313257\n",
      "[179,     1] loss: 0.00041680\n",
      "[179,    11] loss: 0.00311495\n",
      "[180,     1] loss: 0.00041408\n",
      "[180,    11] loss: 0.00309748\n",
      "[181,     1] loss: 0.00041135\n",
      "[181,    11] loss: 0.00308014\n",
      "[182,     1] loss: 0.00040862\n",
      "[182,    11] loss: 0.00306296\n",
      "[183,     1] loss: 0.00040589\n",
      "[183,    11] loss: 0.00304591\n",
      "[184,     1] loss: 0.00040316\n",
      "[184,    11] loss: 0.00302900\n",
      "[185,     1] loss: 0.00040044\n",
      "[185,    11] loss: 0.00301224\n",
      "[186,     1] loss: 0.00039771\n",
      "[186,    11] loss: 0.00299562\n",
      "[187,     1] loss: 0.00039499\n",
      "[187,    11] loss: 0.00297913\n",
      "[188,     1] loss: 0.00039227\n",
      "[188,    11] loss: 0.00296278\n",
      "[189,     1] loss: 0.00038956\n",
      "[189,    11] loss: 0.00294657\n",
      "[190,     1] loss: 0.00038685\n",
      "[190,    11] loss: 0.00293049\n",
      "[191,     1] loss: 0.00038415\n",
      "[191,    11] loss: 0.00291455\n",
      "[192,     1] loss: 0.00038146\n",
      "[192,    11] loss: 0.00289874\n",
      "[193,     1] loss: 0.00037878\n",
      "[193,    11] loss: 0.00288306\n",
      "[194,     1] loss: 0.00037610\n",
      "[194,    11] loss: 0.00286750\n",
      "[195,     1] loss: 0.00037344\n",
      "[195,    11] loss: 0.00285208\n",
      "[196,     1] loss: 0.00037079\n",
      "[196,    11] loss: 0.00283679\n",
      "[197,     1] loss: 0.00036815\n",
      "[197,    11] loss: 0.00282162\n",
      "[198,     1] loss: 0.00036552\n",
      "[198,    11] loss: 0.00280658\n",
      "[199,     1] loss: 0.00036290\n",
      "[199,    11] loss: 0.00279165\n",
      "[200,     1] loss: 0.00036030\n",
      "[200,    11] loss: 0.00277685\n",
      "[201,     1] loss: 0.00035771\n",
      "[201,    11] loss: 0.00276217\n",
      "[202,     1] loss: 0.00035514\n",
      "[202,    11] loss: 0.00274762\n",
      "[203,     1] loss: 0.00035259\n",
      "[203,    11] loss: 0.00273317\n",
      "[204,     1] loss: 0.00035005\n",
      "[204,    11] loss: 0.00271885\n",
      "[205,     1] loss: 0.00034753\n",
      "[205,    11] loss: 0.00270463\n",
      "[206,     1] loss: 0.00034503\n",
      "[206,    11] loss: 0.00269054\n",
      "[207,     1] loss: 0.00034254\n",
      "[207,    11] loss: 0.00267655\n",
      "[208,     1] loss: 0.00034008\n",
      "[208,    11] loss: 0.00266268\n",
      "[209,     1] loss: 0.00033763\n",
      "[209,    11] loss: 0.00264891\n",
      "[210,     1] loss: 0.00033520\n",
      "[210,    11] loss: 0.00263526\n",
      "[211,     1] loss: 0.00033280\n",
      "[211,    11] loss: 0.00262171\n",
      "[212,     1] loss: 0.00033041\n",
      "[212,    11] loss: 0.00260826\n",
      "[213,     1] loss: 0.00032805\n",
      "[213,    11] loss: 0.00259492\n",
      "[214,     1] loss: 0.00032570\n",
      "[214,    11] loss: 0.00258168\n",
      "[215,     1] loss: 0.00032338\n",
      "[215,    11] loss: 0.00256855\n",
      "[216,     1] loss: 0.00032108\n",
      "[216,    11] loss: 0.00255551\n",
      "[217,     1] loss: 0.00031880\n",
      "[217,    11] loss: 0.00254258\n",
      "[218,     1] loss: 0.00031654\n",
      "[218,    11] loss: 0.00252974\n",
      "[219,     1] loss: 0.00031430\n",
      "[219,    11] loss: 0.00251701\n",
      "[220,     1] loss: 0.00031209\n",
      "[220,    11] loss: 0.00250436\n",
      "[221,     1] loss: 0.00030990\n",
      "[221,    11] loss: 0.00249182\n",
      "[222,     1] loss: 0.00030773\n",
      "[222,    11] loss: 0.00247936\n",
      "[223,     1] loss: 0.00030558\n",
      "[223,    11] loss: 0.00246700\n",
      "[224,     1] loss: 0.00030346\n",
      "[224,    11] loss: 0.00245473\n",
      "[225,     1] loss: 0.00030136\n",
      "[225,    11] loss: 0.00244255\n",
      "[226,     1] loss: 0.00029928\n",
      "[226,    11] loss: 0.00243046\n",
      "[227,     1] loss: 0.00029722\n",
      "[227,    11] loss: 0.00241846\n",
      "[228,     1] loss: 0.00029518\n",
      "[228,    11] loss: 0.00240655\n",
      "[229,     1] loss: 0.00029317\n",
      "[229,    11] loss: 0.00239472\n",
      "[230,     1] loss: 0.00029117\n",
      "[230,    11] loss: 0.00238298\n",
      "[231,     1] loss: 0.00028920\n",
      "[231,    11] loss: 0.00237133\n",
      "[232,     1] loss: 0.00028725\n",
      "[232,    11] loss: 0.00235975\n",
      "[233,     1] loss: 0.00028532\n",
      "[233,    11] loss: 0.00234826\n",
      "[234,     1] loss: 0.00028342\n",
      "[234,    11] loss: 0.00233686\n",
      "[235,     1] loss: 0.00028153\n",
      "[235,    11] loss: 0.00232553\n",
      "[236,     1] loss: 0.00027966\n",
      "[236,    11] loss: 0.00231428\n",
      "[237,     1] loss: 0.00027782\n",
      "[237,    11] loss: 0.00230312\n",
      "[238,     1] loss: 0.00027599\n",
      "[238,    11] loss: 0.00229203\n",
      "[239,     1] loss: 0.00027419\n",
      "[239,    11] loss: 0.00228102\n",
      "[240,     1] loss: 0.00027240\n",
      "[240,    11] loss: 0.00227008\n",
      "[241,     1] loss: 0.00027063\n",
      "[241,    11] loss: 0.00225923\n",
      "[242,     1] loss: 0.00026888\n",
      "[242,    11] loss: 0.00224844\n",
      "[243,     1] loss: 0.00026715\n",
      "[243,    11] loss: 0.00223773\n",
      "[244,     1] loss: 0.00026544\n",
      "[244,    11] loss: 0.00222710\n",
      "[245,     1] loss: 0.00026375\n",
      "[245,    11] loss: 0.00221654\n",
      "[246,     1] loss: 0.00026207\n",
      "[246,    11] loss: 0.00220605\n",
      "[247,     1] loss: 0.00026042\n",
      "[247,    11] loss: 0.00219563\n",
      "[248,     1] loss: 0.00025878\n",
      "[248,    11] loss: 0.00218528\n",
      "[249,     1] loss: 0.00025716\n",
      "[249,    11] loss: 0.00217500\n",
      "[250,     1] loss: 0.00025555\n",
      "[250,    11] loss: 0.00216479\n",
      "[251,     1] loss: 0.00025396\n",
      "[251,    11] loss: 0.00215465\n",
      "[252,     1] loss: 0.00025238\n",
      "[252,    11] loss: 0.00214457\n",
      "[253,     1] loss: 0.00025083\n",
      "[253,    11] loss: 0.00213456\n",
      "[254,     1] loss: 0.00024928\n",
      "[254,    11] loss: 0.00212462\n",
      "[255,     1] loss: 0.00024776\n",
      "[255,    11] loss: 0.00211475\n",
      "[256,     1] loss: 0.00024624\n",
      "[256,    11] loss: 0.00210494\n",
      "[257,     1] loss: 0.00024474\n",
      "[257,    11] loss: 0.00209519\n",
      "[258,     1] loss: 0.00024326\n",
      "[258,    11] loss: 0.00208550\n",
      "[259,     1] loss: 0.00024179\n",
      "[259,    11] loss: 0.00207588\n",
      "[260,     1] loss: 0.00024033\n",
      "[260,    11] loss: 0.00206632\n",
      "[261,     1] loss: 0.00023889\n",
      "[261,    11] loss: 0.00205682\n",
      "[262,     1] loss: 0.00023746\n",
      "[262,    11] loss: 0.00204738\n",
      "[263,     1] loss: 0.00023604\n",
      "[263,    11] loss: 0.00203800\n",
      "[264,     1] loss: 0.00023464\n",
      "[264,    11] loss: 0.00202868\n",
      "[265,     1] loss: 0.00023325\n",
      "[265,    11] loss: 0.00201942\n",
      "[266,     1] loss: 0.00023186\n",
      "[266,    11] loss: 0.00201022\n",
      "[267,     1] loss: 0.00023050\n",
      "[267,    11] loss: 0.00200108\n",
      "[268,     1] loss: 0.00022914\n",
      "[268,    11] loss: 0.00199199\n",
      "[269,     1] loss: 0.00022779\n",
      "[269,    11] loss: 0.00198295\n",
      "[270,     1] loss: 0.00022645\n",
      "[270,    11] loss: 0.00197398\n",
      "[271,     1] loss: 0.00022513\n",
      "[271,    11] loss: 0.00196505\n",
      "[272,     1] loss: 0.00022381\n",
      "[272,    11] loss: 0.00195618\n",
      "[273,     1] loss: 0.00022251\n",
      "[273,    11] loss: 0.00194737\n",
      "[274,     1] loss: 0.00022121\n",
      "[274,    11] loss: 0.00193860\n",
      "[275,     1] loss: 0.00021993\n",
      "[275,    11] loss: 0.00192989\n",
      "[276,     1] loss: 0.00021865\n",
      "[276,    11] loss: 0.00192123\n",
      "[277,     1] loss: 0.00021738\n",
      "[277,    11] loss: 0.00191263\n",
      "[278,     1] loss: 0.00021613\n",
      "[278,    11] loss: 0.00190407\n",
      "[279,     1] loss: 0.00021487\n",
      "[279,    11] loss: 0.00189556\n",
      "[280,     1] loss: 0.00021363\n",
      "[280,    11] loss: 0.00188711\n",
      "[281,     1] loss: 0.00021240\n",
      "[281,    11] loss: 0.00187870\n",
      "[282,     1] loss: 0.00021117\n",
      "[282,    11] loss: 0.00187034\n",
      "[283,     1] loss: 0.00020996\n",
      "[283,    11] loss: 0.00186202\n",
      "[284,     1] loss: 0.00020874\n",
      "[284,    11] loss: 0.00185376\n",
      "[285,     1] loss: 0.00020754\n",
      "[285,    11] loss: 0.00184554\n",
      "[286,     1] loss: 0.00020634\n",
      "[286,    11] loss: 0.00183737\n",
      "[287,     1] loss: 0.00020515\n",
      "[287,    11] loss: 0.00182924\n",
      "[288,     1] loss: 0.00020397\n",
      "[288,    11] loss: 0.00182115\n",
      "[289,     1] loss: 0.00020280\n",
      "[289,    11] loss: 0.00181312\n",
      "[290,     1] loss: 0.00020162\n",
      "[290,    11] loss: 0.00180512\n",
      "[291,     1] loss: 0.00020046\n",
      "[291,    11] loss: 0.00179717\n",
      "[292,     1] loss: 0.00019930\n",
      "[292,    11] loss: 0.00178926\n",
      "[293,     1] loss: 0.00019815\n",
      "[293,    11] loss: 0.00178140\n",
      "[294,     1] loss: 0.00019700\n",
      "[294,    11] loss: 0.00177357\n",
      "[295,     1] loss: 0.00019586\n",
      "[295,    11] loss: 0.00176579\n",
      "[296,     1] loss: 0.00019472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296,    11] loss: 0.00175805\n",
      "[297,     1] loss: 0.00019359\n",
      "[297,    11] loss: 0.00175035\n",
      "[298,     1] loss: 0.00019247\n",
      "[298,    11] loss: 0.00174269\n",
      "[299,     1] loss: 0.00019134\n",
      "[299,    11] loss: 0.00173507\n",
      "[300,     1] loss: 0.00019023\n",
      "[300,    11] loss: 0.00172749\n",
      "[301,     1] loss: 0.00018911\n",
      "[301,    11] loss: 0.00171995\n",
      "[302,     1] loss: 0.00018801\n",
      "[302,    11] loss: 0.00171244\n",
      "[303,     1] loss: 0.00018690\n",
      "[303,    11] loss: 0.00170497\n",
      "[304,     1] loss: 0.00018580\n",
      "[304,    11] loss: 0.00169754\n",
      "[305,     1] loss: 0.00018471\n",
      "[305,    11] loss: 0.00169015\n",
      "[306,     1] loss: 0.00018361\n",
      "[306,    11] loss: 0.00168279\n",
      "[307,     1] loss: 0.00018252\n",
      "[307,    11] loss: 0.00167548\n",
      "[308,     1] loss: 0.00018144\n",
      "[308,    11] loss: 0.00166819\n",
      "[309,     1] loss: 0.00018036\n",
      "[309,    11] loss: 0.00166094\n",
      "[310,     1] loss: 0.00017928\n",
      "[310,    11] loss: 0.00165373\n",
      "[311,     1] loss: 0.00017821\n",
      "[311,    11] loss: 0.00164655\n",
      "[312,     1] loss: 0.00017714\n",
      "[312,    11] loss: 0.00163940\n",
      "[313,     1] loss: 0.00017607\n",
      "[313,    11] loss: 0.00163229\n",
      "[314,     1] loss: 0.00017500\n",
      "[314,    11] loss: 0.00162521\n",
      "[315,     1] loss: 0.00017394\n",
      "[315,    11] loss: 0.00161816\n",
      "[316,     1] loss: 0.00017288\n",
      "[316,    11] loss: 0.00161115\n",
      "[317,     1] loss: 0.00017183\n",
      "[317,    11] loss: 0.00160417\n",
      "[318,     1] loss: 0.00017077\n",
      "[318,    11] loss: 0.00159722\n",
      "[319,     1] loss: 0.00016972\n",
      "[319,    11] loss: 0.00159030\n",
      "[320,     1] loss: 0.00016867\n",
      "[320,    11] loss: 0.00158342\n",
      "[321,     1] loss: 0.00016763\n",
      "[321,    11] loss: 0.00157656\n",
      "[322,     1] loss: 0.00016659\n",
      "[322,    11] loss: 0.00156974\n",
      "[323,     1] loss: 0.00016555\n",
      "[323,    11] loss: 0.00156294\n",
      "[324,     1] loss: 0.00016451\n",
      "[324,    11] loss: 0.00155618\n",
      "[325,     1] loss: 0.00016348\n",
      "[325,    11] loss: 0.00154944\n",
      "[326,     1] loss: 0.00016245\n",
      "[326,    11] loss: 0.00154273\n",
      "[327,     1] loss: 0.00016142\n",
      "[327,    11] loss: 0.00153606\n",
      "[328,     1] loss: 0.00016039\n",
      "[328,    11] loss: 0.00152941\n",
      "[329,     1] loss: 0.00015937\n",
      "[329,    11] loss: 0.00152279\n",
      "[330,     1] loss: 0.00015835\n",
      "[330,    11] loss: 0.00151620\n",
      "[331,     1] loss: 0.00015733\n",
      "[331,    11] loss: 0.00150963\n",
      "[332,     1] loss: 0.00015632\n",
      "[332,    11] loss: 0.00150310\n",
      "[333,     1] loss: 0.00015531\n",
      "[333,    11] loss: 0.00149659\n",
      "[334,     1] loss: 0.00015430\n",
      "[334,    11] loss: 0.00149010\n",
      "[335,     1] loss: 0.00015329\n",
      "[335,    11] loss: 0.00148365\n",
      "[336,     1] loss: 0.00015229\n",
      "[336,    11] loss: 0.00147722\n",
      "[337,     1] loss: 0.00015129\n",
      "[337,    11] loss: 0.00147081\n",
      "[338,     1] loss: 0.00015029\n",
      "[338,    11] loss: 0.00146444\n",
      "[339,     1] loss: 0.00014930\n",
      "[339,    11] loss: 0.00145809\n",
      "[340,     1] loss: 0.00014831\n",
      "[340,    11] loss: 0.00145176\n",
      "[341,     1] loss: 0.00014732\n",
      "[341,    11] loss: 0.00144546\n",
      "[342,     1] loss: 0.00014634\n",
      "[342,    11] loss: 0.00143918\n",
      "[343,     1] loss: 0.00014536\n",
      "[343,    11] loss: 0.00143293\n",
      "[344,     1] loss: 0.00014438\n",
      "[344,    11] loss: 0.00142671\n",
      "[345,     1] loss: 0.00014341\n",
      "[345,    11] loss: 0.00142051\n",
      "[346,     1] loss: 0.00014244\n",
      "[346,    11] loss: 0.00141433\n",
      "[347,     1] loss: 0.00014147\n",
      "[347,    11] loss: 0.00140818\n",
      "[348,     1] loss: 0.00014051\n",
      "[348,    11] loss: 0.00140205\n",
      "[349,     1] loss: 0.00013955\n",
      "[349,    11] loss: 0.00139594\n",
      "[350,     1] loss: 0.00013860\n",
      "[350,    11] loss: 0.00138986\n",
      "[351,     1] loss: 0.00013765\n",
      "[351,    11] loss: 0.00138381\n",
      "[352,     1] loss: 0.00013670\n",
      "[352,    11] loss: 0.00137777\n",
      "[353,     1] loss: 0.00013576\n",
      "[353,    11] loss: 0.00137176\n",
      "[354,     1] loss: 0.00013483\n",
      "[354,    11] loss: 0.00136577\n",
      "[355,     1] loss: 0.00013390\n",
      "[355,    11] loss: 0.00135981\n",
      "[356,     1] loss: 0.00013297\n",
      "[356,    11] loss: 0.00135387\n",
      "[357,     1] loss: 0.00013205\n",
      "[357,    11] loss: 0.00134795\n",
      "[358,     1] loss: 0.00013114\n",
      "[358,    11] loss: 0.00134205\n",
      "[359,     1] loss: 0.00013023\n",
      "[359,    11] loss: 0.00133618\n",
      "[360,     1] loss: 0.00012932\n",
      "[360,    11] loss: 0.00133033\n",
      "[361,     1] loss: 0.00012842\n",
      "[361,    11] loss: 0.00132450\n",
      "[362,     1] loss: 0.00012753\n",
      "[362,    11] loss: 0.00131869\n",
      "[363,     1] loss: 0.00012664\n",
      "[363,    11] loss: 0.00131291\n",
      "[364,     1] loss: 0.00012576\n",
      "[364,    11] loss: 0.00130715\n",
      "[365,     1] loss: 0.00012488\n",
      "[365,    11] loss: 0.00130141\n",
      "[366,     1] loss: 0.00012401\n",
      "[366,    11] loss: 0.00129569\n",
      "[367,     1] loss: 0.00012315\n",
      "[367,    11] loss: 0.00128999\n",
      "[368,     1] loss: 0.00012229\n",
      "[368,    11] loss: 0.00128432\n",
      "[369,     1] loss: 0.00012144\n",
      "[369,    11] loss: 0.00127867\n",
      "[370,     1] loss: 0.00012060\n",
      "[370,    11] loss: 0.00127304\n",
      "[371,     1] loss: 0.00011976\n",
      "[371,    11] loss: 0.00126743\n",
      "[372,     1] loss: 0.00011893\n",
      "[372,    11] loss: 0.00126185\n",
      "[373,     1] loss: 0.00011811\n",
      "[373,    11] loss: 0.00125628\n",
      "[374,     1] loss: 0.00011729\n",
      "[374,    11] loss: 0.00125074\n",
      "[375,     1] loss: 0.00011648\n",
      "[375,    11] loss: 0.00124522\n",
      "[376,     1] loss: 0.00011568\n",
      "[376,    11] loss: 0.00123972\n",
      "[377,     1] loss: 0.00011488\n",
      "[377,    11] loss: 0.00123424\n",
      "[378,     1] loss: 0.00011409\n",
      "[378,    11] loss: 0.00122879\n",
      "[379,     1] loss: 0.00011331\n",
      "[379,    11] loss: 0.00122335\n",
      "[380,     1] loss: 0.00011254\n",
      "[380,    11] loss: 0.00121794\n",
      "[381,     1] loss: 0.00011177\n",
      "[381,    11] loss: 0.00121255\n",
      "[382,     1] loss: 0.00011101\n",
      "[382,    11] loss: 0.00120718\n",
      "[383,     1] loss: 0.00011026\n",
      "[383,    11] loss: 0.00120183\n",
      "[384,     1] loss: 0.00010952\n",
      "[384,    11] loss: 0.00119651\n",
      "[385,     1] loss: 0.00010878\n",
      "[385,    11] loss: 0.00119120\n",
      "[386,     1] loss: 0.00010806\n",
      "[386,    11] loss: 0.00118592\n",
      "[387,     1] loss: 0.00010734\n",
      "[387,    11] loss: 0.00118066\n",
      "[388,     1] loss: 0.00010662\n",
      "[388,    11] loss: 0.00117542\n",
      "[389,     1] loss: 0.00010592\n",
      "[389,    11] loss: 0.00117020\n",
      "[390,     1] loss: 0.00010522\n",
      "[390,    11] loss: 0.00116500\n",
      "[391,     1] loss: 0.00010453\n",
      "[391,    11] loss: 0.00115983\n",
      "[392,     1] loss: 0.00010385\n",
      "[392,    11] loss: 0.00115467\n",
      "[393,     1] loss: 0.00010318\n",
      "[393,    11] loss: 0.00114954\n",
      "[394,     1] loss: 0.00010251\n",
      "[394,    11] loss: 0.00114443\n",
      "[395,     1] loss: 0.00010185\n",
      "[395,    11] loss: 0.00113934\n",
      "[396,     1] loss: 0.00010120\n",
      "[396,    11] loss: 0.00113427\n",
      "[397,     1] loss: 0.00010056\n",
      "[397,    11] loss: 0.00112923\n",
      "[398,     1] loss: 0.00009992\n",
      "[398,    11] loss: 0.00112420\n",
      "[399,     1] loss: 0.00009929\n",
      "[399,    11] loss: 0.00111920\n",
      "[400,     1] loss: 0.00009867\n",
      "[400,    11] loss: 0.00111422\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# intialize parameters\n",
    "for parameter in model.parameters():\n",
    "    nn.init.normal_(parameter)\n",
    "\n",
    "# train\n",
    "for epoch in range(400):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        context = data[\"context\"]\n",
    "        target = data[\"target\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(context)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000028F86421EB8>\n"
     ]
    }
   ],
   "source": [
    "word_embedding = None\n",
    "for submodule in model.children():\n",
    "    if type(submodule)== nn.Linear:\n",
    "        print(submodule.parameters())\n",
    "        word_embedding = submodule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = word_embedding.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    return cosine_similarity(word_embedding[word_to_ix[word1]], word_embedding[word_to_ix[word2]], dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_topn(word, n):\n",
    "    words = list(vocab)\n",
    "    words.sort(key=lambda w: similarity(w, word), reverse=True)\n",
    "    return words[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'evolve,',\n",
       " 'program.',\n",
       " 'they',\n",
       " 'inhabit',\n",
       " 'spirits',\n",
       " 'abstract',\n",
       " 'programs',\n",
       " 'rules',\n",
       " 'directed']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_topn(\"We\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f85ade070>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.functional import cosine_similarity\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Skip-gram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('about', 'We'), ('about', 'are'), ('about', 'to'), ('about', 'study'), ('to', 'are')]\n"
     ]
    }
   ],
   "source": [
    "raw_skip_gram_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    center = raw_text[i]\n",
    "    for word in context: \n",
    "        raw_skip_gram_data.append((center,word))\n",
    "print(raw_skip_gram_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.dataset[idx]\n",
    "        \n",
    "        return {\"center\":torch.tensor([word_to_ix[center]]), \"context\":torch.tensor(word_to_ix[context])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_skipgram = skipgram_dataset(raw_skip_gram_data)\n",
    "dataloader_skipgram = DataLoader(dataset_skipgram, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(nn.Embedding(vocab_size, 3),\n",
    "                  nn.Linear(3,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0304, -1.2977,  0.9390],\n",
       "        [-1.0304, -1.2977,  0.9390],\n",
       "        [-1.0304, -1.2977,  0.9390],\n",
       "        [ 2.4358,  0.7929,  0.9873]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center = torch.tensor([39,39,39,41])\n",
    "layer = nn.Embedding(vocab_size, 3)\n",
    "layer(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39],\n",
      "        [39],\n",
      "        [39],\n",
      "        [39]])\n",
      "tensor([17, 18,  3, 12])\n",
      "tensor([[[ 5.1001, -1.3883, -0.4741,  2.1008,  0.0644,  2.5013,  2.7036,\n",
      "           0.6809, -1.6003, -2.5444, -0.9272, -3.2729,  2.3253, -0.2955,\n",
      "          -1.8054,  1.2215,  1.5943, -2.8442, -0.8601,  1.9945, -3.5406,\n",
      "          -0.6422, -0.5790,  1.4110, -0.3164, -0.2242,  2.8887, -1.6497,\n",
      "           1.3752, -0.9250, -2.2290,  2.1204,  3.1357,  5.3337,  0.9207,\n",
      "          -0.4299,  1.2942,  0.2996,  3.6706, -2.3519, -2.7258,  0.6672,\n",
      "           1.2590,  2.1013,  0.3003,  0.9130, -0.2308, -0.1417,  2.2458]],\n",
      "\n",
      "        [[ 5.1001, -1.3883, -0.4741,  2.1008,  0.0644,  2.5013,  2.7036,\n",
      "           0.6809, -1.6003, -2.5444, -0.9272, -3.2729,  2.3253, -0.2955,\n",
      "          -1.8054,  1.2215,  1.5943, -2.8442, -0.8601,  1.9945, -3.5406,\n",
      "          -0.6422, -0.5790,  1.4110, -0.3164, -0.2242,  2.8887, -1.6497,\n",
      "           1.3752, -0.9250, -2.2290,  2.1204,  3.1357,  5.3337,  0.9207,\n",
      "          -0.4299,  1.2942,  0.2996,  3.6706, -2.3519, -2.7258,  0.6672,\n",
      "           1.2590,  2.1013,  0.3003,  0.9130, -0.2308, -0.1417,  2.2458]],\n",
      "\n",
      "        [[ 5.1001, -1.3883, -0.4741,  2.1008,  0.0644,  2.5013,  2.7036,\n",
      "           0.6809, -1.6003, -2.5444, -0.9272, -3.2729,  2.3253, -0.2955,\n",
      "          -1.8054,  1.2215,  1.5943, -2.8442, -0.8601,  1.9945, -3.5406,\n",
      "          -0.6422, -0.5790,  1.4110, -0.3164, -0.2242,  2.8887, -1.6497,\n",
      "           1.3752, -0.9250, -2.2290,  2.1204,  3.1357,  5.3337,  0.9207,\n",
      "          -0.4299,  1.2942,  0.2996,  3.6706, -2.3519, -2.7258,  0.6672,\n",
      "           1.2590,  2.1013,  0.3003,  0.9130, -0.2308, -0.1417,  2.2458]],\n",
      "\n",
      "        [[ 5.1001, -1.3883, -0.4741,  2.1008,  0.0644,  2.5013,  2.7036,\n",
      "           0.6809, -1.6003, -2.5444, -0.9272, -3.2729,  2.3253, -0.2955,\n",
      "          -1.8054,  1.2215,  1.5943, -2.8442, -0.8601,  1.9945, -3.5406,\n",
      "          -0.6422, -0.5790,  1.4110, -0.3164, -0.2242,  2.8887, -1.6497,\n",
      "           1.3752, -0.9250, -2.2290,  2.1204,  3.1357,  5.3337,  0.9207,\n",
      "          -0.4299,  1.2942,  0.2996,  3.6706, -2.3519, -2.7258,  0.6672,\n",
      "           1.2590,  2.1013,  0.3003,  0.9130, -0.2308, -0.1417,  2.2458]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([17, 18,  3, 12])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (4, 49), got torch.Size([4])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-5733dce39d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 942\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 1881\u001b[1;33m                 out_size, target.size()))\n\u001b[0m\u001b[0;32m   1882\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (4, 49), got torch.Size([4])"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "for parameter in model.parameters():\n",
    "    nn.init.normal_(parameter)\n",
    "\n",
    "for epoch in range(300):\n",
    "    for idx, data in enumerate(dataloader_skipgram):\n",
    "        # center is of shape (B, 1)\n",
    "        center = data[\"center\"]\n",
    "        print(center)\n",
    "        # target is of shape ()\n",
    "        target= data[\"context\"]\n",
    "        print(target)\n",
    "        output = model.forward(center)\n",
    "        print(output)\n",
    "        print(target)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 10 == 0:\n",
    "            print(\"epoch:\", epoch, \"iter:\", idx, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
