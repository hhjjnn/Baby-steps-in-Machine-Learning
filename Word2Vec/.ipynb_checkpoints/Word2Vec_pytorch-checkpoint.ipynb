{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f302658230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from early_stopping_pytorch.pytorchtools import EarlyStopping\n",
    "import numpy as np\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate raw corpus for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# vocab set and vocab size\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# construct dictionary to lookup \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "# construct training data: (context, target) pair\n",
    "raw_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    raw_data.append((context, target))\n",
    "print(raw_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 13, 30, 11]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context, target = raw_data[0]\n",
    "context\n",
    "[word_to_ix[word] for word in context]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbow_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.dataset[idx]\n",
    "        return {\"data\":torch.tensor([word_to_ix[word] for word in context]), \"target\":torch.tensor(word_to_ix[target])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cbow_dataset(raw_data)\n",
    "dataloader = DataLoader(dataset,batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        # parameter of shape (vocab_size, 3)\n",
    "        self.embedding = nn.Embedding(vocab_size, 3)\n",
    "        # matrix of shape (3, vocab_size)\n",
    "        self.linear = nn.Linear(3, vocab_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        # for batch this would be (B, 3)\n",
    "        context_embed = self.embedding(x).sum(1)\n",
    "        x = self.linear(context_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=1, early_stopping=False, retrain=False):\n",
    "    # if retraining\n",
    "    if retrain:\n",
    "        for parameter in model.parameters():\n",
    "            nn.init.normal_(parameter)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # if early stopping\n",
    "    if early_stopping:\n",
    "        early_stopper=EarlyStopping()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        average_loss = 0\n",
    "        losses = []\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            context = data[\"data\"]\n",
    "            target = data[\"target\"]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(context)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().numpy())\n",
    "        # average loss of this epoch\n",
    "        average_loss = np.average(losses)\n",
    "        print(\"average loss of epoch\", epoch, \":\", average_loss)\n",
    "        \n",
    "        early_stopper(average_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss of epoch 0 : 0.0071551003\n",
      "average loss of epoch 1 : 0.0071500777\n",
      "average loss of epoch 2 : 0.0071450234\n",
      "average loss of epoch 3 : 0.007140128\n",
      "average loss of epoch 4 : 0.007135423\n",
      "average loss of epoch 5 : 0.0071307183\n",
      "average loss of epoch 6 : 0.0071251234\n",
      "average loss of epoch 7 : 0.007120228\n",
      "average loss of epoch 8 : 0.0071159047\n",
      "average loss of epoch 9 : 0.007110564\n",
      "average loss of epoch 10 : 0.007105732\n",
      "average loss of epoch 11 : 0.007100741\n",
      "average loss of epoch 12 : 0.0070961635\n",
      "average loss of epoch 13 : 0.007091268\n",
      "average loss of epoch 14 : 0.007086118\n",
      "average loss of epoch 15 : 0.0070815403\n",
      "average loss of epoch 16 : 0.0070766765\n",
      "average loss of epoch 17 : 0.007071972\n",
      "average loss of epoch 18 : 0.007066695\n",
      "average loss of epoch 19 : 0.0070616724\n",
      "average loss of epoch 20 : 0.007057667\n",
      "average loss of epoch 21 : 0.0070526125\n",
      "average loss of epoch 22 : 0.0070475894\n",
      "average loss of epoch 23 : 0.007042694\n",
      "average loss of epoch 24 : 0.0070379255\n",
      "average loss of epoch 25 : 0.007033348\n",
      "average loss of epoch 26 : 0.007028357\n",
      "average loss of epoch 27 : 0.007024034\n",
      "average loss of epoch 28 : 0.007018884\n",
      "average loss of epoch 29 : 0.0070143384\n",
      "average loss of epoch 30 : 0.007008934\n",
      "average loss of epoch 31 : 0.0070044836\n",
      "average loss of epoch 32 : 0.0069996514\n",
      "average loss of epoch 33 : 0.0069951057\n",
      "average loss of epoch 34 : 0.0069903373\n",
      "average loss of epoch 35 : 0.0069853785\n",
      "average loss of epoch 36 : 0.006980769\n",
      "average loss of epoch 37 : 0.0069760005\n",
      "average loss of epoch 38 : 0.006971105\n",
      "average loss of epoch 39 : 0.0069663683\n",
      "average loss of epoch 40 : 0.006961918\n",
      "average loss of epoch 41 : 0.00695734\n",
      "average loss of epoch 42 : 0.0069522858\n",
      "average loss of epoch 43 : 0.0069475174\n",
      "average loss of epoch 44 : 0.0069428445\n",
      "average loss of epoch 45 : 0.0069382666\n",
      "average loss of epoch 46 : 0.006933435\n",
      "average loss of epoch 47 : 0.0069292067\n",
      "average loss of epoch 48 : 0.0069238665\n",
      "average loss of epoch 49 : 0.006919575\n",
      "average loss of epoch 50 : 0.0069146794\n",
      "average loss of epoch 51 : 0.006909911\n",
      "average loss of epoch 52 : 0.0069057466\n",
      "average loss of epoch 53 : 0.0069011054\n",
      "average loss of epoch 54 : 0.0068960506\n",
      "average loss of epoch 55 : 0.0068917274\n",
      "average loss of epoch 56 : 0.0068869907\n",
      "average loss of epoch 57 : 0.0068827313\n",
      "average loss of epoch 58 : 0.006877931\n",
      "average loss of epoch 59 : 0.0068733534\n",
      "average loss of epoch 60 : 0.0068687755\n",
      "average loss of epoch 61 : 0.006863912\n",
      "average loss of epoch 62 : 0.006859557\n",
      "average loss of epoch 63 : 0.0068549793\n",
      "average loss of epoch 64 : 0.0068501155\n",
      "average loss of epoch 65 : 0.006845665\n",
      "average loss of epoch 66 : 0.0068412465\n",
      "average loss of epoch 67 : 0.0068363505\n",
      "average loss of epoch 68 : 0.006831932\n",
      "average loss of epoch 69 : 0.0068275454\n",
      "average loss of epoch 70 : 0.0068226815\n",
      "average loss of epoch 71 : 0.0068186126\n",
      "average loss of epoch 72 : 0.006813558\n",
      "average loss of epoch 73 : 0.0068089487\n",
      "average loss of epoch 74 : 0.006804816\n",
      "average loss of epoch 75 : 0.006800143\n",
      "average loss of epoch 76 : 0.006795883\n",
      "average loss of epoch 77 : 0.0067912736\n",
      "average loss of epoch 78 : 0.00678641\n",
      "average loss of epoch 79 : 0.0067819594\n",
      "average loss of epoch 80 : 0.006777795\n",
      "average loss of epoch 81 : 0.006773186\n",
      "average loss of epoch 82 : 0.006768481\n",
      "average loss of epoch 83 : 0.006764253\n",
      "average loss of epoch 84 : 0.0067594526\n",
      "average loss of epoch 85 : 0.0067553204\n",
      "average loss of epoch 86 : 0.006750965\n",
      "average loss of epoch 87 : 0.006746642\n",
      "average loss of epoch 88 : 0.006741969\n",
      "average loss of epoch 89 : 0.0067374865\n",
      "average loss of epoch 90 : 0.0067332266\n",
      "average loss of epoch 91 : 0.0067284266\n",
      "average loss of epoch 92 : 0.0067240717\n",
      "average loss of epoch 93 : 0.0067200977\n",
      "average loss of epoch 94 : 0.006715393\n",
      "average loss of epoch 95 : 0.006711038\n",
      "average loss of epoch 96 : 0.006706492\n",
      "average loss of epoch 97 : 0.0067018825\n",
      "average loss of epoch 98 : 0.0066978773\n",
      "average loss of epoch 99 : 0.006693713\n",
      "average loss of epoch 100 : 0.0066891033\n",
      "average loss of epoch 101 : 0.006684844\n",
      "average loss of epoch 102 : 0.0066801705\n",
      "average loss of epoch 103 : 0.00667607\n",
      "average loss of epoch 104 : 0.0066715875\n",
      "average loss of epoch 105 : 0.0066670734\n",
      "average loss of epoch 106 : 0.0066624642\n",
      "average loss of epoch 107 : 0.006658427\n",
      "average loss of epoch 108 : 0.0066540083\n",
      "average loss of epoch 109 : 0.006649907\n",
      "average loss of epoch 110 : 0.0066454886\n",
      "average loss of epoch 111 : 0.006641229\n",
      "average loss of epoch 112 : 0.006637001\n",
      "average loss of epoch 113 : 0.006632392\n",
      "average loss of epoch 114 : 0.0066280365\n",
      "average loss of epoch 115 : 0.0066241263\n",
      "average loss of epoch 116 : 0.0066193896\n",
      "average loss of epoch 117 : 0.0066149393\n",
      "average loss of epoch 118 : 0.006610902\n",
      "average loss of epoch 119 : 0.0066066105\n",
      "average loss of epoch 120 : 0.00660251\n",
      "average loss of epoch 121 : 0.0065985997\n",
      "average loss of epoch 122 : 0.006593736\n",
      "average loss of epoch 123 : 0.006589508\n",
      "average loss of epoch 124 : 0.0065853437\n",
      "average loss of epoch 125 : 0.006580766\n",
      "average loss of epoch 126 : 0.0065769833\n",
      "average loss of epoch 127 : 0.0065725646\n",
      "average loss of epoch 128 : 0.0065682093\n",
      "average loss of epoch 129 : 0.006564236\n",
      "average loss of epoch 130 : 0.0065598805\n",
      "average loss of epoch 131 : 0.0065553985\n",
      "average loss of epoch 132 : 0.0065514883\n",
      "average loss of epoch 133 : 0.006547387\n",
      "average loss of epoch 134 : 0.0065430957\n",
      "average loss of epoch 135 : 0.006538741\n",
      "average loss of epoch 136 : 0.0065347673\n",
      "average loss of epoch 137 : 0.006530476\n",
      "average loss of epoch 138 : 0.0065261205\n",
      "average loss of epoch 139 : 0.006522083\n",
      "average loss of epoch 140 : 0.0065177917\n",
      "average loss of epoch 141 : 0.0065136272\n",
      "average loss of epoch 142 : 0.0065093357\n",
      "average loss of epoch 143 : 0.0065056803\n",
      "average loss of epoch 144 : 0.006501198\n",
      "average loss of epoch 145 : 0.00649697\n",
      "average loss of epoch 146 : 0.006492996\n",
      "average loss of epoch 147 : 0.006488387\n",
      "average loss of epoch 148 : 0.006484413\n",
      "average loss of epoch 149 : 0.0064805667\n",
      "average loss of epoch 150 : 0.006476275\n",
      "average loss of epoch 151 : 0.006472492\n",
      "average loss of epoch 152 : 0.0064682644\n",
      "average loss of epoch 153 : 0.006463782\n",
      "average loss of epoch 154 : 0.0064599356\n",
      "average loss of epoch 155 : 0.006455644\n",
      "average loss of epoch 156 : 0.0064514796\n",
      "average loss of epoch 157 : 0.006447474\n",
      "average loss of epoch 158 : 0.0064435643\n",
      "average loss of epoch 159 : 0.0064394632\n",
      "average loss of epoch 160 : 0.0064352355\n",
      "average loss of epoch 161 : 0.00643088\n",
      "average loss of epoch 162 : 0.00642697\n",
      "average loss of epoch 163 : 0.0064228694\n",
      "average loss of epoch 164 : 0.006419023\n",
      "average loss of epoch 165 : 0.006414922\n",
      "average loss of epoch 166 : 0.006410917\n",
      "average loss of epoch 167 : 0.0064070066\n",
      "average loss of epoch 168 : 0.00640227\n",
      "average loss of epoch 169 : 0.0063987416\n",
      "average loss of epoch 170 : 0.0063947677\n",
      "average loss of epoch 171 : 0.006390794\n",
      "average loss of epoch 172 : 0.006386725\n",
      "average loss of epoch 173 : 0.0063824337\n",
      "average loss of epoch 174 : 0.0063783964\n",
      "average loss of epoch 175 : 0.0063746134\n",
      "average loss of epoch 176 : 0.006370449\n",
      "average loss of epoch 177 : 0.0063666343\n",
      "average loss of epoch 178 : 0.0063623427\n",
      "average loss of epoch 179 : 0.0063584326\n",
      "average loss of epoch 180 : 0.006354586\n",
      "average loss of epoch 181 : 0.006350549\n",
      "average loss of epoch 182 : 0.006346798\n",
      "average loss of epoch 183 : 0.0063424427\n",
      "average loss of epoch 184 : 0.0063387873\n",
      "average loss of epoch 185 : 0.006334559\n",
      "average loss of epoch 186 : 0.006330363\n",
      "average loss of epoch 187 : 0.006326834\n",
      "average loss of epoch 188 : 0.0063229245\n",
      "average loss of epoch 189 : 0.006319173\n",
      "average loss of epoch 190 : 0.0063150725\n",
      "average loss of epoch 191 : 0.0063110986\n",
      "average loss of epoch 192 : 0.0063067437\n",
      "average loss of epoch 193 : 0.006302738\n",
      "average loss of epoch 194 : 0.0062994002\n",
      "average loss of epoch 195 : 0.006295554\n",
      "average loss of epoch 196 : 0.006291167\n",
      "average loss of epoch 197 : 0.006287511\n",
      "average loss of epoch 198 : 0.006283474\n",
      "average loss of epoch 199 : 0.0062797545\n",
      "average loss of epoch 200 : 0.0062755584\n",
      "average loss of epoch 201 : 0.006271648\n",
      "average loss of epoch 202 : 0.00626812\n",
      "average loss of epoch 203 : 0.006264178\n",
      "average loss of epoch 204 : 0.006260077\n",
      "average loss of epoch 205 : 0.0062563894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss of epoch 206 : 0.0062523526\n",
      "average loss of epoch 207 : 0.006248506\n",
      "average loss of epoch 208 : 0.006244564\n",
      "average loss of epoch 209 : 0.006240781\n",
      "average loss of epoch 210 : 0.006237189\n",
      "average loss of epoch 211 : 0.0062329927\n",
      "average loss of epoch 212 : 0.0062292735\n",
      "average loss of epoch 213 : 0.006225459\n",
      "average loss of epoch 214 : 0.0062212306\n",
      "average loss of epoch 215 : 0.0062177023\n",
      "average loss of epoch 216 : 0.006213951\n",
      "average loss of epoch 217 : 0.006210041\n",
      "average loss of epoch 218 : 0.0062060994\n",
      "average loss of epoch 219 : 0.0062023164\n",
      "average loss of epoch 220 : 0.006198756\n",
      "average loss of epoch 221 : 0.006194846\n",
      "average loss of epoch 222 : 0.0061908723\n",
      "average loss of epoch 223 : 0.006187439\n",
      "average loss of epoch 224 : 0.0061834017\n",
      "average loss of epoch 225 : 0.006179587\n",
      "average loss of epoch 226 : 0.006176249\n",
      "average loss of epoch 227 : 0.00617218\n",
      "average loss of epoch 228 : 0.006168143\n",
      "average loss of epoch 229 : 0.0061647096\n",
      "average loss of epoch 230 : 0.006160545\n",
      "average loss of epoch 231 : 0.0061569214\n",
      "average loss of epoch 232 : 0.006153202\n",
      "average loss of epoch 233 : 0.0061493237\n",
      "average loss of epoch 234 : 0.0061458587\n",
      "average loss of epoch 235 : 0.0061419806\n",
      "average loss of epoch 236 : 0.006138579\n",
      "average loss of epoch 237 : 0.006134256\n",
      "average loss of epoch 238 : 0.0061306\n",
      "average loss of epoch 239 : 0.006126849\n",
      "average loss of epoch 240 : 0.006123066\n",
      "average loss of epoch 241 : 0.0061196326\n",
      "average loss of epoch 242 : 0.0061158496\n",
      "average loss of epoch 243 : 0.006112226\n",
      "average loss of epoch 244 : 0.00610822\n",
      "average loss of epoch 245 : 0.006104692\n",
      "average loss of epoch 246 : 0.00610075\n",
      "average loss of epoch 247 : 0.006097031\n",
      "average loss of epoch 248 : 0.006093661\n",
      "average loss of epoch 249 : 0.006090037\n",
      "average loss of epoch 250 : 0.0060859364\n",
      "average loss of epoch 251 : 0.0060824393\n",
      "average loss of epoch 252 : 0.006078847\n",
      "average loss of epoch 253 : 0.006075414\n",
      "average loss of epoch 254 : 0.0060714404\n",
      "average loss of epoch 255 : 0.00606788\n",
      "average loss of epoch 256 : 0.0060638743\n",
      "average loss of epoch 257 : 0.006060346\n",
      "average loss of epoch 258 : 0.0060565947\n",
      "average loss of epoch 259 : 0.006053289\n",
      "average loss of epoch 260 : 0.006049697\n",
      "average loss of epoch 261 : 0.0060458183\n",
      "average loss of epoch 262 : 0.0060424805\n",
      "average loss of epoch 263 : 0.006039047\n",
      "average loss of epoch 264 : 0.006035169\n",
      "average loss of epoch 265 : 0.006031672\n",
      "average loss of epoch 266 : 0.0060276347\n",
      "average loss of epoch 267 : 0.0060239476\n",
      "average loss of epoch 268 : 0.0060204505\n",
      "average loss of epoch 269 : 0.006016922\n",
      "average loss of epoch 270 : 0.006013552\n",
      "average loss of epoch 271 : 0.00600942\n",
      "average loss of epoch 272 : 0.0060057\n",
      "average loss of epoch 273 : 0.006002458\n",
      "average loss of epoch 274 : 0.005998961\n",
      "average loss of epoch 275 : 0.005995242\n",
      "average loss of epoch 276 : 0.0059918086\n",
      "average loss of epoch 277 : 0.005988312\n",
      "average loss of epoch 278 : 0.0059845923\n",
      "average loss of epoch 279 : 0.0059808413\n",
      "average loss of epoch 280 : 0.005977472\n",
      "average loss of epoch 281 : 0.0059737843\n",
      "average loss of epoch 282 : 0.0059703826\n",
      "average loss of epoch 283 : 0.005966822\n",
      "average loss of epoch 284 : 0.0059631984\n",
      "average loss of epoch 285 : 0.005959892\n",
      "average loss of epoch 286 : 0.005955696\n",
      "average loss of epoch 287 : 0.0059528667\n",
      "average loss of epoch 288 : 0.0059488616\n",
      "average loss of epoch 289 : 0.0059456187\n",
      "average loss of epoch 290 : 0.005942186\n",
      "average loss of epoch 291 : 0.005938403\n",
      "average loss of epoch 292 : 0.005935224\n",
      "average loss of epoch 293 : 0.005931282\n",
      "average loss of epoch 294 : 0.0059281033\n",
      "average loss of epoch 295 : 0.005924511\n",
      "average loss of epoch 296 : 0.00592076\n",
      "average loss of epoch 297 : 0.005917263\n",
      "average loss of epoch 298 : 0.005914084\n",
      "average loss of epoch 299 : 0.0059105237\n",
      "average loss of epoch 300 : 0.0059069316\n",
      "average loss of epoch 301 : 0.0059034983\n",
      "average loss of epoch 302 : 0.0059000016\n",
      "average loss of epoch 303 : 0.0058965045\n",
      "average loss of epoch 304 : 0.0058928174\n",
      "average loss of epoch 305 : 0.0058897017\n",
      "average loss of epoch 306 : 0.005886046\n",
      "average loss of epoch 307 : 0.005882295\n",
      "average loss of epoch 308 : 0.005878798\n",
      "average loss of epoch 309 : 0.005875619\n",
      "average loss of epoch 310 : 0.0058721225\n",
      "average loss of epoch 311 : 0.0058689434\n",
      "average loss of epoch 312 : 0.0058653196\n",
      "average loss of epoch 313 : 0.005862109\n",
      "average loss of epoch 314 : 0.005858485\n",
      "average loss of epoch 315 : 0.005855179\n",
      "average loss of epoch 316 : 0.0058514276\n",
      "average loss of epoch 317 : 0.005847931\n",
      "average loss of epoch 318 : 0.0058446247\n",
      "average loss of epoch 319 : 0.005841319\n",
      "average loss of epoch 320 : 0.0058377585\n",
      "average loss of epoch 321 : 0.0058344523\n",
      "average loss of epoch 322 : 0.0058313687\n",
      "average loss of epoch 323 : 0.0058281897\n",
      "average loss of epoch 324 : 0.005824248\n",
      "average loss of epoch 325 : 0.005820624\n",
      "average loss of epoch 326 : 0.005817572\n",
      "average loss of epoch 327 : 0.0058140117\n",
      "average loss of epoch 328 : 0.005810579\n",
      "average loss of epoch 329 : 0.005807336\n",
      "average loss of epoch 330 : 0.0058042207\n",
      "average loss of epoch 331 : 0.005800406\n",
      "average loss of epoch 332 : 0.005797418\n",
      "average loss of epoch 333 : 0.005793476\n",
      "average loss of epoch 334 : 0.005790615\n",
      "average loss of epoch 335 : 0.0057870545\n",
      "average loss of epoch 336 : 0.0057839393\n",
      "average loss of epoch 337 : 0.0057801884\n",
      "average loss of epoch 338 : 0.005776882\n",
      "average loss of epoch 339 : 0.005773576\n",
      "average loss of epoch 340 : 0.0057702702\n",
      "average loss of epoch 341 : 0.005767091\n",
      "average loss of epoch 342 : 0.005763785\n",
      "average loss of epoch 343 : 0.0057605426\n",
      "average loss of epoch 344 : 0.005756855\n",
      "average loss of epoch 345 : 0.0057537397\n",
      "average loss of epoch 346 : 0.0057501155\n",
      "average loss of epoch 347 : 0.005747064\n",
      "average loss of epoch 348 : 0.005743885\n",
      "average loss of epoch 349 : 0.0057405154\n",
      "average loss of epoch 350 : 0.005737273\n",
      "average loss of epoch 351 : 0.005734094\n",
      "average loss of epoch 352 : 0.0057305335\n",
      "average loss of epoch 353 : 0.005726846\n",
      "average loss of epoch 354 : 0.0057237945\n",
      "average loss of epoch 355 : 0.0057209334\n",
      "average loss of epoch 356 : 0.0057175634\n",
      "average loss of epoch 357 : 0.005714067\n",
      "average loss of epoch 358 : 0.0057108877\n",
      "average loss of epoch 359 : 0.0057080267\n",
      "average loss of epoch 360 : 0.005703926\n",
      "average loss of epoch 361 : 0.005700938\n",
      "average loss of epoch 362 : 0.00569795\n",
      "average loss of epoch 363 : 0.0056943893\n",
      "average loss of epoch 364 : 0.005691401\n",
      "average loss of epoch 365 : 0.005688159\n",
      "average loss of epoch 366 : 0.0056849164\n",
      "average loss of epoch 367 : 0.00568161\n",
      "average loss of epoch 368 : 0.005677986\n",
      "average loss of epoch 369 : 0.005675125\n",
      "average loss of epoch 370 : 0.005671692\n",
      "average loss of epoch 371 : 0.0056682904\n",
      "average loss of epoch 372 : 0.0056652385\n",
      "average loss of epoch 373 : 0.0056622503\n",
      "average loss of epoch 374 : 0.005658881\n",
      "average loss of epoch 375 : 0.0056553204\n",
      "average loss of epoch 376 : 0.005652523\n",
      "average loss of epoch 377 : 0.00564909\n",
      "average loss of epoch 378 : 0.005645879\n",
      "average loss of epoch 379 : 0.005642573\n",
      "average loss of epoch 380 : 0.0056394576\n",
      "average loss of epoch 381 : 0.0056367237\n",
      "average loss of epoch 382 : 0.0056332904\n",
      "average loss of epoch 383 : 0.005629921\n",
      "average loss of epoch 384 : 0.005626647\n",
      "average loss of epoch 385 : 0.0056239762\n",
      "average loss of epoch 386 : 0.0056206067\n",
      "average loss of epoch 387 : 0.0056173005\n",
      "average loss of epoch 388 : 0.0056141852\n",
      "average loss of epoch 389 : 0.00561107\n",
      "average loss of epoch 390 : 0.005607192\n",
      "average loss of epoch 391 : 0.0056043626\n",
      "average loss of epoch 392 : 0.0056011835\n",
      "average loss of epoch 393 : 0.0055980044\n",
      "average loss of epoch 394 : 0.005595207\n",
      "average loss of epoch 395 : 0.0055922507\n",
      "average loss of epoch 396 : 0.0055886903\n",
      "average loss of epoch 397 : 0.005585702\n",
      "average loss of epoch 398 : 0.0055823964\n",
      "average loss of epoch 399 : 0.0055792807\n",
      "average loss of epoch 400 : 0.005575943\n",
      "average loss of epoch 401 : 0.0055728913\n",
      "average loss of epoch 402 : 0.005569776\n",
      "average loss of epoch 403 : 0.005566788\n",
      "average loss of epoch 404 : 0.0055636405\n",
      "average loss of epoch 405 : 0.0055603343\n",
      "average loss of epoch 406 : 0.005557346\n",
      "average loss of epoch 407 : 0.0055544535\n",
      "average loss of epoch 408 : 0.005551211\n",
      "average loss of epoch 409 : 0.0055478415\n",
      "average loss of epoch 410 : 0.005545044\n",
      "average loss of epoch 411 : 0.0055416427\n",
      "average loss of epoch 412 : 0.005538718\n",
      "average loss of epoch 413 : 0.0055354754\n",
      "average loss of epoch 414 : 0.0055322964\n",
      "average loss of epoch 415 : 0.0055291494\n",
      "average loss of epoch 416 : 0.0055263517\n",
      "average loss of epoch 417 : 0.0055231093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss of epoch 418 : 0.005520153\n",
      "average loss of epoch 419 : 0.005516847\n",
      "average loss of epoch 420 : 0.0055143037\n",
      "average loss of epoch 421 : 0.0055110296\n",
      "average loss of epoch 422 : 0.0055080415\n",
      "average loss of epoch 423 : 0.0055049895\n",
      "average loss of epoch 424 : 0.0055015883\n",
      "average loss of epoch 425 : 0.0054987273\n",
      "average loss of epoch 426 : 0.005495294\n",
      "average loss of epoch 427 : 0.005492274\n",
      "average loss of epoch 428 : 0.0054894765\n",
      "average loss of epoch 429 : 0.0054866155\n",
      "average loss of epoch 430 : 0.00548385\n",
      "average loss of epoch 431 : 0.0054802895\n",
      "average loss of epoch 432 : 0.005477174\n",
      "average loss of epoch 433 : 0.005474599\n",
      "average loss of epoch 434 : 0.0054712933\n",
      "average loss of epoch 435 : 0.0054679234\n",
      "average loss of epoch 436 : 0.0054649035\n",
      "average loss of epoch 437 : 0.005462424\n",
      "average loss of epoch 438 : 0.005459277\n",
      "average loss of epoch 439 : 0.0054559708\n",
      "average loss of epoch 440 : 0.0054531097\n",
      "average loss of epoch 441 : 0.00545009\n",
      "average loss of epoch 442 : 0.005447229\n",
      "average loss of epoch 443 : 0.0054440815\n",
      "average loss of epoch 444 : 0.0054412843\n",
      "average loss of epoch 445 : 0.0054382323\n",
      "average loss of epoch 446 : 0.0054353396\n",
      "average loss of epoch 447 : 0.0054323515\n",
      "average loss of epoch 448 : 0.00542895\n",
      "average loss of epoch 449 : 0.0054260255\n",
      "average loss of epoch 450 : 0.0054229735\n",
      "average loss of epoch 451 : 0.005420335\n",
      "average loss of epoch 452 : 0.0054170927\n",
      "average loss of epoch 453 : 0.005414073\n",
      "average loss of epoch 454 : 0.0054114023\n",
      "average loss of epoch 455 : 0.0054083825\n",
      "average loss of epoch 456 : 0.0054054577\n",
      "average loss of epoch 457 : 0.005402692\n",
      "average loss of epoch 458 : 0.005399513\n",
      "average loss of epoch 459 : 0.0053963023\n",
      "average loss of epoch 460 : 0.005393251\n",
      "average loss of epoch 461 : 0.0053902627\n",
      "average loss of epoch 462 : 0.005387497\n",
      "average loss of epoch 463 : 0.0053850175\n",
      "average loss of epoch 464 : 0.005382125\n",
      "average loss of epoch 465 : 0.005378755\n",
      "average loss of epoch 466 : 0.005375926\n",
      "average loss of epoch 467 : 0.005373065\n",
      "average loss of epoch 468 : 0.0053696632\n",
      "average loss of epoch 469 : 0.0053669293\n",
      "average loss of epoch 470 : 0.0053640367\n",
      "average loss of epoch 471 : 0.005361557\n",
      "average loss of epoch 472 : 0.005358855\n",
      "average loss of epoch 473 : 0.005355549\n",
      "average loss of epoch 474 : 0.005352211\n",
      "average loss of epoch 475 : 0.005349223\n",
      "average loss of epoch 476 : 0.005346648\n",
      "average loss of epoch 477 : 0.005343469\n",
      "average loss of epoch 478 : 0.0053408304\n",
      "average loss of epoch 479 : 0.0053382237\n",
      "average loss of epoch 480 : 0.005335649\n",
      "average loss of epoch 481 : 0.005332343\n",
      "average loss of epoch 482 : 0.005329005\n",
      "average loss of epoch 483 : 0.005325985\n",
      "average loss of epoch 484 : 0.0053235055\n",
      "average loss of epoch 485 : 0.00532074\n",
      "average loss of epoch 486 : 0.0053174337\n",
      "average loss of epoch 487 : 0.005315304\n",
      "average loss of epoch 488 : 0.0053120614\n",
      "average loss of epoch 489 : 0.005309296\n",
      "average loss of epoch 490 : 0.0053058625\n",
      "average loss of epoch 491 : 0.0053035417\n",
      "average loss of epoch 492 : 0.005300363\n",
      "average loss of epoch 493 : 0.005298042\n",
      "average loss of epoch 494 : 0.0052947043\n",
      "average loss of epoch 495 : 0.0052920342\n",
      "average loss of epoch 496 : 0.005288887\n",
      "average loss of epoch 497 : 0.005286535\n",
      "average loss of epoch 498 : 0.0052835783\n",
      "average loss of epoch 499 : 0.005280876\n"
     ]
    }
   ],
   "source": [
    "model = train(model, dataloader,epochs=500, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001F303A6A728>\n"
     ]
    }
   ],
   "source": [
    "word_embedding = None\n",
    "for submodule in model.children():\n",
    "    if type(submodule)== nn.Linear:\n",
    "        print(submodule.parameters())\n",
    "        word_embedding = submodule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = word_embedding.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    return cosine_similarity(word_embedding[word_to_ix[word1]], word_embedding[word_to_ix[word2]], dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_topn(word, n):\n",
    "    words = list(vocab)\n",
    "    words.sort(key=lambda w: similarity(w, word), reverse=True)\n",
    "    return words[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idea',\n",
       " 'computational',\n",
       " 'evolution',\n",
       " 'pattern',\n",
       " 'is',\n",
       " 'by',\n",
       " 'As',\n",
       " 'the',\n",
       " 'process.',\n",
       " 'computers.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_topn(\"idea\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f302658230>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.functional import cosine_similarity\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Skip-gram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('about', 'We'), ('about', 'are'), ('about', 'to'), ('about', 'study'), ('to', 'are')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# vocab set and vocab size\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# construct dictionary to lookup \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "raw_skip_gram_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    center = raw_text[i]\n",
    "    for word in context: \n",
    "        raw_skip_gram_data.append((center,word))\n",
    "print(raw_skip_gram_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.dataset[idx]\n",
    "        \n",
    "        return {\"data\":torch.tensor(word_to_ix[center]), \"target\":torch.tensor(word_to_ix[context])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_skipgram = skipgram_dataset(raw_skip_gram_data)\n",
    "dataloader_skipgram = DataLoader(dataset_skipgram, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(nn.Embedding(vocab_size, 3),\n",
    "                  nn.Linear(3,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 1 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 2 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 3 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 4 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 5 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 6 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 7 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 8 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 9 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 10 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 11 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 12 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 13 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 14 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 15 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 16 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 17 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 18 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 19 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "best model saved to model.pytorch\n",
      "min loss: tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 20 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 21 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 22 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 23 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 24 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 25 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 26 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 27 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 28 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 29 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 30 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 31 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 32 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 33 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 34 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 35 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 36 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 37 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 38 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 39 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 40 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 41 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 42 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 43 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 44 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 45 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 46 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 47 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 48 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 49 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 50 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 51 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 52 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 53 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 54 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 55 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 56 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 57 loss tensor(1.8673, grad_fn=<NllLossBackward>)\n",
      "epoch 58 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 59 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 60 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 61 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 62 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 63 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 64 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 65 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 66 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 67 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 68 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 69 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 70 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 71 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 72 loss tensor(1.8674, grad_fn=<NllLossBackward>)\n",
      "epoch 73 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 74 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 75 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 76 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 77 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 78 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 79 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 80 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 81 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 82 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 83 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 84 loss tensor(1.8675, grad_fn=<NllLossBackward>)\n",
      "epoch 85 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 86 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 87 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 88 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 89 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 90 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 91 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 92 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 93 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 94 loss tensor(1.8676, grad_fn=<NllLossBackward>)\n",
      "epoch 95 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 96 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 97 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 98 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 99 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 100 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 101 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 102 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 103 loss tensor(1.8677, grad_fn=<NllLossBackward>)\n",
      "epoch 104 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 105 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 106 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 107 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 108 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 109 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 110 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 111 loss tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "epoch 112 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 113 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 114 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 115 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 116 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 117 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 118 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 119 loss tensor(1.8679, grad_fn=<NllLossBackward>)\n",
      "epoch 120 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 121 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 122 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 123 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 124 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 125 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 126 loss tensor(1.8680, grad_fn=<NllLossBackward>)\n",
      "epoch 127 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 128 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 129 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 130 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 131 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 132 loss tensor(1.8681, grad_fn=<NllLossBackward>)\n",
      "epoch 133 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 134 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 135 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 136 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 137 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 138 loss tensor(1.8682, grad_fn=<NllLossBackward>)\n",
      "epoch 139 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 140 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 141 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 142 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 143 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 144 loss tensor(1.8683, grad_fn=<NllLossBackward>)\n",
      "epoch 145 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 146 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 147 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 148 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 149 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 150 loss tensor(1.8684, grad_fn=<NllLossBackward>)\n",
      "epoch 151 loss tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "epoch 152 loss tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "epoch 153 loss tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "epoch 154 loss tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "epoch 155 loss tensor(1.8685, grad_fn=<NllLossBackward>)\n",
      "epoch 156 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 157 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 158 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 159 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 160 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 161 loss tensor(1.8686, grad_fn=<NllLossBackward>)\n",
      "epoch 162 loss tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "epoch 163 loss tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "epoch 164 loss tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "epoch 165 loss tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "epoch 166 loss tensor(1.8687, grad_fn=<NllLossBackward>)\n",
      "epoch 167 loss tensor(1.8688, grad_fn=<NllLossBackward>)\n",
      "epoch 168 loss tensor(1.8688, grad_fn=<NllLossBackward>)\n",
      "epoch 169 loss tensor(1.8688, grad_fn=<NllLossBackward>)\n",
      "epoch 170 loss tensor(1.8688, grad_fn=<NllLossBackward>)\n",
      "epoch 171 loss tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "epoch 172 loss tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "epoch 173 loss tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "epoch 174 loss tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "epoch 175 loss tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "epoch 176 loss tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "epoch 177 loss tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "epoch 178 loss tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "epoch 179 loss tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "epoch 180 loss tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "epoch 181 loss tensor(1.8691, grad_fn=<NllLossBackward>)\n",
      "epoch 182 loss tensor(1.8691, grad_fn=<NllLossBackward>)\n",
      "epoch 183 loss tensor(1.8691, grad_fn=<NllLossBackward>)\n",
      "epoch 184 loss tensor(1.8691, grad_fn=<NllLossBackward>)\n",
      "epoch 185 loss tensor(1.8692, grad_fn=<NllLossBackward>)\n",
      "epoch 186 loss tensor(1.8692, grad_fn=<NllLossBackward>)\n",
      "epoch 187 loss tensor(1.8692, grad_fn=<NllLossBackward>)\n",
      "epoch 188 loss tensor(1.8692, grad_fn=<NllLossBackward>)\n",
      "epoch 189 loss tensor(1.8692, grad_fn=<NllLossBackward>)\n",
      "epoch 190 loss tensor(1.8693, grad_fn=<NllLossBackward>)\n",
      "epoch 191 loss tensor(1.8693, grad_fn=<NllLossBackward>)\n",
      "epoch 192 loss tensor(1.8693, grad_fn=<NllLossBackward>)\n",
      "epoch 193 loss tensor(1.8693, grad_fn=<NllLossBackward>)\n",
      "epoch 194 loss tensor(1.8694, grad_fn=<NllLossBackward>)\n",
      "epoch 195 loss tensor(1.8694, grad_fn=<NllLossBackward>)\n",
      "epoch 196 loss tensor(1.8694, grad_fn=<NllLossBackward>)\n",
      "epoch 197 loss tensor(1.8694, grad_fn=<NllLossBackward>)\n",
      "epoch 198 loss tensor(1.8695, grad_fn=<NllLossBackward>)\n",
      "epoch 199 loss tensor(1.8695, grad_fn=<NllLossBackward>)\n",
      "epoch 200 loss tensor(1.8695, grad_fn=<NllLossBackward>)\n",
      "epoch 201 loss tensor(1.8695, grad_fn=<NllLossBackward>)\n",
      "epoch 202 loss tensor(1.8696, grad_fn=<NllLossBackward>)\n",
      "epoch 203 loss tensor(1.8696, grad_fn=<NllLossBackward>)\n",
      "epoch 204 loss tensor(1.8696, grad_fn=<NllLossBackward>)\n",
      "epoch 205 loss tensor(1.8696, grad_fn=<NllLossBackward>)\n",
      "epoch 206 loss tensor(1.8697, grad_fn=<NllLossBackward>)\n",
      "epoch 207 loss tensor(1.8697, grad_fn=<NllLossBackward>)\n",
      "epoch 208 loss tensor(1.8697, grad_fn=<NllLossBackward>)\n",
      "epoch 209 loss tensor(1.8697, grad_fn=<NllLossBackward>)\n",
      "epoch 210 loss tensor(1.8698, grad_fn=<NllLossBackward>)\n",
      "epoch 211 loss tensor(1.8698, grad_fn=<NllLossBackward>)\n",
      "epoch 212 loss tensor(1.8698, grad_fn=<NllLossBackward>)\n",
      "epoch 213 loss tensor(1.8698, grad_fn=<NllLossBackward>)\n",
      "epoch 214 loss tensor(1.8699, grad_fn=<NllLossBackward>)\n",
      "epoch 215 loss tensor(1.8699, grad_fn=<NllLossBackward>)\n",
      "epoch 216 loss tensor(1.8699, grad_fn=<NllLossBackward>)\n",
      "epoch 217 loss tensor(1.8699, grad_fn=<NllLossBackward>)\n",
      "epoch 218 loss tensor(1.8700, grad_fn=<NllLossBackward>)\n",
      "epoch 219 loss tensor(1.8700, grad_fn=<NllLossBackward>)\n",
      "epoch 220 loss tensor(1.8700, grad_fn=<NllLossBackward>)\n",
      "epoch 221 loss tensor(1.8700, grad_fn=<NllLossBackward>)\n",
      "epoch 222 loss tensor(1.8701, grad_fn=<NllLossBackward>)\n",
      "epoch 223 loss tensor(1.8701, grad_fn=<NllLossBackward>)\n",
      "epoch 224 loss tensor(1.8701, grad_fn=<NllLossBackward>)\n",
      "epoch 225 loss tensor(1.8702, grad_fn=<NllLossBackward>)\n",
      "epoch 226 loss tensor(1.8702, grad_fn=<NllLossBackward>)\n",
      "epoch 227 loss tensor(1.8702, grad_fn=<NllLossBackward>)\n",
      "epoch 228 loss tensor(1.8702, grad_fn=<NllLossBackward>)\n",
      "epoch 229 loss tensor(1.8703, grad_fn=<NllLossBackward>)\n",
      "epoch 230 loss tensor(1.8703, grad_fn=<NllLossBackward>)\n",
      "epoch 231 loss tensor(1.8703, grad_fn=<NllLossBackward>)\n",
      "epoch 232 loss tensor(1.8704, grad_fn=<NllLossBackward>)\n",
      "epoch 233 loss tensor(1.8704, grad_fn=<NllLossBackward>)\n",
      "epoch 234 loss tensor(1.8704, grad_fn=<NllLossBackward>)\n",
      "epoch 235 loss tensor(1.8704, grad_fn=<NllLossBackward>)\n",
      "epoch 236 loss tensor(1.8705, grad_fn=<NllLossBackward>)\n",
      "epoch 237 loss tensor(1.8705, grad_fn=<NllLossBackward>)\n",
      "epoch 238 loss tensor(1.8705, grad_fn=<NllLossBackward>)\n",
      "epoch 239 loss tensor(1.8706, grad_fn=<NllLossBackward>)\n",
      "epoch 240 loss tensor(1.8706, grad_fn=<NllLossBackward>)\n",
      "epoch 241 loss tensor(1.8706, grad_fn=<NllLossBackward>)\n",
      "epoch 242 loss tensor(1.8706, grad_fn=<NllLossBackward>)\n",
      "epoch 243 loss tensor(1.8707, grad_fn=<NllLossBackward>)\n",
      "epoch 244 loss tensor(1.8707, grad_fn=<NllLossBackward>)\n",
      "epoch 245 loss tensor(1.8707, grad_fn=<NllLossBackward>)\n",
      "epoch 246 loss tensor(1.8708, grad_fn=<NllLossBackward>)\n",
      "epoch 247 loss tensor(1.8708, grad_fn=<NllLossBackward>)\n",
      "epoch 248 loss tensor(1.8708, grad_fn=<NllLossBackward>)\n",
      "epoch 249 loss tensor(1.8708, grad_fn=<NllLossBackward>)\n",
      "epoch 250 loss tensor(1.8709, grad_fn=<NllLossBackward>)\n",
      "epoch 251 loss tensor(1.8709, grad_fn=<NllLossBackward>)\n",
      "epoch 252 loss tensor(1.8709, grad_fn=<NllLossBackward>)\n",
      "epoch 253 loss tensor(1.8710, grad_fn=<NllLossBackward>)\n",
      "epoch 254 loss tensor(1.8710, grad_fn=<NllLossBackward>)\n",
      "epoch 255 loss tensor(1.8710, grad_fn=<NllLossBackward>)\n",
      "epoch 256 loss tensor(1.8711, grad_fn=<NllLossBackward>)\n",
      "epoch 257 loss tensor(1.8711, grad_fn=<NllLossBackward>)\n",
      "epoch 258 loss tensor(1.8711, grad_fn=<NllLossBackward>)\n",
      "epoch 259 loss tensor(1.8712, grad_fn=<NllLossBackward>)\n",
      "epoch 260 loss tensor(1.8712, grad_fn=<NllLossBackward>)\n",
      "epoch 261 loss tensor(1.8712, grad_fn=<NllLossBackward>)\n",
      "epoch 262 loss tensor(1.8712, grad_fn=<NllLossBackward>)\n",
      "epoch 263 loss tensor(1.8713, grad_fn=<NllLossBackward>)\n",
      "epoch 264 loss tensor(1.8713, grad_fn=<NllLossBackward>)\n",
      "epoch 265 loss tensor(1.8713, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 266 loss tensor(1.8714, grad_fn=<NllLossBackward>)\n",
      "epoch 267 loss tensor(1.8714, grad_fn=<NllLossBackward>)\n",
      "epoch 268 loss tensor(1.8714, grad_fn=<NllLossBackward>)\n",
      "epoch 269 loss tensor(1.8715, grad_fn=<NllLossBackward>)\n",
      "epoch 270 loss tensor(1.8715, grad_fn=<NllLossBackward>)\n",
      "epoch 271 loss tensor(1.8715, grad_fn=<NllLossBackward>)\n",
      "epoch 272 loss tensor(1.8716, grad_fn=<NllLossBackward>)\n",
      "epoch 273 loss tensor(1.8716, grad_fn=<NllLossBackward>)\n",
      "epoch 274 loss tensor(1.8716, grad_fn=<NllLossBackward>)\n",
      "epoch 275 loss tensor(1.8717, grad_fn=<NllLossBackward>)\n",
      "epoch 276 loss tensor(1.8717, grad_fn=<NllLossBackward>)\n",
      "epoch 277 loss tensor(1.8717, grad_fn=<NllLossBackward>)\n",
      "epoch 278 loss tensor(1.8718, grad_fn=<NllLossBackward>)\n",
      "epoch 279 loss tensor(1.8718, grad_fn=<NllLossBackward>)\n",
      "epoch 280 loss tensor(1.8718, grad_fn=<NllLossBackward>)\n",
      "epoch 281 loss tensor(1.8719, grad_fn=<NllLossBackward>)\n",
      "epoch 282 loss tensor(1.8719, grad_fn=<NllLossBackward>)\n",
      "epoch 283 loss tensor(1.8719, grad_fn=<NllLossBackward>)\n",
      "epoch 284 loss tensor(1.8720, grad_fn=<NllLossBackward>)\n",
      "epoch 285 loss tensor(1.8720, grad_fn=<NllLossBackward>)\n",
      "epoch 286 loss tensor(1.8720, grad_fn=<NllLossBackward>)\n",
      "epoch 287 loss tensor(1.8721, grad_fn=<NllLossBackward>)\n",
      "epoch 288 loss tensor(1.8721, grad_fn=<NllLossBackward>)\n",
      "epoch 289 loss tensor(1.8721, grad_fn=<NllLossBackward>)\n",
      "epoch 290 loss tensor(1.8722, grad_fn=<NllLossBackward>)\n",
      "epoch 291 loss tensor(1.8722, grad_fn=<NllLossBackward>)\n",
      "epoch 292 loss tensor(1.8723, grad_fn=<NllLossBackward>)\n",
      "epoch 293 loss tensor(1.8723, grad_fn=<NllLossBackward>)\n",
      "epoch 294 loss tensor(1.8723, grad_fn=<NllLossBackward>)\n",
      "epoch 295 loss tensor(1.8724, grad_fn=<NllLossBackward>)\n",
      "epoch 296 loss tensor(1.8724, grad_fn=<NllLossBackward>)\n",
      "epoch 297 loss tensor(1.8724, grad_fn=<NllLossBackward>)\n",
      "epoch 298 loss tensor(1.8725, grad_fn=<NllLossBackward>)\n",
      "epoch 299 loss tensor(1.8725, grad_fn=<NllLossBackward>)\n",
      "epoch 300 loss tensor(1.8725, grad_fn=<NllLossBackward>)\n",
      "epoch 301 loss tensor(1.8726, grad_fn=<NllLossBackward>)\n",
      "epoch 302 loss tensor(1.8726, grad_fn=<NllLossBackward>)\n",
      "epoch 303 loss tensor(1.8726, grad_fn=<NllLossBackward>)\n",
      "epoch 304 loss tensor(1.8727, grad_fn=<NllLossBackward>)\n",
      "epoch 305 loss tensor(1.8727, grad_fn=<NllLossBackward>)\n",
      "epoch 306 loss tensor(1.8728, grad_fn=<NllLossBackward>)\n",
      "epoch 307 loss tensor(1.8728, grad_fn=<NllLossBackward>)\n",
      "epoch 308 loss tensor(1.8728, grad_fn=<NllLossBackward>)\n",
      "epoch 309 loss tensor(1.8729, grad_fn=<NllLossBackward>)\n",
      "epoch 310 loss tensor(1.8729, grad_fn=<NllLossBackward>)\n",
      "epoch 311 loss tensor(1.8729, grad_fn=<NllLossBackward>)\n",
      "epoch 312 loss tensor(1.8730, grad_fn=<NllLossBackward>)\n",
      "epoch 313 loss tensor(1.8730, grad_fn=<NllLossBackward>)\n",
      "epoch 314 loss tensor(1.8730, grad_fn=<NllLossBackward>)\n",
      "epoch 315 loss tensor(1.8731, grad_fn=<NllLossBackward>)\n",
      "epoch 316 loss tensor(1.8731, grad_fn=<NllLossBackward>)\n",
      "epoch 317 loss tensor(1.8732, grad_fn=<NllLossBackward>)\n",
      "epoch 318 loss tensor(1.8732, grad_fn=<NllLossBackward>)\n",
      "epoch 319 loss tensor(1.8732, grad_fn=<NllLossBackward>)\n",
      "epoch 320 loss tensor(1.8733, grad_fn=<NllLossBackward>)\n",
      "epoch 321 loss tensor(1.8733, grad_fn=<NllLossBackward>)\n",
      "epoch 322 loss tensor(1.8733, grad_fn=<NllLossBackward>)\n",
      "epoch 323 loss tensor(1.8734, grad_fn=<NllLossBackward>)\n",
      "epoch 324 loss tensor(1.8734, grad_fn=<NllLossBackward>)\n",
      "epoch 325 loss tensor(1.8735, grad_fn=<NllLossBackward>)\n",
      "epoch 326 loss tensor(1.8735, grad_fn=<NllLossBackward>)\n",
      "epoch 327 loss tensor(1.8735, grad_fn=<NllLossBackward>)\n",
      "epoch 328 loss tensor(1.8736, grad_fn=<NllLossBackward>)\n",
      "epoch 329 loss tensor(1.8736, grad_fn=<NllLossBackward>)\n",
      "epoch 330 loss tensor(1.8737, grad_fn=<NllLossBackward>)\n",
      "epoch 331 loss tensor(1.8737, grad_fn=<NllLossBackward>)\n",
      "epoch 332 loss tensor(1.8737, grad_fn=<NllLossBackward>)\n",
      "epoch 333 loss tensor(1.8738, grad_fn=<NllLossBackward>)\n",
      "epoch 334 loss tensor(1.8738, grad_fn=<NllLossBackward>)\n",
      "epoch 335 loss tensor(1.8738, grad_fn=<NllLossBackward>)\n",
      "epoch 336 loss tensor(1.8739, grad_fn=<NllLossBackward>)\n",
      "epoch 337 loss tensor(1.8739, grad_fn=<NllLossBackward>)\n",
      "epoch 338 loss tensor(1.8740, grad_fn=<NllLossBackward>)\n",
      "epoch 339 loss tensor(1.8740, grad_fn=<NllLossBackward>)\n",
      "epoch 340 loss tensor(1.8740, grad_fn=<NllLossBackward>)\n",
      "epoch 341 loss tensor(1.8741, grad_fn=<NllLossBackward>)\n",
      "epoch 342 loss tensor(1.8741, grad_fn=<NllLossBackward>)\n",
      "epoch 343 loss tensor(1.8742, grad_fn=<NllLossBackward>)\n",
      "epoch 344 loss tensor(1.8742, grad_fn=<NllLossBackward>)\n",
      "epoch 345 loss tensor(1.8742, grad_fn=<NllLossBackward>)\n",
      "epoch 346 loss tensor(1.8743, grad_fn=<NllLossBackward>)\n",
      "epoch 347 loss tensor(1.8743, grad_fn=<NllLossBackward>)\n",
      "epoch 348 loss tensor(1.8744, grad_fn=<NllLossBackward>)\n",
      "epoch 349 loss tensor(1.8744, grad_fn=<NllLossBackward>)\n",
      "epoch 350 loss tensor(1.8744, grad_fn=<NllLossBackward>)\n",
      "epoch 351 loss tensor(1.8745, grad_fn=<NllLossBackward>)\n",
      "epoch 352 loss tensor(1.8745, grad_fn=<NllLossBackward>)\n",
      "epoch 353 loss tensor(1.8746, grad_fn=<NllLossBackward>)\n",
      "epoch 354 loss tensor(1.8746, grad_fn=<NllLossBackward>)\n",
      "epoch 355 loss tensor(1.8746, grad_fn=<NllLossBackward>)\n",
      "epoch 356 loss tensor(1.8747, grad_fn=<NllLossBackward>)\n",
      "epoch 357 loss tensor(1.8747, grad_fn=<NllLossBackward>)\n",
      "epoch 358 loss tensor(1.8748, grad_fn=<NllLossBackward>)\n",
      "epoch 359 loss tensor(1.8748, grad_fn=<NllLossBackward>)\n",
      "epoch 360 loss tensor(1.8749, grad_fn=<NllLossBackward>)\n",
      "epoch 361 loss tensor(1.8749, grad_fn=<NllLossBackward>)\n",
      "epoch 362 loss tensor(1.8749, grad_fn=<NllLossBackward>)\n",
      "epoch 363 loss tensor(1.8750, grad_fn=<NllLossBackward>)\n",
      "epoch 364 loss tensor(1.8750, grad_fn=<NllLossBackward>)\n",
      "epoch 365 loss tensor(1.8751, grad_fn=<NllLossBackward>)\n",
      "epoch 366 loss tensor(1.8751, grad_fn=<NllLossBackward>)\n",
      "epoch 367 loss tensor(1.8751, grad_fn=<NllLossBackward>)\n",
      "epoch 368 loss tensor(1.8752, grad_fn=<NllLossBackward>)\n",
      "epoch 369 loss tensor(1.8752, grad_fn=<NllLossBackward>)\n",
      "epoch 370 loss tensor(1.8753, grad_fn=<NllLossBackward>)\n",
      "epoch 371 loss tensor(1.8753, grad_fn=<NllLossBackward>)\n",
      "epoch 372 loss tensor(1.8754, grad_fn=<NllLossBackward>)\n",
      "epoch 373 loss tensor(1.8754, grad_fn=<NllLossBackward>)\n",
      "epoch 374 loss tensor(1.8754, grad_fn=<NllLossBackward>)\n",
      "epoch 375 loss tensor(1.8755, grad_fn=<NllLossBackward>)\n",
      "epoch 376 loss tensor(1.8755, grad_fn=<NllLossBackward>)\n",
      "epoch 377 loss tensor(1.8756, grad_fn=<NllLossBackward>)\n",
      "epoch 378 loss tensor(1.8756, grad_fn=<NllLossBackward>)\n",
      "epoch 379 loss tensor(1.8757, grad_fn=<NllLossBackward>)\n",
      "epoch 380 loss tensor(1.8757, grad_fn=<NllLossBackward>)\n",
      "epoch 381 loss tensor(1.8757, grad_fn=<NllLossBackward>)\n",
      "epoch 382 loss tensor(1.8758, grad_fn=<NllLossBackward>)\n",
      "epoch 383 loss tensor(1.8758, grad_fn=<NllLossBackward>)\n",
      "epoch 384 loss tensor(1.8759, grad_fn=<NllLossBackward>)\n",
      "epoch 385 loss tensor(1.8759, grad_fn=<NllLossBackward>)\n",
      "epoch 386 loss tensor(1.8760, grad_fn=<NllLossBackward>)\n",
      "epoch 387 loss tensor(1.8760, grad_fn=<NllLossBackward>)\n",
      "epoch 388 loss tensor(1.8760, grad_fn=<NllLossBackward>)\n",
      "epoch 389 loss tensor(1.8761, grad_fn=<NllLossBackward>)\n",
      "epoch 390 loss tensor(1.8761, grad_fn=<NllLossBackward>)\n",
      "epoch 391 loss tensor(1.8762, grad_fn=<NllLossBackward>)\n",
      "epoch 392 loss tensor(1.8762, grad_fn=<NllLossBackward>)\n",
      "epoch 393 loss tensor(1.8763, grad_fn=<NllLossBackward>)\n",
      "epoch 394 loss tensor(1.8763, grad_fn=<NllLossBackward>)\n",
      "epoch 395 loss tensor(1.8763, grad_fn=<NllLossBackward>)\n",
      "epoch 396 loss tensor(1.8764, grad_fn=<NllLossBackward>)\n",
      "epoch 397 loss tensor(1.8764, grad_fn=<NllLossBackward>)\n",
      "epoch 398 loss tensor(1.8765, grad_fn=<NllLossBackward>)\n",
      "epoch 399 loss tensor(1.8765, grad_fn=<NllLossBackward>)\n",
      "epoch 400 loss tensor(1.8766, grad_fn=<NllLossBackward>)\n",
      "epoch 401 loss tensor(1.8766, grad_fn=<NllLossBackward>)\n",
      "epoch 402 loss tensor(1.8767, grad_fn=<NllLossBackward>)\n",
      "epoch 403 loss tensor(1.8767, grad_fn=<NllLossBackward>)\n",
      "epoch 404 loss tensor(1.8767, grad_fn=<NllLossBackward>)\n",
      "epoch 405 loss tensor(1.8768, grad_fn=<NllLossBackward>)\n",
      "epoch 406 loss tensor(1.8768, grad_fn=<NllLossBackward>)\n",
      "epoch 407 loss tensor(1.8769, grad_fn=<NllLossBackward>)\n",
      "epoch 408 loss tensor(1.8769, grad_fn=<NllLossBackward>)\n",
      "epoch 409 loss tensor(1.8770, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 410 loss tensor(1.8770, grad_fn=<NllLossBackward>)\n",
      "epoch 411 loss tensor(1.8771, grad_fn=<NllLossBackward>)\n",
      "epoch 412 loss tensor(1.8771, grad_fn=<NllLossBackward>)\n",
      "epoch 413 loss tensor(1.8772, grad_fn=<NllLossBackward>)\n",
      "epoch 414 loss tensor(1.8772, grad_fn=<NllLossBackward>)\n",
      "epoch 415 loss tensor(1.8772, grad_fn=<NllLossBackward>)\n",
      "epoch 416 loss tensor(1.8773, grad_fn=<NllLossBackward>)\n",
      "epoch 417 loss tensor(1.8773, grad_fn=<NllLossBackward>)\n",
      "epoch 418 loss tensor(1.8774, grad_fn=<NllLossBackward>)\n",
      "epoch 419 loss tensor(1.8774, grad_fn=<NllLossBackward>)\n",
      "epoch 420 loss tensor(1.8775, grad_fn=<NllLossBackward>)\n",
      "epoch 421 loss tensor(1.8775, grad_fn=<NllLossBackward>)\n",
      "epoch 422 loss tensor(1.8776, grad_fn=<NllLossBackward>)\n",
      "epoch 423 loss tensor(1.8776, grad_fn=<NllLossBackward>)\n",
      "epoch 424 loss tensor(1.8777, grad_fn=<NllLossBackward>)\n",
      "epoch 425 loss tensor(1.8777, grad_fn=<NllLossBackward>)\n",
      "epoch 426 loss tensor(1.8778, grad_fn=<NllLossBackward>)\n",
      "epoch 427 loss tensor(1.8778, grad_fn=<NllLossBackward>)\n",
      "epoch 428 loss tensor(1.8778, grad_fn=<NllLossBackward>)\n",
      "epoch 429 loss tensor(1.8779, grad_fn=<NllLossBackward>)\n",
      "epoch 430 loss tensor(1.8779, grad_fn=<NllLossBackward>)\n",
      "epoch 431 loss tensor(1.8780, grad_fn=<NllLossBackward>)\n",
      "epoch 432 loss tensor(1.8780, grad_fn=<NllLossBackward>)\n",
      "epoch 433 loss tensor(1.8781, grad_fn=<NllLossBackward>)\n",
      "epoch 434 loss tensor(1.8781, grad_fn=<NllLossBackward>)\n",
      "epoch 435 loss tensor(1.8782, grad_fn=<NllLossBackward>)\n",
      "epoch 436 loss tensor(1.8782, grad_fn=<NllLossBackward>)\n",
      "epoch 437 loss tensor(1.8783, grad_fn=<NllLossBackward>)\n",
      "epoch 438 loss tensor(1.8783, grad_fn=<NllLossBackward>)\n",
      "epoch 439 loss tensor(1.8784, grad_fn=<NllLossBackward>)\n",
      "epoch 440 loss tensor(1.8784, grad_fn=<NllLossBackward>)\n",
      "epoch 441 loss tensor(1.8785, grad_fn=<NllLossBackward>)\n",
      "epoch 442 loss tensor(1.8785, grad_fn=<NllLossBackward>)\n",
      "epoch 443 loss tensor(1.8786, grad_fn=<NllLossBackward>)\n",
      "epoch 444 loss tensor(1.8786, grad_fn=<NllLossBackward>)\n",
      "epoch 445 loss tensor(1.8786, grad_fn=<NllLossBackward>)\n",
      "epoch 446 loss tensor(1.8787, grad_fn=<NllLossBackward>)\n",
      "epoch 447 loss tensor(1.8787, grad_fn=<NllLossBackward>)\n",
      "epoch 448 loss tensor(1.8788, grad_fn=<NllLossBackward>)\n",
      "epoch 449 loss tensor(1.8788, grad_fn=<NllLossBackward>)\n",
      "epoch 450 loss tensor(1.8789, grad_fn=<NllLossBackward>)\n",
      "epoch 451 loss tensor(1.8789, grad_fn=<NllLossBackward>)\n",
      "epoch 452 loss tensor(1.8790, grad_fn=<NllLossBackward>)\n",
      "epoch 453 loss tensor(1.8790, grad_fn=<NllLossBackward>)\n",
      "epoch 454 loss tensor(1.8791, grad_fn=<NllLossBackward>)\n",
      "epoch 455 loss tensor(1.8791, grad_fn=<NllLossBackward>)\n",
      "epoch 456 loss tensor(1.8792, grad_fn=<NllLossBackward>)\n",
      "epoch 457 loss tensor(1.8792, grad_fn=<NllLossBackward>)\n",
      "epoch 458 loss tensor(1.8793, grad_fn=<NllLossBackward>)\n",
      "epoch 459 loss tensor(1.8793, grad_fn=<NllLossBackward>)\n",
      "epoch 460 loss tensor(1.8794, grad_fn=<NllLossBackward>)\n",
      "epoch 461 loss tensor(1.8794, grad_fn=<NllLossBackward>)\n",
      "epoch 462 loss tensor(1.8795, grad_fn=<NllLossBackward>)\n",
      "epoch 463 loss tensor(1.8795, grad_fn=<NllLossBackward>)\n",
      "epoch 464 loss tensor(1.8796, grad_fn=<NllLossBackward>)\n",
      "epoch 465 loss tensor(1.8796, grad_fn=<NllLossBackward>)\n",
      "epoch 466 loss tensor(1.8797, grad_fn=<NllLossBackward>)\n",
      "epoch 467 loss tensor(1.8797, grad_fn=<NllLossBackward>)\n",
      "epoch 468 loss tensor(1.8798, grad_fn=<NllLossBackward>)\n",
      "epoch 469 loss tensor(1.8798, grad_fn=<NllLossBackward>)\n",
      "epoch 470 loss tensor(1.8799, grad_fn=<NllLossBackward>)\n",
      "epoch 471 loss tensor(1.8799, grad_fn=<NllLossBackward>)\n",
      "epoch 472 loss tensor(1.8800, grad_fn=<NllLossBackward>)\n",
      "epoch 473 loss tensor(1.8800, grad_fn=<NllLossBackward>)\n",
      "epoch 474 loss tensor(1.8801, grad_fn=<NllLossBackward>)\n",
      "epoch 475 loss tensor(1.8801, grad_fn=<NllLossBackward>)\n",
      "epoch 476 loss tensor(1.8802, grad_fn=<NllLossBackward>)\n",
      "epoch 477 loss tensor(1.8802, grad_fn=<NllLossBackward>)\n",
      "epoch 478 loss tensor(1.8803, grad_fn=<NllLossBackward>)\n",
      "epoch 479 loss tensor(1.8803, grad_fn=<NllLossBackward>)\n",
      "epoch 480 loss tensor(1.8804, grad_fn=<NllLossBackward>)\n",
      "epoch 481 loss tensor(1.8804, grad_fn=<NllLossBackward>)\n",
      "epoch 482 loss tensor(1.8805, grad_fn=<NllLossBackward>)\n",
      "epoch 483 loss tensor(1.8805, grad_fn=<NllLossBackward>)\n",
      "epoch 484 loss tensor(1.8805, grad_fn=<NllLossBackward>)\n",
      "epoch 485 loss tensor(1.8806, grad_fn=<NllLossBackward>)\n",
      "epoch 486 loss tensor(1.8806, grad_fn=<NllLossBackward>)\n",
      "epoch 487 loss tensor(1.8807, grad_fn=<NllLossBackward>)\n",
      "epoch 488 loss tensor(1.8807, grad_fn=<NllLossBackward>)\n",
      "epoch 489 loss tensor(1.8808, grad_fn=<NllLossBackward>)\n",
      "epoch 490 loss tensor(1.8808, grad_fn=<NllLossBackward>)\n",
      "epoch 491 loss tensor(1.8809, grad_fn=<NllLossBackward>)\n",
      "epoch 492 loss tensor(1.8810, grad_fn=<NllLossBackward>)\n",
      "epoch 493 loss tensor(1.8810, grad_fn=<NllLossBackward>)\n",
      "epoch 494 loss tensor(1.8810, grad_fn=<NllLossBackward>)\n",
      "epoch 495 loss tensor(1.8811, grad_fn=<NllLossBackward>)\n",
      "epoch 496 loss tensor(1.8812, grad_fn=<NllLossBackward>)\n",
      "epoch 497 loss tensor(1.8812, grad_fn=<NllLossBackward>)\n",
      "epoch 498 loss tensor(1.8813, grad_fn=<NllLossBackward>)\n",
      "epoch 499 loss tensor(1.8813, grad_fn=<NllLossBackward>)\n",
      "epoch 500 loss tensor(1.8814, grad_fn=<NllLossBackward>)\n",
      "epoch 501 loss tensor(1.8814, grad_fn=<NllLossBackward>)\n",
      "epoch 502 loss tensor(1.8815, grad_fn=<NllLossBackward>)\n",
      "epoch 503 loss tensor(1.8815, grad_fn=<NllLossBackward>)\n",
      "epoch 504 loss tensor(1.8816, grad_fn=<NllLossBackward>)\n",
      "epoch 505 loss tensor(1.8816, grad_fn=<NllLossBackward>)\n",
      "epoch 506 loss tensor(1.8817, grad_fn=<NllLossBackward>)\n",
      "epoch 507 loss tensor(1.8817, grad_fn=<NllLossBackward>)\n",
      "epoch 508 loss tensor(1.8818, grad_fn=<NllLossBackward>)\n",
      "epoch 509 loss tensor(1.8818, grad_fn=<NllLossBackward>)\n",
      "epoch 510 loss tensor(1.8819, grad_fn=<NllLossBackward>)\n",
      "epoch 511 loss tensor(1.8819, grad_fn=<NllLossBackward>)\n",
      "epoch 512 loss tensor(1.8820, grad_fn=<NllLossBackward>)\n",
      "epoch 513 loss tensor(1.8820, grad_fn=<NllLossBackward>)\n",
      "epoch 514 loss tensor(1.8821, grad_fn=<NllLossBackward>)\n",
      "epoch 515 loss tensor(1.8821, grad_fn=<NllLossBackward>)\n",
      "epoch 516 loss tensor(1.8822, grad_fn=<NllLossBackward>)\n",
      "epoch 517 loss tensor(1.8822, grad_fn=<NllLossBackward>)\n",
      "epoch 518 loss tensor(1.8823, grad_fn=<NllLossBackward>)\n",
      "epoch 519 loss tensor(1.8823, grad_fn=<NllLossBackward>)\n",
      "epoch 520 loss tensor(1.8824, grad_fn=<NllLossBackward>)\n",
      "epoch 521 loss tensor(1.8824, grad_fn=<NllLossBackward>)\n",
      "epoch 522 loss tensor(1.8825, grad_fn=<NllLossBackward>)\n",
      "epoch 523 loss tensor(1.8825, grad_fn=<NllLossBackward>)\n",
      "epoch 524 loss tensor(1.8826, grad_fn=<NllLossBackward>)\n",
      "epoch 525 loss tensor(1.8826, grad_fn=<NllLossBackward>)\n",
      "epoch 526 loss tensor(1.8827, grad_fn=<NllLossBackward>)\n",
      "epoch 527 loss tensor(1.8827, grad_fn=<NllLossBackward>)\n",
      "epoch 528 loss tensor(1.8828, grad_fn=<NllLossBackward>)\n",
      "epoch 529 loss tensor(1.8828, grad_fn=<NllLossBackward>)\n",
      "epoch 530 loss tensor(1.8829, grad_fn=<NllLossBackward>)\n",
      "epoch 531 loss tensor(1.8829, grad_fn=<NllLossBackward>)\n",
      "epoch 532 loss tensor(1.8830, grad_fn=<NllLossBackward>)\n",
      "epoch 533 loss tensor(1.8831, grad_fn=<NllLossBackward>)\n",
      "epoch 534 loss tensor(1.8831, grad_fn=<NllLossBackward>)\n",
      "epoch 535 loss tensor(1.8832, grad_fn=<NllLossBackward>)\n",
      "epoch 536 loss tensor(1.8832, grad_fn=<NllLossBackward>)\n",
      "epoch 537 loss tensor(1.8833, grad_fn=<NllLossBackward>)\n",
      "epoch 538 loss tensor(1.8833, grad_fn=<NllLossBackward>)\n",
      "epoch 539 loss tensor(1.8834, grad_fn=<NllLossBackward>)\n",
      "epoch 540 loss tensor(1.8834, grad_fn=<NllLossBackward>)\n",
      "epoch 541 loss tensor(1.8835, grad_fn=<NllLossBackward>)\n",
      "epoch 542 loss tensor(1.8835, grad_fn=<NllLossBackward>)\n",
      "epoch 543 loss tensor(1.8836, grad_fn=<NllLossBackward>)\n",
      "epoch 544 loss tensor(1.8836, grad_fn=<NllLossBackward>)\n",
      "epoch 545 loss tensor(1.8837, grad_fn=<NllLossBackward>)\n",
      "epoch 546 loss tensor(1.8837, grad_fn=<NllLossBackward>)\n",
      "epoch 547 loss tensor(1.8838, grad_fn=<NllLossBackward>)\n",
      "epoch 548 loss tensor(1.8838, grad_fn=<NllLossBackward>)\n",
      "epoch 549 loss tensor(1.8839, grad_fn=<NllLossBackward>)\n",
      "epoch 550 loss tensor(1.8839, grad_fn=<NllLossBackward>)\n",
      "epoch 551 loss tensor(1.8840, grad_fn=<NllLossBackward>)\n",
      "epoch 552 loss tensor(1.8841, grad_fn=<NllLossBackward>)\n",
      "epoch 553 loss tensor(1.8841, grad_fn=<NllLossBackward>)\n",
      "epoch 554 loss tensor(1.8842, grad_fn=<NllLossBackward>)\n",
      "epoch 555 loss tensor(1.8842, grad_fn=<NllLossBackward>)\n",
      "epoch 556 loss tensor(1.8843, grad_fn=<NllLossBackward>)\n",
      "epoch 557 loss tensor(1.8843, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 558 loss tensor(1.8844, grad_fn=<NllLossBackward>)\n",
      "epoch 559 loss tensor(1.8844, grad_fn=<NllLossBackward>)\n",
      "epoch 560 loss tensor(1.8845, grad_fn=<NllLossBackward>)\n",
      "epoch 561 loss tensor(1.8845, grad_fn=<NllLossBackward>)\n",
      "epoch 562 loss tensor(1.8846, grad_fn=<NllLossBackward>)\n",
      "epoch 563 loss tensor(1.8846, grad_fn=<NllLossBackward>)\n",
      "epoch 564 loss tensor(1.8847, grad_fn=<NllLossBackward>)\n",
      "epoch 565 loss tensor(1.8847, grad_fn=<NllLossBackward>)\n",
      "epoch 566 loss tensor(1.8848, grad_fn=<NllLossBackward>)\n",
      "epoch 567 loss tensor(1.8848, grad_fn=<NllLossBackward>)\n",
      "epoch 568 loss tensor(1.8849, grad_fn=<NllLossBackward>)\n",
      "epoch 569 loss tensor(1.8850, grad_fn=<NllLossBackward>)\n",
      "epoch 570 loss tensor(1.8850, grad_fn=<NllLossBackward>)\n",
      "epoch 571 loss tensor(1.8851, grad_fn=<NllLossBackward>)\n",
      "epoch 572 loss tensor(1.8851, grad_fn=<NllLossBackward>)\n",
      "epoch 573 loss tensor(1.8852, grad_fn=<NllLossBackward>)\n",
      "epoch 574 loss tensor(1.8852, grad_fn=<NllLossBackward>)\n",
      "epoch 575 loss tensor(1.8853, grad_fn=<NllLossBackward>)\n",
      "epoch 576 loss tensor(1.8853, grad_fn=<NllLossBackward>)\n",
      "epoch 577 loss tensor(1.8854, grad_fn=<NllLossBackward>)\n",
      "epoch 578 loss tensor(1.8854, grad_fn=<NllLossBackward>)\n",
      "epoch 579 loss tensor(1.8855, grad_fn=<NllLossBackward>)\n",
      "epoch 580 loss tensor(1.8855, grad_fn=<NllLossBackward>)\n",
      "epoch 581 loss tensor(1.8856, grad_fn=<NllLossBackward>)\n",
      "epoch 582 loss tensor(1.8856, grad_fn=<NllLossBackward>)\n",
      "epoch 583 loss tensor(1.8857, grad_fn=<NllLossBackward>)\n",
      "epoch 584 loss tensor(1.8858, grad_fn=<NllLossBackward>)\n",
      "epoch 585 loss tensor(1.8858, grad_fn=<NllLossBackward>)\n",
      "epoch 586 loss tensor(1.8859, grad_fn=<NllLossBackward>)\n",
      "epoch 587 loss tensor(1.8859, grad_fn=<NllLossBackward>)\n",
      "epoch 588 loss tensor(1.8860, grad_fn=<NllLossBackward>)\n",
      "epoch 589 loss tensor(1.8860, grad_fn=<NllLossBackward>)\n",
      "epoch 590 loss tensor(1.8861, grad_fn=<NllLossBackward>)\n",
      "epoch 591 loss tensor(1.8861, grad_fn=<NllLossBackward>)\n",
      "epoch 592 loss tensor(1.8862, grad_fn=<NllLossBackward>)\n",
      "epoch 593 loss tensor(1.8862, grad_fn=<NllLossBackward>)\n",
      "epoch 594 loss tensor(1.8863, grad_fn=<NllLossBackward>)\n",
      "epoch 595 loss tensor(1.8864, grad_fn=<NllLossBackward>)\n",
      "epoch 596 loss tensor(1.8864, grad_fn=<NllLossBackward>)\n",
      "epoch 597 loss tensor(1.8865, grad_fn=<NllLossBackward>)\n",
      "epoch 598 loss tensor(1.8865, grad_fn=<NllLossBackward>)\n",
      "epoch 599 loss tensor(1.8866, grad_fn=<NllLossBackward>)\n",
      "epoch 600 loss tensor(1.8866, grad_fn=<NllLossBackward>)\n",
      "epoch 601 loss tensor(1.8867, grad_fn=<NllLossBackward>)\n",
      "epoch 602 loss tensor(1.8867, grad_fn=<NllLossBackward>)\n",
      "epoch 603 loss tensor(1.8868, grad_fn=<NllLossBackward>)\n",
      "epoch 604 loss tensor(1.8868, grad_fn=<NllLossBackward>)\n",
      "epoch 605 loss tensor(1.8869, grad_fn=<NllLossBackward>)\n",
      "epoch 606 loss tensor(1.8870, grad_fn=<NllLossBackward>)\n",
      "epoch 607 loss tensor(1.8870, grad_fn=<NllLossBackward>)\n",
      "epoch 608 loss tensor(1.8871, grad_fn=<NllLossBackward>)\n",
      "epoch 609 loss tensor(1.8871, grad_fn=<NllLossBackward>)\n",
      "epoch 610 loss tensor(1.8872, grad_fn=<NllLossBackward>)\n",
      "epoch 611 loss tensor(1.8872, grad_fn=<NllLossBackward>)\n",
      "epoch 612 loss tensor(1.8873, grad_fn=<NllLossBackward>)\n",
      "epoch 613 loss tensor(1.8873, grad_fn=<NllLossBackward>)\n",
      "epoch 614 loss tensor(1.8874, grad_fn=<NllLossBackward>)\n",
      "epoch 615 loss tensor(1.8874, grad_fn=<NllLossBackward>)\n",
      "epoch 616 loss tensor(1.8875, grad_fn=<NllLossBackward>)\n",
      "epoch 617 loss tensor(1.8876, grad_fn=<NllLossBackward>)\n",
      "epoch 618 loss tensor(1.8876, grad_fn=<NllLossBackward>)\n",
      "epoch 619 loss tensor(1.8877, grad_fn=<NllLossBackward>)\n",
      "epoch 620 loss tensor(1.8877, grad_fn=<NllLossBackward>)\n",
      "epoch 621 loss tensor(1.8878, grad_fn=<NllLossBackward>)\n",
      "epoch 622 loss tensor(1.8878, grad_fn=<NllLossBackward>)\n",
      "epoch 623 loss tensor(1.8879, grad_fn=<NllLossBackward>)\n",
      "epoch 624 loss tensor(1.8879, grad_fn=<NllLossBackward>)\n",
      "epoch 625 loss tensor(1.8880, grad_fn=<NllLossBackward>)\n",
      "epoch 626 loss tensor(1.8880, grad_fn=<NllLossBackward>)\n",
      "epoch 627 loss tensor(1.8881, grad_fn=<NllLossBackward>)\n",
      "epoch 628 loss tensor(1.8882, grad_fn=<NllLossBackward>)\n",
      "epoch 629 loss tensor(1.8882, grad_fn=<NllLossBackward>)\n",
      "epoch 630 loss tensor(1.8883, grad_fn=<NllLossBackward>)\n",
      "epoch 631 loss tensor(1.8883, grad_fn=<NllLossBackward>)\n",
      "epoch 632 loss tensor(1.8884, grad_fn=<NllLossBackward>)\n",
      "epoch 633 loss tensor(1.8884, grad_fn=<NllLossBackward>)\n",
      "epoch 634 loss tensor(1.8885, grad_fn=<NllLossBackward>)\n",
      "epoch 635 loss tensor(1.8885, grad_fn=<NllLossBackward>)\n",
      "epoch 636 loss tensor(1.8886, grad_fn=<NllLossBackward>)\n",
      "epoch 637 loss tensor(1.8887, grad_fn=<NllLossBackward>)\n",
      "epoch 638 loss tensor(1.8887, grad_fn=<NllLossBackward>)\n",
      "epoch 639 loss tensor(1.8888, grad_fn=<NllLossBackward>)\n",
      "epoch 640 loss tensor(1.8888, grad_fn=<NllLossBackward>)\n",
      "epoch 641 loss tensor(1.8889, grad_fn=<NllLossBackward>)\n",
      "epoch 642 loss tensor(1.8889, grad_fn=<NllLossBackward>)\n",
      "epoch 643 loss tensor(1.8890, grad_fn=<NllLossBackward>)\n",
      "epoch 644 loss tensor(1.8890, grad_fn=<NllLossBackward>)\n",
      "epoch 645 loss tensor(1.8891, grad_fn=<NllLossBackward>)\n",
      "epoch 646 loss tensor(1.8891, grad_fn=<NllLossBackward>)\n",
      "epoch 647 loss tensor(1.8892, grad_fn=<NllLossBackward>)\n",
      "epoch 648 loss tensor(1.8893, grad_fn=<NllLossBackward>)\n",
      "epoch 649 loss tensor(1.8893, grad_fn=<NllLossBackward>)\n",
      "epoch 650 loss tensor(1.8894, grad_fn=<NllLossBackward>)\n",
      "epoch 651 loss tensor(1.8894, grad_fn=<NllLossBackward>)\n",
      "epoch 652 loss tensor(1.8895, grad_fn=<NllLossBackward>)\n",
      "epoch 653 loss tensor(1.8895, grad_fn=<NllLossBackward>)\n",
      "epoch 654 loss tensor(1.8896, grad_fn=<NllLossBackward>)\n",
      "epoch 655 loss tensor(1.8896, grad_fn=<NllLossBackward>)\n",
      "epoch 656 loss tensor(1.8897, grad_fn=<NllLossBackward>)\n",
      "epoch 657 loss tensor(1.8898, grad_fn=<NllLossBackward>)\n",
      "epoch 658 loss tensor(1.8898, grad_fn=<NllLossBackward>)\n",
      "epoch 659 loss tensor(1.8899, grad_fn=<NllLossBackward>)\n",
      "epoch 660 loss tensor(1.8899, grad_fn=<NllLossBackward>)\n",
      "epoch 661 loss tensor(1.8900, grad_fn=<NllLossBackward>)\n",
      "epoch 662 loss tensor(1.8900, grad_fn=<NllLossBackward>)\n",
      "epoch 663 loss tensor(1.8901, grad_fn=<NllLossBackward>)\n",
      "epoch 664 loss tensor(1.8901, grad_fn=<NllLossBackward>)\n",
      "epoch 665 loss tensor(1.8902, grad_fn=<NllLossBackward>)\n",
      "epoch 666 loss tensor(1.8903, grad_fn=<NllLossBackward>)\n",
      "epoch 667 loss tensor(1.8903, grad_fn=<NllLossBackward>)\n",
      "epoch 668 loss tensor(1.8904, grad_fn=<NllLossBackward>)\n",
      "epoch 669 loss tensor(1.8904, grad_fn=<NllLossBackward>)\n",
      "epoch 670 loss tensor(1.8905, grad_fn=<NllLossBackward>)\n",
      "epoch 671 loss tensor(1.8905, grad_fn=<NllLossBackward>)\n",
      "epoch 672 loss tensor(1.8906, grad_fn=<NllLossBackward>)\n",
      "epoch 673 loss tensor(1.8906, grad_fn=<NllLossBackward>)\n",
      "epoch 674 loss tensor(1.8907, grad_fn=<NllLossBackward>)\n",
      "epoch 675 loss tensor(1.8908, grad_fn=<NllLossBackward>)\n",
      "epoch 676 loss tensor(1.8908, grad_fn=<NllLossBackward>)\n",
      "epoch 677 loss tensor(1.8909, grad_fn=<NllLossBackward>)\n",
      "epoch 678 loss tensor(1.8909, grad_fn=<NllLossBackward>)\n",
      "epoch 679 loss tensor(1.8910, grad_fn=<NllLossBackward>)\n",
      "epoch 680 loss tensor(1.8910, grad_fn=<NllLossBackward>)\n",
      "epoch 681 loss tensor(1.8911, grad_fn=<NllLossBackward>)\n",
      "epoch 682 loss tensor(1.8911, grad_fn=<NllLossBackward>)\n",
      "epoch 683 loss tensor(1.8912, grad_fn=<NllLossBackward>)\n",
      "epoch 684 loss tensor(1.8913, grad_fn=<NllLossBackward>)\n",
      "epoch 685 loss tensor(1.8913, grad_fn=<NllLossBackward>)\n",
      "epoch 686 loss tensor(1.8914, grad_fn=<NllLossBackward>)\n",
      "epoch 687 loss tensor(1.8914, grad_fn=<NllLossBackward>)\n",
      "epoch 688 loss tensor(1.8915, grad_fn=<NllLossBackward>)\n",
      "epoch 689 loss tensor(1.8915, grad_fn=<NllLossBackward>)\n",
      "epoch 690 loss tensor(1.8916, grad_fn=<NllLossBackward>)\n",
      "epoch 691 loss tensor(1.8917, grad_fn=<NllLossBackward>)\n",
      "epoch 692 loss tensor(1.8917, grad_fn=<NllLossBackward>)\n",
      "epoch 693 loss tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "epoch 694 loss tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "epoch 695 loss tensor(1.8919, grad_fn=<NllLossBackward>)\n",
      "epoch 696 loss tensor(1.8919, grad_fn=<NllLossBackward>)\n",
      "epoch 697 loss tensor(1.8920, grad_fn=<NllLossBackward>)\n",
      "epoch 698 loss tensor(1.8920, grad_fn=<NllLossBackward>)\n",
      "epoch 699 loss tensor(1.8921, grad_fn=<NllLossBackward>)\n",
      "epoch 700 loss tensor(1.8922, grad_fn=<NllLossBackward>)\n",
      "epoch 701 loss tensor(1.8922, grad_fn=<NllLossBackward>)\n",
      "epoch 702 loss tensor(1.8923, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 703 loss tensor(1.8923, grad_fn=<NllLossBackward>)\n",
      "epoch 704 loss tensor(1.8924, grad_fn=<NllLossBackward>)\n",
      "epoch 705 loss tensor(1.8924, grad_fn=<NllLossBackward>)\n",
      "epoch 706 loss tensor(1.8925, grad_fn=<NllLossBackward>)\n",
      "epoch 707 loss tensor(1.8925, grad_fn=<NllLossBackward>)\n",
      "epoch 708 loss tensor(1.8926, grad_fn=<NllLossBackward>)\n",
      "epoch 709 loss tensor(1.8927, grad_fn=<NllLossBackward>)\n",
      "epoch 710 loss tensor(1.8927, grad_fn=<NllLossBackward>)\n",
      "epoch 711 loss tensor(1.8928, grad_fn=<NllLossBackward>)\n",
      "epoch 712 loss tensor(1.8928, grad_fn=<NllLossBackward>)\n",
      "epoch 713 loss tensor(1.8929, grad_fn=<NllLossBackward>)\n",
      "epoch 714 loss tensor(1.8929, grad_fn=<NllLossBackward>)\n",
      "epoch 715 loss tensor(1.8930, grad_fn=<NllLossBackward>)\n",
      "epoch 716 loss tensor(1.8930, grad_fn=<NllLossBackward>)\n",
      "epoch 717 loss tensor(1.8931, grad_fn=<NllLossBackward>)\n",
      "epoch 718 loss tensor(1.8932, grad_fn=<NllLossBackward>)\n",
      "epoch 719 loss tensor(1.8932, grad_fn=<NllLossBackward>)\n",
      "epoch 720 loss tensor(1.8933, grad_fn=<NllLossBackward>)\n",
      "epoch 721 loss tensor(1.8933, grad_fn=<NllLossBackward>)\n",
      "epoch 722 loss tensor(1.8934, grad_fn=<NllLossBackward>)\n",
      "epoch 723 loss tensor(1.8934, grad_fn=<NllLossBackward>)\n",
      "epoch 724 loss tensor(1.8935, grad_fn=<NllLossBackward>)\n",
      "epoch 725 loss tensor(1.8936, grad_fn=<NllLossBackward>)\n",
      "epoch 726 loss tensor(1.8936, grad_fn=<NllLossBackward>)\n",
      "epoch 727 loss tensor(1.8937, grad_fn=<NllLossBackward>)\n",
      "epoch 728 loss tensor(1.8937, grad_fn=<NllLossBackward>)\n",
      "epoch 729 loss tensor(1.8938, grad_fn=<NllLossBackward>)\n",
      "epoch 730 loss tensor(1.8938, grad_fn=<NllLossBackward>)\n",
      "epoch 731 loss tensor(1.8939, grad_fn=<NllLossBackward>)\n",
      "epoch 732 loss tensor(1.8939, grad_fn=<NllLossBackward>)\n",
      "epoch 733 loss tensor(1.8940, grad_fn=<NllLossBackward>)\n",
      "epoch 734 loss tensor(1.8941, grad_fn=<NllLossBackward>)\n",
      "epoch 735 loss tensor(1.8941, grad_fn=<NllLossBackward>)\n",
      "epoch 736 loss tensor(1.8942, grad_fn=<NllLossBackward>)\n",
      "epoch 737 loss tensor(1.8942, grad_fn=<NllLossBackward>)\n",
      "epoch 738 loss tensor(1.8943, grad_fn=<NllLossBackward>)\n",
      "epoch 739 loss tensor(1.8943, grad_fn=<NllLossBackward>)\n",
      "epoch 740 loss tensor(1.8944, grad_fn=<NllLossBackward>)\n",
      "epoch 741 loss tensor(1.8944, grad_fn=<NllLossBackward>)\n",
      "epoch 742 loss tensor(1.8945, grad_fn=<NllLossBackward>)\n",
      "epoch 743 loss tensor(1.8946, grad_fn=<NllLossBackward>)\n",
      "epoch 744 loss tensor(1.8946, grad_fn=<NllLossBackward>)\n",
      "epoch 745 loss tensor(1.8947, grad_fn=<NllLossBackward>)\n",
      "epoch 746 loss tensor(1.8947, grad_fn=<NllLossBackward>)\n",
      "epoch 747 loss tensor(1.8948, grad_fn=<NllLossBackward>)\n",
      "epoch 748 loss tensor(1.8948, grad_fn=<NllLossBackward>)\n",
      "epoch 749 loss tensor(1.8949, grad_fn=<NllLossBackward>)\n",
      "epoch 750 loss tensor(1.8950, grad_fn=<NllLossBackward>)\n",
      "epoch 751 loss tensor(1.8950, grad_fn=<NllLossBackward>)\n",
      "epoch 752 loss tensor(1.8951, grad_fn=<NllLossBackward>)\n",
      "epoch 753 loss tensor(1.8951, grad_fn=<NllLossBackward>)\n",
      "epoch 754 loss tensor(1.8952, grad_fn=<NllLossBackward>)\n",
      "epoch 755 loss tensor(1.8952, grad_fn=<NllLossBackward>)\n",
      "epoch 756 loss tensor(1.8953, grad_fn=<NllLossBackward>)\n",
      "epoch 757 loss tensor(1.8953, grad_fn=<NllLossBackward>)\n",
      "epoch 758 loss tensor(1.8954, grad_fn=<NllLossBackward>)\n",
      "epoch 759 loss tensor(1.8955, grad_fn=<NllLossBackward>)\n",
      "epoch 760 loss tensor(1.8955, grad_fn=<NllLossBackward>)\n",
      "epoch 761 loss tensor(1.8956, grad_fn=<NllLossBackward>)\n",
      "epoch 762 loss tensor(1.8956, grad_fn=<NllLossBackward>)\n",
      "epoch 763 loss tensor(1.8957, grad_fn=<NllLossBackward>)\n",
      "epoch 764 loss tensor(1.8957, grad_fn=<NllLossBackward>)\n",
      "epoch 765 loss tensor(1.8958, grad_fn=<NllLossBackward>)\n",
      "epoch 766 loss tensor(1.8959, grad_fn=<NllLossBackward>)\n",
      "epoch 767 loss tensor(1.8959, grad_fn=<NllLossBackward>)\n",
      "epoch 768 loss tensor(1.8960, grad_fn=<NllLossBackward>)\n",
      "epoch 769 loss tensor(1.8960, grad_fn=<NllLossBackward>)\n",
      "epoch 770 loss tensor(1.8961, grad_fn=<NllLossBackward>)\n",
      "epoch 771 loss tensor(1.8961, grad_fn=<NllLossBackward>)\n",
      "epoch 772 loss tensor(1.8962, grad_fn=<NllLossBackward>)\n",
      "epoch 773 loss tensor(1.8962, grad_fn=<NllLossBackward>)\n",
      "epoch 774 loss tensor(1.8963, grad_fn=<NllLossBackward>)\n",
      "epoch 775 loss tensor(1.8964, grad_fn=<NllLossBackward>)\n",
      "epoch 776 loss tensor(1.8964, grad_fn=<NllLossBackward>)\n",
      "epoch 777 loss tensor(1.8965, grad_fn=<NllLossBackward>)\n",
      "epoch 778 loss tensor(1.8965, grad_fn=<NllLossBackward>)\n",
      "epoch 779 loss tensor(1.8966, grad_fn=<NllLossBackward>)\n",
      "epoch 780 loss tensor(1.8966, grad_fn=<NllLossBackward>)\n",
      "epoch 781 loss tensor(1.8967, grad_fn=<NllLossBackward>)\n",
      "epoch 782 loss tensor(1.8968, grad_fn=<NllLossBackward>)\n",
      "epoch 783 loss tensor(1.8968, grad_fn=<NllLossBackward>)\n",
      "epoch 784 loss tensor(1.8969, grad_fn=<NllLossBackward>)\n",
      "epoch 785 loss tensor(1.8969, grad_fn=<NllLossBackward>)\n",
      "epoch 786 loss tensor(1.8970, grad_fn=<NllLossBackward>)\n",
      "epoch 787 loss tensor(1.8970, grad_fn=<NllLossBackward>)\n",
      "epoch 788 loss tensor(1.8971, grad_fn=<NllLossBackward>)\n",
      "epoch 789 loss tensor(1.8971, grad_fn=<NllLossBackward>)\n",
      "epoch 790 loss tensor(1.8972, grad_fn=<NllLossBackward>)\n",
      "epoch 791 loss tensor(1.8973, grad_fn=<NllLossBackward>)\n",
      "epoch 792 loss tensor(1.8973, grad_fn=<NllLossBackward>)\n",
      "epoch 793 loss tensor(1.8974, grad_fn=<NllLossBackward>)\n",
      "epoch 794 loss tensor(1.8974, grad_fn=<NllLossBackward>)\n",
      "epoch 795 loss tensor(1.8975, grad_fn=<NllLossBackward>)\n",
      "epoch 796 loss tensor(1.8975, grad_fn=<NllLossBackward>)\n",
      "epoch 797 loss tensor(1.8976, grad_fn=<NllLossBackward>)\n",
      "epoch 798 loss tensor(1.8976, grad_fn=<NllLossBackward>)\n",
      "epoch 799 loss tensor(1.8977, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = train(model,dataloader=dataloader_skipgram, early_stopping=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = model[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    return cosine_similarity(word_embedding[word_to_ix[word1]], word_embedding[word_to_ix[word2]], dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_topn(word, n):\n",
    "    words = list(vocab)\n",
    "    words.sort(key=lambda w: similarity(w, word), reverse=True)\n",
    "    return words[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'is',\n",
       " 'directed',\n",
       " 'by',\n",
       " 'process',\n",
       " 'a',\n",
       " 'are',\n",
       " 'process.',\n",
       " 'inhabit',\n",
       " 'pattern']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_topn(\"We\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram with hierarchical softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
