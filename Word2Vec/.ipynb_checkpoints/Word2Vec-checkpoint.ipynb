{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28f85ade070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import cosine_similarity\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate raw corpus for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# vocab set and vocab size\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# construct dictionary to lookup \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "# construct training data: (context, target) pair\n",
    "raw_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    raw_data.append((context, target))\n",
    "print(raw_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 13, 30, 11]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context, target = raw_data[0]\n",
    "context\n",
    "[word_to_ix[word] for word in context]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbow_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.dataset[idx]\n",
    "        return {\"context\":torch.tensor([word_to_ix[word] for word in context]), \"target\":torch.tensor(word_to_ix[target])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cbow_dataset(raw_data)\n",
    "dataloader = DataLoader(dataset,batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        # parameter of shape (vocab_size, 3)\n",
    "        self.embedding = nn.Embedding(vocab_size, 3)\n",
    "        # matrix of shape (3, vocab_size)\n",
    "        self.linear = nn.Linear(3, vocab_size, bias=False)\n",
    "    def forward(self, x):\n",
    "        # for batch this would be (B, 3)\n",
    "        context_embed = self.embedding(x).sum(1)\n",
    "        x = self.linear(context_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.00402282\n",
      "[1,    11] loss: 0.04147724\n",
      "[2,     1] loss: 0.00342317\n",
      "[2,    11] loss: 0.03446168\n",
      "[3,     1] loss: 0.00285365\n",
      "[3,    11] loss: 0.02923987\n",
      "[4,     1] loss: 0.00249494\n",
      "[4,    11] loss: 0.02584207\n",
      "[5,     1] loss: 0.00226498\n",
      "[5,    11] loss: 0.02343580\n",
      "[6,     1] loss: 0.00210213\n",
      "[6,    11] loss: 0.02161346\n",
      "[7,     1] loss: 0.00197792\n",
      "[7,    11] loss: 0.02017254\n",
      "[8,     1] loss: 0.00187943\n",
      "[8,    11] loss: 0.01899862\n",
      "[9,     1] loss: 0.00179976\n",
      "[9,    11] loss: 0.01802174\n",
      "[10,     1] loss: 0.00173438\n",
      "[10,    11] loss: 0.01719609\n",
      "[11,     1] loss: 0.00167990\n",
      "[11,    11] loss: 0.01648965\n",
      "[12,     1] loss: 0.00163375\n",
      "[12,    11] loss: 0.01587869\n",
      "[13,     1] loss: 0.00159396\n",
      "[13,    11] loss: 0.01534478\n",
      "[14,     1] loss: 0.00155908\n",
      "[14,    11] loss: 0.01487314\n",
      "[15,     1] loss: 0.00152801\n",
      "[15,    11] loss: 0.01445185\n",
      "[16,     1] loss: 0.00149994\n",
      "[16,    11] loss: 0.01407127\n",
      "[17,     1] loss: 0.00147425\n",
      "[17,    11] loss: 0.01372370\n",
      "[18,     1] loss: 0.00145047\n",
      "[18,    11] loss: 0.01340304\n",
      "[19,     1] loss: 0.00142820\n",
      "[19,    11] loss: 0.01310444\n",
      "[20,     1] loss: 0.00140712\n",
      "[20,    11] loss: 0.01282413\n",
      "[21,     1] loss: 0.00138699\n",
      "[21,    11] loss: 0.01255914\n",
      "[22,     1] loss: 0.00136757\n",
      "[22,    11] loss: 0.01230718\n",
      "[23,     1] loss: 0.00134868\n",
      "[23,    11] loss: 0.01206650\n",
      "[24,     1] loss: 0.00133016\n",
      "[24,    11] loss: 0.01183578\n",
      "[25,     1] loss: 0.00131185\n",
      "[25,    11] loss: 0.01161411\n",
      "[26,     1] loss: 0.00129364\n",
      "[26,    11] loss: 0.01140085\n",
      "[27,     1] loss: 0.00127542\n",
      "[27,    11] loss: 0.01119566\n",
      "[28,     1] loss: 0.00125709\n",
      "[28,    11] loss: 0.01099835\n",
      "[29,     1] loss: 0.00123857\n",
      "[29,    11] loss: 0.01080884\n",
      "[30,     1] loss: 0.00121984\n",
      "[30,    11] loss: 0.01062699\n",
      "[31,     1] loss: 0.00120088\n",
      "[31,    11] loss: 0.01045254\n",
      "[32,     1] loss: 0.00118178\n",
      "[32,    11] loss: 0.01028503\n",
      "[33,     1] loss: 0.00116267\n",
      "[33,    11] loss: 0.01012382\n",
      "[34,     1] loss: 0.00114371\n",
      "[34,    11] loss: 0.00996815\n",
      "[35,     1] loss: 0.00112509\n",
      "[35,    11] loss: 0.00981733\n",
      "[36,     1] loss: 0.00110697\n",
      "[36,    11] loss: 0.00967077\n",
      "[37,     1] loss: 0.00108947\n",
      "[37,    11] loss: 0.00952808\n",
      "[38,     1] loss: 0.00107264\n",
      "[38,    11] loss: 0.00938903\n",
      "[39,     1] loss: 0.00105648\n",
      "[39,    11] loss: 0.00925351\n",
      "[40,     1] loss: 0.00104098\n",
      "[40,    11] loss: 0.00912150\n",
      "[41,     1] loss: 0.00102610\n",
      "[41,    11] loss: 0.00899300\n",
      "[42,     1] loss: 0.00101177\n",
      "[42,    11] loss: 0.00886803\n",
      "[43,     1] loss: 0.00099796\n",
      "[43,    11] loss: 0.00874658\n",
      "[44,     1] loss: 0.00098462\n",
      "[44,    11] loss: 0.00862863\n",
      "[45,     1] loss: 0.00097171\n",
      "[45,    11] loss: 0.00851415\n",
      "[46,     1] loss: 0.00095920\n",
      "[46,    11] loss: 0.00840305\n",
      "[47,     1] loss: 0.00094707\n",
      "[47,    11] loss: 0.00829528\n",
      "[48,     1] loss: 0.00093530\n",
      "[48,    11] loss: 0.00819073\n",
      "[49,     1] loss: 0.00092387\n",
      "[49,    11] loss: 0.00808930\n",
      "[50,     1] loss: 0.00091279\n",
      "[50,    11] loss: 0.00799088\n",
      "[51,     1] loss: 0.00090203\n",
      "[51,    11] loss: 0.00789534\n",
      "[52,     1] loss: 0.00089160\n",
      "[52,    11] loss: 0.00780256\n",
      "[53,     1] loss: 0.00088149\n",
      "[53,    11] loss: 0.00771243\n",
      "[54,     1] loss: 0.00087169\n",
      "[54,    11] loss: 0.00762482\n",
      "[55,     1] loss: 0.00086219\n",
      "[55,    11] loss: 0.00753961\n",
      "[56,     1] loss: 0.00085300\n",
      "[56,    11] loss: 0.00745670\n",
      "[57,     1] loss: 0.00084409\n",
      "[57,    11] loss: 0.00737598\n",
      "[58,     1] loss: 0.00083548\n",
      "[58,    11] loss: 0.00729734\n",
      "[59,     1] loss: 0.00082714\n",
      "[59,    11] loss: 0.00722070\n",
      "[60,     1] loss: 0.00081907\n",
      "[60,    11] loss: 0.00714595\n",
      "[61,     1] loss: 0.00081126\n",
      "[61,    11] loss: 0.00707303\n",
      "[62,     1] loss: 0.00080370\n",
      "[62,    11] loss: 0.00700184\n",
      "[63,     1] loss: 0.00079638\n",
      "[63,    11] loss: 0.00693232\n",
      "[64,     1] loss: 0.00078930\n",
      "[64,    11] loss: 0.00686439\n",
      "[65,     1] loss: 0.00078244\n",
      "[65,    11] loss: 0.00679798\n",
      "[66,     1] loss: 0.00077580\n",
      "[66,    11] loss: 0.00673304\n",
      "[67,     1] loss: 0.00076937\n",
      "[67,    11] loss: 0.00666950\n",
      "[68,     1] loss: 0.00076314\n",
      "[68,    11] loss: 0.00660731\n",
      "[69,     1] loss: 0.00075710\n",
      "[69,    11] loss: 0.00654641\n",
      "[70,     1] loss: 0.00075124\n",
      "[70,    11] loss: 0.00648675\n",
      "[71,     1] loss: 0.00074555\n",
      "[71,    11] loss: 0.00642828\n",
      "[72,     1] loss: 0.00074003\n",
      "[72,    11] loss: 0.00637096\n",
      "[73,     1] loss: 0.00073466\n",
      "[73,    11] loss: 0.00631474\n",
      "[74,     1] loss: 0.00072944\n",
      "[74,    11] loss: 0.00625958\n",
      "[75,     1] loss: 0.00072436\n",
      "[75,    11] loss: 0.00620544\n",
      "[76,     1] loss: 0.00071942\n",
      "[76,    11] loss: 0.00615228\n",
      "[77,     1] loss: 0.00071459\n",
      "[77,    11] loss: 0.00610007\n",
      "[78,     1] loss: 0.00070989\n",
      "[78,    11] loss: 0.00604877\n",
      "[79,     1] loss: 0.00070529\n",
      "[79,    11] loss: 0.00599835\n",
      "[80,     1] loss: 0.00070080\n",
      "[80,    11] loss: 0.00594879\n",
      "[81,     1] loss: 0.00069640\n",
      "[81,    11] loss: 0.00590004\n",
      "[82,     1] loss: 0.00069210\n",
      "[82,    11] loss: 0.00585209\n",
      "[83,     1] loss: 0.00068789\n",
      "[83,    11] loss: 0.00580492\n",
      "[84,     1] loss: 0.00068375\n",
      "[84,    11] loss: 0.00575849\n",
      "[85,     1] loss: 0.00067970\n",
      "[85,    11] loss: 0.00571278\n",
      "[86,     1] loss: 0.00067572\n",
      "[86,    11] loss: 0.00566778\n",
      "[87,     1] loss: 0.00067181\n",
      "[87,    11] loss: 0.00562346\n",
      "[88,     1] loss: 0.00066796\n",
      "[88,    11] loss: 0.00557980\n",
      "[89,     1] loss: 0.00066418\n",
      "[89,    11] loss: 0.00553679\n",
      "[90,     1] loss: 0.00066047\n",
      "[90,    11] loss: 0.00549440\n",
      "[91,     1] loss: 0.00065680\n",
      "[91,    11] loss: 0.00545262\n",
      "[92,     1] loss: 0.00065320\n",
      "[92,    11] loss: 0.00541144\n",
      "[93,     1] loss: 0.00064965\n",
      "[93,    11] loss: 0.00537083\n",
      "[94,     1] loss: 0.00064615\n",
      "[94,    11] loss: 0.00533078\n",
      "[95,     1] loss: 0.00064270\n",
      "[95,    11] loss: 0.00529127\n",
      "[96,     1] loss: 0.00063930\n",
      "[96,    11] loss: 0.00525229\n",
      "[97,     1] loss: 0.00063594\n",
      "[97,    11] loss: 0.00521384\n",
      "[98,     1] loss: 0.00063263\n",
      "[98,    11] loss: 0.00517588\n",
      "[99,     1] loss: 0.00062936\n",
      "[99,    11] loss: 0.00513841\n",
      "[100,     1] loss: 0.00062614\n",
      "[100,    11] loss: 0.00510142\n",
      "[101,     1] loss: 0.00062295\n",
      "[101,    11] loss: 0.00506490\n",
      "[102,     1] loss: 0.00061980\n",
      "[102,    11] loss: 0.00502882\n",
      "[103,     1] loss: 0.00061669\n",
      "[103,    11] loss: 0.00499319\n",
      "[104,     1] loss: 0.00061362\n",
      "[104,    11] loss: 0.00495798\n",
      "[105,     1] loss: 0.00061058\n",
      "[105,    11] loss: 0.00492320\n",
      "[106,     1] loss: 0.00060757\n",
      "[106,    11] loss: 0.00488882\n",
      "[107,     1] loss: 0.00060460\n",
      "[107,    11] loss: 0.00485485\n",
      "[108,     1] loss: 0.00060166\n",
      "[108,    11] loss: 0.00482126\n",
      "[109,     1] loss: 0.00059875\n",
      "[109,    11] loss: 0.00478806\n",
      "[110,     1] loss: 0.00059587\n",
      "[110,    11] loss: 0.00475524\n",
      "[111,     1] loss: 0.00059301\n",
      "[111,    11] loss: 0.00472278\n",
      "[112,     1] loss: 0.00059018\n",
      "[112,    11] loss: 0.00469068\n",
      "[113,     1] loss: 0.00058738\n",
      "[113,    11] loss: 0.00465894\n",
      "[114,     1] loss: 0.00058460\n",
      "[114,    11] loss: 0.00462754\n",
      "[115,     1] loss: 0.00058185\n",
      "[115,    11] loss: 0.00459649\n",
      "[116,     1] loss: 0.00057912\n",
      "[116,    11] loss: 0.00456577\n",
      "[117,     1] loss: 0.00057641\n",
      "[117,    11] loss: 0.00453539\n",
      "[118,     1] loss: 0.00057372\n",
      "[118,    11] loss: 0.00450534\n",
      "[119,     1] loss: 0.00057105\n",
      "[119,    11] loss: 0.00447561\n",
      "[120,     1] loss: 0.00056839\n",
      "[120,    11] loss: 0.00444620\n",
      "[121,     1] loss: 0.00056576\n",
      "[121,    11] loss: 0.00441710\n",
      "[122,     1] loss: 0.00056314\n",
      "[122,    11] loss: 0.00438831\n",
      "[123,     1] loss: 0.00056054\n",
      "[123,    11] loss: 0.00435983\n",
      "[124,     1] loss: 0.00055795\n",
      "[124,    11] loss: 0.00433166\n",
      "[125,     1] loss: 0.00055538\n",
      "[125,    11] loss: 0.00430378\n",
      "[126,     1] loss: 0.00055281\n",
      "[126,    11] loss: 0.00427619\n",
      "[127,     1] loss: 0.00055026\n",
      "[127,    11] loss: 0.00424889\n",
      "[128,     1] loss: 0.00054772\n",
      "[128,    11] loss: 0.00422187\n",
      "[129,     1] loss: 0.00054519\n",
      "[129,    11] loss: 0.00419513\n",
      "[130,     1] loss: 0.00054267\n",
      "[130,    11] loss: 0.00416867\n",
      "[131,     1] loss: 0.00054016\n",
      "[131,    11] loss: 0.00414246\n",
      "[132,     1] loss: 0.00053766\n",
      "[132,    11] loss: 0.00411652\n",
      "[133,     1] loss: 0.00053516\n",
      "[133,    11] loss: 0.00409084\n",
      "[134,     1] loss: 0.00053267\n",
      "[134,    11] loss: 0.00406540\n",
      "[135,     1] loss: 0.00053018\n",
      "[135,    11] loss: 0.00404021\n",
      "[136,     1] loss: 0.00052769\n",
      "[136,    11] loss: 0.00401525\n",
      "[137,     1] loss: 0.00052521\n",
      "[137,    11] loss: 0.00399052\n",
      "[138,     1] loss: 0.00052274\n",
      "[138,    11] loss: 0.00396602\n",
      "[139,     1] loss: 0.00052026\n",
      "[139,    11] loss: 0.00394174\n",
      "[140,     1] loss: 0.00051778\n",
      "[140,    11] loss: 0.00391768\n",
      "[141,     1] loss: 0.00051531\n",
      "[141,    11] loss: 0.00389382\n",
      "[142,     1] loss: 0.00051283\n",
      "[142,    11] loss: 0.00387017\n",
      "[143,     1] loss: 0.00051035\n",
      "[143,    11] loss: 0.00384672\n",
      "[144,     1] loss: 0.00050787\n",
      "[144,    11] loss: 0.00382346\n",
      "[145,     1] loss: 0.00050539\n",
      "[145,    11] loss: 0.00380040\n",
      "[146,     1] loss: 0.00050290\n",
      "[146,    11] loss: 0.00377752\n",
      "[147,     1] loss: 0.00050042\n",
      "[147,    11] loss: 0.00375483\n",
      "[148,     1] loss: 0.00049792\n",
      "[148,    11] loss: 0.00373232\n",
      "[149,     1] loss: 0.00049542\n",
      "[149,    11] loss: 0.00370999\n",
      "[150,     1] loss: 0.00049292\n",
      "[150,    11] loss: 0.00368783\n",
      "[151,     1] loss: 0.00049041\n",
      "[151,    11] loss: 0.00366584\n",
      "[152,     1] loss: 0.00048789\n",
      "[152,    11] loss: 0.00364403\n",
      "[153,     1] loss: 0.00048536\n",
      "[153,    11] loss: 0.00362238\n",
      "[154,     1] loss: 0.00048283\n",
      "[154,    11] loss: 0.00360090\n",
      "[155,     1] loss: 0.00048029\n",
      "[155,    11] loss: 0.00357959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156,     1] loss: 0.00047775\n",
      "[156,    11] loss: 0.00355843\n",
      "[157,     1] loss: 0.00047519\n",
      "[157,    11] loss: 0.00353744\n",
      "[158,     1] loss: 0.00047262\n",
      "[158,    11] loss: 0.00351661\n",
      "[159,     1] loss: 0.00047005\n",
      "[159,    11] loss: 0.00349594\n",
      "[160,     1] loss: 0.00046747\n",
      "[160,    11] loss: 0.00347543\n",
      "[161,     1] loss: 0.00046488\n",
      "[161,    11] loss: 0.00345508\n",
      "[162,     1] loss: 0.00046228\n",
      "[162,    11] loss: 0.00343488\n",
      "[163,     1] loss: 0.00045967\n",
      "[163,    11] loss: 0.00341484\n",
      "[164,     1] loss: 0.00045705\n",
      "[164,    11] loss: 0.00339496\n",
      "[165,     1] loss: 0.00045442\n",
      "[165,    11] loss: 0.00337523\n",
      "[166,     1] loss: 0.00045178\n",
      "[166,    11] loss: 0.00335565\n",
      "[167,     1] loss: 0.00044913\n",
      "[167,    11] loss: 0.00333623\n",
      "[168,     1] loss: 0.00044648\n",
      "[168,    11] loss: 0.00331696\n",
      "[169,     1] loss: 0.00044381\n",
      "[169,    11] loss: 0.00329784\n",
      "[170,     1] loss: 0.00044114\n",
      "[170,    11] loss: 0.00327888\n",
      "[171,     1] loss: 0.00043846\n",
      "[171,    11] loss: 0.00326007\n",
      "[172,     1] loss: 0.00043577\n",
      "[172,    11] loss: 0.00324141\n",
      "[173,     1] loss: 0.00043308\n",
      "[173,    11] loss: 0.00322289\n",
      "[174,     1] loss: 0.00043038\n",
      "[174,    11] loss: 0.00320453\n",
      "[175,     1] loss: 0.00042767\n",
      "[175,    11] loss: 0.00318632\n",
      "[176,     1] loss: 0.00042496\n",
      "[176,    11] loss: 0.00316826\n",
      "[177,     1] loss: 0.00042225\n",
      "[177,    11] loss: 0.00315034\n",
      "[178,     1] loss: 0.00041953\n",
      "[178,    11] loss: 0.00313257\n",
      "[179,     1] loss: 0.00041680\n",
      "[179,    11] loss: 0.00311495\n",
      "[180,     1] loss: 0.00041408\n",
      "[180,    11] loss: 0.00309748\n",
      "[181,     1] loss: 0.00041135\n",
      "[181,    11] loss: 0.00308014\n",
      "[182,     1] loss: 0.00040862\n",
      "[182,    11] loss: 0.00306296\n",
      "[183,     1] loss: 0.00040589\n",
      "[183,    11] loss: 0.00304591\n",
      "[184,     1] loss: 0.00040316\n",
      "[184,    11] loss: 0.00302900\n",
      "[185,     1] loss: 0.00040044\n",
      "[185,    11] loss: 0.00301224\n",
      "[186,     1] loss: 0.00039771\n",
      "[186,    11] loss: 0.00299562\n",
      "[187,     1] loss: 0.00039499\n",
      "[187,    11] loss: 0.00297913\n",
      "[188,     1] loss: 0.00039227\n",
      "[188,    11] loss: 0.00296278\n",
      "[189,     1] loss: 0.00038956\n",
      "[189,    11] loss: 0.00294657\n",
      "[190,     1] loss: 0.00038685\n",
      "[190,    11] loss: 0.00293049\n",
      "[191,     1] loss: 0.00038415\n",
      "[191,    11] loss: 0.00291455\n",
      "[192,     1] loss: 0.00038146\n",
      "[192,    11] loss: 0.00289874\n",
      "[193,     1] loss: 0.00037878\n",
      "[193,    11] loss: 0.00288306\n",
      "[194,     1] loss: 0.00037610\n",
      "[194,    11] loss: 0.00286750\n",
      "[195,     1] loss: 0.00037344\n",
      "[195,    11] loss: 0.00285208\n",
      "[196,     1] loss: 0.00037079\n",
      "[196,    11] loss: 0.00283679\n",
      "[197,     1] loss: 0.00036815\n",
      "[197,    11] loss: 0.00282162\n",
      "[198,     1] loss: 0.00036552\n",
      "[198,    11] loss: 0.00280658\n",
      "[199,     1] loss: 0.00036290\n",
      "[199,    11] loss: 0.00279165\n",
      "[200,     1] loss: 0.00036030\n",
      "[200,    11] loss: 0.00277685\n",
      "[201,     1] loss: 0.00035771\n",
      "[201,    11] loss: 0.00276217\n",
      "[202,     1] loss: 0.00035514\n",
      "[202,    11] loss: 0.00274762\n",
      "[203,     1] loss: 0.00035259\n",
      "[203,    11] loss: 0.00273317\n",
      "[204,     1] loss: 0.00035005\n",
      "[204,    11] loss: 0.00271885\n",
      "[205,     1] loss: 0.00034753\n",
      "[205,    11] loss: 0.00270463\n",
      "[206,     1] loss: 0.00034503\n",
      "[206,    11] loss: 0.00269054\n",
      "[207,     1] loss: 0.00034254\n",
      "[207,    11] loss: 0.00267655\n",
      "[208,     1] loss: 0.00034008\n",
      "[208,    11] loss: 0.00266268\n",
      "[209,     1] loss: 0.00033763\n",
      "[209,    11] loss: 0.00264891\n",
      "[210,     1] loss: 0.00033520\n",
      "[210,    11] loss: 0.00263526\n",
      "[211,     1] loss: 0.00033280\n",
      "[211,    11] loss: 0.00262171\n",
      "[212,     1] loss: 0.00033041\n",
      "[212,    11] loss: 0.00260826\n",
      "[213,     1] loss: 0.00032805\n",
      "[213,    11] loss: 0.00259492\n",
      "[214,     1] loss: 0.00032570\n",
      "[214,    11] loss: 0.00258168\n",
      "[215,     1] loss: 0.00032338\n",
      "[215,    11] loss: 0.00256855\n",
      "[216,     1] loss: 0.00032108\n",
      "[216,    11] loss: 0.00255551\n",
      "[217,     1] loss: 0.00031880\n",
      "[217,    11] loss: 0.00254258\n",
      "[218,     1] loss: 0.00031654\n",
      "[218,    11] loss: 0.00252974\n",
      "[219,     1] loss: 0.00031430\n",
      "[219,    11] loss: 0.00251701\n",
      "[220,     1] loss: 0.00031209\n",
      "[220,    11] loss: 0.00250436\n",
      "[221,     1] loss: 0.00030990\n",
      "[221,    11] loss: 0.00249182\n",
      "[222,     1] loss: 0.00030773\n",
      "[222,    11] loss: 0.00247936\n",
      "[223,     1] loss: 0.00030558\n",
      "[223,    11] loss: 0.00246700\n",
      "[224,     1] loss: 0.00030346\n",
      "[224,    11] loss: 0.00245473\n",
      "[225,     1] loss: 0.00030136\n",
      "[225,    11] loss: 0.00244255\n",
      "[226,     1] loss: 0.00029928\n",
      "[226,    11] loss: 0.00243046\n",
      "[227,     1] loss: 0.00029722\n",
      "[227,    11] loss: 0.00241846\n",
      "[228,     1] loss: 0.00029518\n",
      "[228,    11] loss: 0.00240655\n",
      "[229,     1] loss: 0.00029317\n",
      "[229,    11] loss: 0.00239472\n",
      "[230,     1] loss: 0.00029117\n",
      "[230,    11] loss: 0.00238298\n",
      "[231,     1] loss: 0.00028920\n",
      "[231,    11] loss: 0.00237133\n",
      "[232,     1] loss: 0.00028725\n",
      "[232,    11] loss: 0.00235975\n",
      "[233,     1] loss: 0.00028532\n",
      "[233,    11] loss: 0.00234826\n",
      "[234,     1] loss: 0.00028342\n",
      "[234,    11] loss: 0.00233686\n",
      "[235,     1] loss: 0.00028153\n",
      "[235,    11] loss: 0.00232553\n",
      "[236,     1] loss: 0.00027966\n",
      "[236,    11] loss: 0.00231428\n",
      "[237,     1] loss: 0.00027782\n",
      "[237,    11] loss: 0.00230312\n",
      "[238,     1] loss: 0.00027599\n",
      "[238,    11] loss: 0.00229203\n",
      "[239,     1] loss: 0.00027419\n",
      "[239,    11] loss: 0.00228102\n",
      "[240,     1] loss: 0.00027240\n",
      "[240,    11] loss: 0.00227008\n",
      "[241,     1] loss: 0.00027063\n",
      "[241,    11] loss: 0.00225923\n",
      "[242,     1] loss: 0.00026888\n",
      "[242,    11] loss: 0.00224844\n",
      "[243,     1] loss: 0.00026715\n",
      "[243,    11] loss: 0.00223773\n",
      "[244,     1] loss: 0.00026544\n",
      "[244,    11] loss: 0.00222710\n",
      "[245,     1] loss: 0.00026375\n",
      "[245,    11] loss: 0.00221654\n",
      "[246,     1] loss: 0.00026207\n",
      "[246,    11] loss: 0.00220605\n",
      "[247,     1] loss: 0.00026042\n",
      "[247,    11] loss: 0.00219563\n",
      "[248,     1] loss: 0.00025878\n",
      "[248,    11] loss: 0.00218528\n",
      "[249,     1] loss: 0.00025716\n",
      "[249,    11] loss: 0.00217500\n",
      "[250,     1] loss: 0.00025555\n",
      "[250,    11] loss: 0.00216479\n",
      "[251,     1] loss: 0.00025396\n",
      "[251,    11] loss: 0.00215465\n",
      "[252,     1] loss: 0.00025238\n",
      "[252,    11] loss: 0.00214457\n",
      "[253,     1] loss: 0.00025083\n",
      "[253,    11] loss: 0.00213456\n",
      "[254,     1] loss: 0.00024928\n",
      "[254,    11] loss: 0.00212462\n",
      "[255,     1] loss: 0.00024776\n",
      "[255,    11] loss: 0.00211475\n",
      "[256,     1] loss: 0.00024624\n",
      "[256,    11] loss: 0.00210494\n",
      "[257,     1] loss: 0.00024474\n",
      "[257,    11] loss: 0.00209519\n",
      "[258,     1] loss: 0.00024326\n",
      "[258,    11] loss: 0.00208550\n",
      "[259,     1] loss: 0.00024179\n",
      "[259,    11] loss: 0.00207588\n",
      "[260,     1] loss: 0.00024033\n",
      "[260,    11] loss: 0.00206632\n",
      "[261,     1] loss: 0.00023889\n",
      "[261,    11] loss: 0.00205682\n",
      "[262,     1] loss: 0.00023746\n",
      "[262,    11] loss: 0.00204738\n",
      "[263,     1] loss: 0.00023604\n",
      "[263,    11] loss: 0.00203800\n",
      "[264,     1] loss: 0.00023464\n",
      "[264,    11] loss: 0.00202868\n",
      "[265,     1] loss: 0.00023325\n",
      "[265,    11] loss: 0.00201942\n",
      "[266,     1] loss: 0.00023186\n",
      "[266,    11] loss: 0.00201022\n",
      "[267,     1] loss: 0.00023050\n",
      "[267,    11] loss: 0.00200108\n",
      "[268,     1] loss: 0.00022914\n",
      "[268,    11] loss: 0.00199199\n",
      "[269,     1] loss: 0.00022779\n",
      "[269,    11] loss: 0.00198295\n",
      "[270,     1] loss: 0.00022645\n",
      "[270,    11] loss: 0.00197398\n",
      "[271,     1] loss: 0.00022513\n",
      "[271,    11] loss: 0.00196505\n",
      "[272,     1] loss: 0.00022381\n",
      "[272,    11] loss: 0.00195618\n",
      "[273,     1] loss: 0.00022251\n",
      "[273,    11] loss: 0.00194737\n",
      "[274,     1] loss: 0.00022121\n",
      "[274,    11] loss: 0.00193860\n",
      "[275,     1] loss: 0.00021993\n",
      "[275,    11] loss: 0.00192989\n",
      "[276,     1] loss: 0.00021865\n",
      "[276,    11] loss: 0.00192123\n",
      "[277,     1] loss: 0.00021738\n",
      "[277,    11] loss: 0.00191263\n",
      "[278,     1] loss: 0.00021613\n",
      "[278,    11] loss: 0.00190407\n",
      "[279,     1] loss: 0.00021487\n",
      "[279,    11] loss: 0.00189556\n",
      "[280,     1] loss: 0.00021363\n",
      "[280,    11] loss: 0.00188711\n",
      "[281,     1] loss: 0.00021240\n",
      "[281,    11] loss: 0.00187870\n",
      "[282,     1] loss: 0.00021117\n",
      "[282,    11] loss: 0.00187034\n",
      "[283,     1] loss: 0.00020996\n",
      "[283,    11] loss: 0.00186202\n",
      "[284,     1] loss: 0.00020874\n",
      "[284,    11] loss: 0.00185376\n",
      "[285,     1] loss: 0.00020754\n",
      "[285,    11] loss: 0.00184554\n",
      "[286,     1] loss: 0.00020634\n",
      "[286,    11] loss: 0.00183737\n",
      "[287,     1] loss: 0.00020515\n",
      "[287,    11] loss: 0.00182924\n",
      "[288,     1] loss: 0.00020397\n",
      "[288,    11] loss: 0.00182115\n",
      "[289,     1] loss: 0.00020280\n",
      "[289,    11] loss: 0.00181312\n",
      "[290,     1] loss: 0.00020162\n",
      "[290,    11] loss: 0.00180512\n",
      "[291,     1] loss: 0.00020046\n",
      "[291,    11] loss: 0.00179717\n",
      "[292,     1] loss: 0.00019930\n",
      "[292,    11] loss: 0.00178926\n",
      "[293,     1] loss: 0.00019815\n",
      "[293,    11] loss: 0.00178140\n",
      "[294,     1] loss: 0.00019700\n",
      "[294,    11] loss: 0.00177357\n",
      "[295,     1] loss: 0.00019586\n",
      "[295,    11] loss: 0.00176579\n",
      "[296,     1] loss: 0.00019472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[296,    11] loss: 0.00175805\n",
      "[297,     1] loss: 0.00019359\n",
      "[297,    11] loss: 0.00175035\n",
      "[298,     1] loss: 0.00019247\n",
      "[298,    11] loss: 0.00174269\n",
      "[299,     1] loss: 0.00019134\n",
      "[299,    11] loss: 0.00173507\n",
      "[300,     1] loss: 0.00019023\n",
      "[300,    11] loss: 0.00172749\n",
      "[301,     1] loss: 0.00018911\n",
      "[301,    11] loss: 0.00171995\n",
      "[302,     1] loss: 0.00018801\n",
      "[302,    11] loss: 0.00171244\n",
      "[303,     1] loss: 0.00018690\n",
      "[303,    11] loss: 0.00170497\n",
      "[304,     1] loss: 0.00018580\n",
      "[304,    11] loss: 0.00169754\n",
      "[305,     1] loss: 0.00018471\n",
      "[305,    11] loss: 0.00169015\n",
      "[306,     1] loss: 0.00018361\n",
      "[306,    11] loss: 0.00168279\n",
      "[307,     1] loss: 0.00018252\n",
      "[307,    11] loss: 0.00167548\n",
      "[308,     1] loss: 0.00018144\n",
      "[308,    11] loss: 0.00166819\n",
      "[309,     1] loss: 0.00018036\n",
      "[309,    11] loss: 0.00166094\n",
      "[310,     1] loss: 0.00017928\n",
      "[310,    11] loss: 0.00165373\n",
      "[311,     1] loss: 0.00017821\n",
      "[311,    11] loss: 0.00164655\n",
      "[312,     1] loss: 0.00017714\n",
      "[312,    11] loss: 0.00163940\n",
      "[313,     1] loss: 0.00017607\n",
      "[313,    11] loss: 0.00163229\n",
      "[314,     1] loss: 0.00017500\n",
      "[314,    11] loss: 0.00162521\n",
      "[315,     1] loss: 0.00017394\n",
      "[315,    11] loss: 0.00161816\n",
      "[316,     1] loss: 0.00017288\n",
      "[316,    11] loss: 0.00161115\n",
      "[317,     1] loss: 0.00017183\n",
      "[317,    11] loss: 0.00160417\n",
      "[318,     1] loss: 0.00017077\n",
      "[318,    11] loss: 0.00159722\n",
      "[319,     1] loss: 0.00016972\n",
      "[319,    11] loss: 0.00159030\n",
      "[320,     1] loss: 0.00016867\n",
      "[320,    11] loss: 0.00158342\n",
      "[321,     1] loss: 0.00016763\n",
      "[321,    11] loss: 0.00157656\n",
      "[322,     1] loss: 0.00016659\n",
      "[322,    11] loss: 0.00156974\n",
      "[323,     1] loss: 0.00016555\n",
      "[323,    11] loss: 0.00156294\n",
      "[324,     1] loss: 0.00016451\n",
      "[324,    11] loss: 0.00155618\n",
      "[325,     1] loss: 0.00016348\n",
      "[325,    11] loss: 0.00154944\n",
      "[326,     1] loss: 0.00016245\n",
      "[326,    11] loss: 0.00154273\n",
      "[327,     1] loss: 0.00016142\n",
      "[327,    11] loss: 0.00153606\n",
      "[328,     1] loss: 0.00016039\n",
      "[328,    11] loss: 0.00152941\n",
      "[329,     1] loss: 0.00015937\n",
      "[329,    11] loss: 0.00152279\n",
      "[330,     1] loss: 0.00015835\n",
      "[330,    11] loss: 0.00151620\n",
      "[331,     1] loss: 0.00015733\n",
      "[331,    11] loss: 0.00150963\n",
      "[332,     1] loss: 0.00015632\n",
      "[332,    11] loss: 0.00150310\n",
      "[333,     1] loss: 0.00015531\n",
      "[333,    11] loss: 0.00149659\n",
      "[334,     1] loss: 0.00015430\n",
      "[334,    11] loss: 0.00149010\n",
      "[335,     1] loss: 0.00015329\n",
      "[335,    11] loss: 0.00148365\n",
      "[336,     1] loss: 0.00015229\n",
      "[336,    11] loss: 0.00147722\n",
      "[337,     1] loss: 0.00015129\n",
      "[337,    11] loss: 0.00147081\n",
      "[338,     1] loss: 0.00015029\n",
      "[338,    11] loss: 0.00146444\n",
      "[339,     1] loss: 0.00014930\n",
      "[339,    11] loss: 0.00145809\n",
      "[340,     1] loss: 0.00014831\n",
      "[340,    11] loss: 0.00145176\n",
      "[341,     1] loss: 0.00014732\n",
      "[341,    11] loss: 0.00144546\n",
      "[342,     1] loss: 0.00014634\n",
      "[342,    11] loss: 0.00143918\n",
      "[343,     1] loss: 0.00014536\n",
      "[343,    11] loss: 0.00143293\n",
      "[344,     1] loss: 0.00014438\n",
      "[344,    11] loss: 0.00142671\n",
      "[345,     1] loss: 0.00014341\n",
      "[345,    11] loss: 0.00142051\n",
      "[346,     1] loss: 0.00014244\n",
      "[346,    11] loss: 0.00141433\n",
      "[347,     1] loss: 0.00014147\n",
      "[347,    11] loss: 0.00140818\n",
      "[348,     1] loss: 0.00014051\n",
      "[348,    11] loss: 0.00140205\n",
      "[349,     1] loss: 0.00013955\n",
      "[349,    11] loss: 0.00139594\n",
      "[350,     1] loss: 0.00013860\n",
      "[350,    11] loss: 0.00138986\n",
      "[351,     1] loss: 0.00013765\n",
      "[351,    11] loss: 0.00138381\n",
      "[352,     1] loss: 0.00013670\n",
      "[352,    11] loss: 0.00137777\n",
      "[353,     1] loss: 0.00013576\n",
      "[353,    11] loss: 0.00137176\n",
      "[354,     1] loss: 0.00013483\n",
      "[354,    11] loss: 0.00136577\n",
      "[355,     1] loss: 0.00013390\n",
      "[355,    11] loss: 0.00135981\n",
      "[356,     1] loss: 0.00013297\n",
      "[356,    11] loss: 0.00135387\n",
      "[357,     1] loss: 0.00013205\n",
      "[357,    11] loss: 0.00134795\n",
      "[358,     1] loss: 0.00013114\n",
      "[358,    11] loss: 0.00134205\n",
      "[359,     1] loss: 0.00013023\n",
      "[359,    11] loss: 0.00133618\n",
      "[360,     1] loss: 0.00012932\n",
      "[360,    11] loss: 0.00133033\n",
      "[361,     1] loss: 0.00012842\n",
      "[361,    11] loss: 0.00132450\n",
      "[362,     1] loss: 0.00012753\n",
      "[362,    11] loss: 0.00131869\n",
      "[363,     1] loss: 0.00012664\n",
      "[363,    11] loss: 0.00131291\n",
      "[364,     1] loss: 0.00012576\n",
      "[364,    11] loss: 0.00130715\n",
      "[365,     1] loss: 0.00012488\n",
      "[365,    11] loss: 0.00130141\n",
      "[366,     1] loss: 0.00012401\n",
      "[366,    11] loss: 0.00129569\n",
      "[367,     1] loss: 0.00012315\n",
      "[367,    11] loss: 0.00128999\n",
      "[368,     1] loss: 0.00012229\n",
      "[368,    11] loss: 0.00128432\n",
      "[369,     1] loss: 0.00012144\n",
      "[369,    11] loss: 0.00127867\n",
      "[370,     1] loss: 0.00012060\n",
      "[370,    11] loss: 0.00127304\n",
      "[371,     1] loss: 0.00011976\n",
      "[371,    11] loss: 0.00126743\n",
      "[372,     1] loss: 0.00011893\n",
      "[372,    11] loss: 0.00126185\n",
      "[373,     1] loss: 0.00011811\n",
      "[373,    11] loss: 0.00125628\n",
      "[374,     1] loss: 0.00011729\n",
      "[374,    11] loss: 0.00125074\n",
      "[375,     1] loss: 0.00011648\n",
      "[375,    11] loss: 0.00124522\n",
      "[376,     1] loss: 0.00011568\n",
      "[376,    11] loss: 0.00123972\n",
      "[377,     1] loss: 0.00011488\n",
      "[377,    11] loss: 0.00123424\n",
      "[378,     1] loss: 0.00011409\n",
      "[378,    11] loss: 0.00122879\n",
      "[379,     1] loss: 0.00011331\n",
      "[379,    11] loss: 0.00122335\n",
      "[380,     1] loss: 0.00011254\n",
      "[380,    11] loss: 0.00121794\n",
      "[381,     1] loss: 0.00011177\n",
      "[381,    11] loss: 0.00121255\n",
      "[382,     1] loss: 0.00011101\n",
      "[382,    11] loss: 0.00120718\n",
      "[383,     1] loss: 0.00011026\n",
      "[383,    11] loss: 0.00120183\n",
      "[384,     1] loss: 0.00010952\n",
      "[384,    11] loss: 0.00119651\n",
      "[385,     1] loss: 0.00010878\n",
      "[385,    11] loss: 0.00119120\n",
      "[386,     1] loss: 0.00010806\n",
      "[386,    11] loss: 0.00118592\n",
      "[387,     1] loss: 0.00010734\n",
      "[387,    11] loss: 0.00118066\n",
      "[388,     1] loss: 0.00010662\n",
      "[388,    11] loss: 0.00117542\n",
      "[389,     1] loss: 0.00010592\n",
      "[389,    11] loss: 0.00117020\n",
      "[390,     1] loss: 0.00010522\n",
      "[390,    11] loss: 0.00116500\n",
      "[391,     1] loss: 0.00010453\n",
      "[391,    11] loss: 0.00115983\n",
      "[392,     1] loss: 0.00010385\n",
      "[392,    11] loss: 0.00115467\n",
      "[393,     1] loss: 0.00010318\n",
      "[393,    11] loss: 0.00114954\n",
      "[394,     1] loss: 0.00010251\n",
      "[394,    11] loss: 0.00114443\n",
      "[395,     1] loss: 0.00010185\n",
      "[395,    11] loss: 0.00113934\n",
      "[396,     1] loss: 0.00010120\n",
      "[396,    11] loss: 0.00113427\n",
      "[397,     1] loss: 0.00010056\n",
      "[397,    11] loss: 0.00112923\n",
      "[398,     1] loss: 0.00009992\n",
      "[398,    11] loss: 0.00112420\n",
      "[399,     1] loss: 0.00009929\n",
      "[399,    11] loss: 0.00111920\n",
      "[400,     1] loss: 0.00009867\n",
      "[400,    11] loss: 0.00111422\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# intialize parameters\n",
    "for parameter in model.parameters():\n",
    "    nn.init.normal_(parameter)\n",
    "\n",
    "# train\n",
    "for epoch in range(400):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        context = data[\"context\"]\n",
    "        target = data[\"target\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(context)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000028F86421EB8>\n"
     ]
    }
   ],
   "source": [
    "word_embedding = None\n",
    "for submodule in model.children():\n",
    "    if type(submodule)== nn.Linear:\n",
    "        print(submodule.parameters())\n",
    "        word_embedding = submodule.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = word_embedding.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1, word2):\n",
    "    return cosine_similarity(word_embedding[word_to_ix[word1]], word_embedding[word_to_ix[word2]], dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_topn(word, n):\n",
    "    words = list(vocab)\n",
    "    words.sort(key=lambda w: similarity(w, word), reverse=True)\n",
    "    return words[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'evolve,',\n",
       " 'program.',\n",
       " 'they',\n",
       " 'inhabit',\n",
       " 'spirits',\n",
       " 'abstract',\n",
       " 'programs',\n",
       " 'rules',\n",
       " 'directed']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_topn(\"We\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24ce8fb28f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Sequential\n",
    "from torch.nn.functional import cosine_similarity\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form Skip-gram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('about', 'We'), ('about', 'are'), ('about', 'to'), ('about', 'study'), ('to', 'are')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# vocab set and vocab size\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# construct dictionary to lookup \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "raw_skip_gram_data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    center = raw_text[i]\n",
    "    for word in context: \n",
    "        raw_skip_gram_data.append((center,word))\n",
    "print(raw_skip_gram_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram_dataset(Dataset):\n",
    "    def __init__(self, raw_dataset, transform=None):\n",
    "        # raw_dataset is a list of (context, target) pair\n",
    "        self.dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.dataset[idx]\n",
    "        \n",
    "        return {\"center\":torch.tensor(word_to_ix[center]), \"context\":torch.tensor(word_to_ix[context])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_skipgram = skipgram_dataset(raw_skip_gram_data)\n",
    "dataloader_skipgram = DataLoader(dataset_skipgram, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(nn.Embedding(vocab_size, 3),\n",
    "                  nn.Linear(3,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0571, -0.0711, -0.8671],\n",
       "        [-0.0571, -0.0711, -0.8671],\n",
       "        [-0.0571, -0.0711, -0.8671],\n",
       "        [ 0.1601, -0.3638, -0.9139]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center = torch.tensor([39,39,39,41])\n",
    "layer = nn.Embedding(vocab_size, 3)\n",
    "layer(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array = np.array([15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 1 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 2 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 3 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 4 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 5 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 6 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 7 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 8 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 9 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 10 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 11 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 12 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 13 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 14 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 15 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 16 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 17 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 18 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 19 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 20 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 21 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 22 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 23 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 24 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 25 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 26 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 27 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 28 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 29 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 30 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 31 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 32 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 33 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 34 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 35 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 36 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 37 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 38 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 39 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 40 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 41 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 42 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 43 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 44 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 45 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 46 loss tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "epoch 47 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 48 loss tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "epoch 49 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 50 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 51 loss tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "epoch 52 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 53 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 54 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 55 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 56 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 57 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 58 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 59 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 60 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 61 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 62 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 63 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 64 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 65 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 66 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 67 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 68 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 69 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 70 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 71 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 72 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 73 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 74 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 75 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 76 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 77 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 78 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 79 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 80 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 81 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 82 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 83 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 84 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 85 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 86 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 87 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 88 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 89 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 90 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 91 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 92 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 93 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 94 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 95 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 96 loss tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "epoch 97 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 98 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 99 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 100 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 101 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 102 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 103 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 104 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 105 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 106 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 107 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 108 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 109 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 110 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 111 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 112 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 113 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 114 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 115 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 116 loss tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "epoch 117 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 118 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 119 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 120 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 121 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 122 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 123 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 124 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 125 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 126 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 127 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 128 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 129 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 130 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 131 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 132 loss tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "epoch 133 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 134 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 135 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 136 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 137 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 138 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 139 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 140 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 141 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 142 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 143 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 144 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 145 loss tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "epoch 146 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 147 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 148 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 149 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 150 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 151 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 152 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 153 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 154 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 155 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 156 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 157 loss tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "epoch 158 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 159 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 160 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 161 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 162 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 163 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 164 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 165 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 166 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 167 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 168 loss tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "epoch 169 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 170 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 171 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 172 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 173 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 174 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 175 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 176 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 177 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 178 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 179 loss tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "epoch 180 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 181 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 182 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 183 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 184 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 185 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 186 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 187 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 188 loss tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "epoch 189 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 190 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 191 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 192 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 193 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 194 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 195 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 196 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 197 loss tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "epoch 198 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 199 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 200 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 201 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 202 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 203 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 204 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 205 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 206 loss tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "epoch 207 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 208 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 209 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 210 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 211 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 212 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 213 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 214 loss tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "epoch 215 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 216 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 217 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 218 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 219 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 220 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 221 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 222 loss tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "epoch 223 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 224 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 225 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 226 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 227 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 228 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 229 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 230 loss tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "epoch 231 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 232 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 233 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 234 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 235 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 236 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 237 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 238 loss tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "epoch 239 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 240 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 241 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 242 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 243 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 244 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 245 loss tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "epoch 246 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 247 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 248 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 249 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 250 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 251 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 252 loss tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "epoch 253 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 254 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 255 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 256 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 257 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 258 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 259 loss tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "epoch 260 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 261 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 262 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 263 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 264 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 265 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 266 loss tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "epoch 267 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 268 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 269 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 270 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 271 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 272 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 273 loss tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "epoch 274 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 275 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 276 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 277 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 278 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 279 loss tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "epoch 280 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 281 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 282 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 283 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 284 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 285 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 286 loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "epoch 287 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 288 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 289 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 290 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 291 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 292 loss tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "epoch 293 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "epoch 294 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "epoch 295 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "epoch 296 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "epoch 297 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 298 loss tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "epoch 299 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 300 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 301 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 302 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 303 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 304 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 305 loss tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "epoch 306 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 307 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 308 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 309 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 310 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 311 loss tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "epoch 312 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 313 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 314 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 315 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 316 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 317 loss tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "epoch 318 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 319 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 320 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 321 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 322 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 323 loss tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "epoch 324 loss tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "epoch 325 loss tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "epoch 326 loss tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "epoch 327 loss tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "epoch 328 loss tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "epoch 329 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 330 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 331 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 332 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 333 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 334 loss tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "epoch 335 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 336 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 337 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 338 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 339 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 340 loss tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "epoch 341 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 342 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 343 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 344 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 345 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 346 loss tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "epoch 347 loss tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "epoch 348 loss tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "epoch 349 loss tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "epoch 350 loss tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "epoch 351 loss tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "epoch 352 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 353 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 354 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 355 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 356 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 357 loss tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "epoch 358 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 359 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 360 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 361 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 362 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 363 loss tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "epoch 364 loss tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "epoch 365 loss tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "epoch 366 loss tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "epoch 367 loss tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "epoch 368 loss tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "epoch 369 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 370 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 371 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 372 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 373 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 374 loss tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "epoch 375 loss tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "epoch 376 loss tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "epoch 377 loss tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "epoch 378 loss tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "epoch 379 loss tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "epoch 380 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 381 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 382 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 383 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 384 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 385 loss tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "epoch 386 loss tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "epoch 387 loss tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "epoch 388 loss tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "epoch 389 loss tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "epoch 390 loss tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "epoch 391 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 392 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 393 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 394 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 395 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 396 loss tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "epoch 397 loss tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "epoch 398 loss tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "epoch 399 loss tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "epoch 400 loss tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "epoch 401 loss tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "epoch 402 loss tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "epoch 403 loss tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "epoch 404 loss tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "epoch 405 loss tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "epoch 406 loss tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "epoch 407 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 408 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 409 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 410 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 411 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 412 loss tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "epoch 413 loss tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "epoch 414 loss tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "epoch 415 loss tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "epoch 416 loss tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "epoch 417 loss tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "epoch 418 loss tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "epoch 419 loss tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "epoch 420 loss tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "epoch 421 loss tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "epoch 422 loss tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "epoch 423 loss tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "epoch 424 loss tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "epoch 425 loss tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "epoch 426 loss tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "epoch 427 loss tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "epoch 428 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 429 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 430 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 431 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 432 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 433 loss tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "epoch 434 loss tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "epoch 435 loss tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "epoch 436 loss tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "epoch 437 loss tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "epoch 438 loss tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "epoch 439 loss tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "epoch 440 loss tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "epoch 441 loss tensor(1.4749, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 442 loss tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "epoch 443 loss tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "epoch 444 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 445 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 446 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 447 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 448 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 449 loss tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "epoch 450 loss tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "epoch 451 loss tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "epoch 452 loss tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "epoch 453 loss tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "epoch 454 loss tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "epoch 455 loss tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "epoch 456 loss tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "epoch 457 loss tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "epoch 458 loss tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "epoch 459 loss tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "epoch 460 loss tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "epoch 461 loss tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "epoch 462 loss tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "epoch 463 loss tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "epoch 464 loss tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "epoch 465 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 466 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 467 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 468 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 469 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 470 loss tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "epoch 471 loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "epoch 472 loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "epoch 473 loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "epoch 474 loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "epoch 475 loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "epoch 476 loss tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "epoch 477 loss tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "epoch 478 loss tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "epoch 479 loss tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "epoch 480 loss tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "epoch 481 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 482 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 483 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 484 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 485 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 486 loss tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "epoch 487 loss tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "epoch 488 loss tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "epoch 489 loss tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "epoch 490 loss tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "epoch 491 loss tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "epoch 492 loss tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "epoch 493 loss tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "epoch 494 loss tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "epoch 495 loss tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "epoch 496 loss tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "epoch 497 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 498 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 499 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 500 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 501 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 502 loss tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "epoch 503 loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "epoch 504 loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "epoch 505 loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "epoch 506 loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "epoch 507 loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "epoch 508 loss tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "epoch 509 loss tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "epoch 510 loss tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "epoch 511 loss tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "epoch 512 loss tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "epoch 513 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 514 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 515 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 516 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 517 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 518 loss tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "epoch 519 loss tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "epoch 520 loss tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "epoch 521 loss tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "epoch 522 loss tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "epoch 523 loss tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "epoch 524 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 525 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 526 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 527 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 528 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 529 loss tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "epoch 530 loss tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "epoch 531 loss tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "epoch 532 loss tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "epoch 533 loss tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "epoch 534 loss tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "epoch 535 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 536 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 537 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 538 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 539 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 540 loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "epoch 541 loss tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "epoch 542 loss tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "epoch 543 loss tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "epoch 544 loss tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "epoch 545 loss tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "epoch 546 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 547 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 548 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 549 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 550 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 551 loss tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "epoch 552 loss tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "epoch 553 loss tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "epoch 554 loss tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "epoch 555 loss tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "epoch 556 loss tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "epoch 557 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 558 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 559 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 560 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 561 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 562 loss tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "epoch 563 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 564 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 565 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 566 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 567 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 568 loss tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "epoch 569 loss tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "epoch 570 loss tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "epoch 571 loss tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "epoch 572 loss tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "epoch 573 loss tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "epoch 574 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 575 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 576 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 577 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 578 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 579 loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "epoch 580 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "epoch 581 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "epoch 582 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "epoch 583 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "epoch 584 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "epoch 585 loss tensor(1.4775, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 586 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 587 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 588 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 589 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 590 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 591 loss tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "epoch 592 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 593 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 594 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 595 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 596 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 597 loss tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "epoch 598 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 599 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 600 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 601 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 602 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 603 loss tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "epoch 604 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 605 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 606 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 607 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 608 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 609 loss tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "epoch 610 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 611 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 612 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 613 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 614 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 615 loss tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "epoch 616 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 617 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 618 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 619 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 620 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 621 loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "epoch 622 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 623 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 624 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 625 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 626 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 627 loss tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "epoch 628 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 629 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 630 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 631 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 632 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 633 loss tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "epoch 634 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 635 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 636 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 637 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 638 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 639 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 640 loss tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "epoch 641 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 642 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 643 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 644 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 645 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 646 loss tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "epoch 647 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 648 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 649 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 650 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 651 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 652 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 653 loss tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "epoch 654 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 655 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 656 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 657 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 658 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 659 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 660 loss tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "epoch 661 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 662 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 663 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 664 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 665 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 666 loss tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "epoch 667 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 668 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 669 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 670 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 671 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 672 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 673 loss tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "epoch 674 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 675 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 676 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 677 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 678 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 679 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 680 loss tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "epoch 681 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 682 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 683 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 684 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 685 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 686 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 687 loss tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "epoch 688 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 689 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 690 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 691 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 692 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 693 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 694 loss tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "epoch 695 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 696 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 697 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 698 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 699 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 700 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 701 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 702 loss tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "epoch 703 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 704 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 705 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 706 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 707 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 708 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 709 loss tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "epoch 710 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 711 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 712 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 713 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 714 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 715 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 716 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 717 loss tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "epoch 718 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 719 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 720 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 721 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 722 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 723 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 724 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 725 loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "epoch 726 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 727 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 728 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 729 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 730 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 731 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 732 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 733 loss tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "epoch 734 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 735 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 736 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 737 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 738 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 739 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 740 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 741 loss tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "epoch 742 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 743 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 744 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 745 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 746 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 747 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 748 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 749 loss tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "epoch 750 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 751 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 752 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 753 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 754 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 755 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 756 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 757 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 758 loss tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "epoch 759 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 760 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 761 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 762 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 763 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 764 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 765 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 766 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 767 loss tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "epoch 768 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 769 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 770 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 771 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 772 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 773 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 774 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 775 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 776 loss tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "epoch 777 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 778 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 779 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 780 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 781 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 782 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 783 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 784 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 785 loss tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "epoch 786 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 787 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 788 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 789 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 790 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 791 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 792 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 793 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 794 loss tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "epoch 795 loss tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "epoch 796 loss tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "epoch 797 loss tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "epoch 798 loss tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "epoch 799 loss tensor(1.4805, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "\"\"\"\n",
    "for parameter in model.parameters():\n",
    "    nn.init.normal_(parameter)\n",
    "\"\"\"\n",
    "for epoch in range(800):\n",
    "    loss = 0\n",
    "    for idx, data in enumerate(dataloader_skipgram):\n",
    "        optimizer.zero_grad()\n",
    "        # center is of shape (B, 1)       \n",
    "        center = data[\"center\"]\n",
    "        # target is of shape ()\n",
    "        target= data[\"context\"]\n",
    "        output = model.forward(center)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch\",epoch, \"loss\", loss)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
